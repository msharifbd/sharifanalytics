[
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "4  Data Visualization",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#what-is-visualization",
    "href": "visualization.html#what-is-visualization",
    "title": "4  Data Visualization",
    "section": "4.1 What is Visualization?",
    "text": "4.1 What is Visualization?\n\n     Visualization refers to the graphical presentation of data. Visualization involves using charts, graphs, maps, and other visualization tools. Visualization enables to understand trends, partterns, and outliers, if any, in the dataset. Visualization is very critical in data analytics process in that it allows complex data to be presented in a way that is easy to understand and interpret. Visualization tools can range from simple charts to more sophisticated advanced tools like interactive dashboards.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#importance-of-visualization",
    "href": "visualization.html#importance-of-visualization",
    "title": "4  Data Visualization",
    "section": "4.2 Importance of Visualization",
    "text": "4.2 Importance of Visualization\n\n     Mastering visualization technique is important because it allows to simplify story telling of complex data, identify pattern, enhance communication and so on.\n\nSimlifying Complex Data: More complex data can be made simpler and understandable using visualization, thus enabling decsion makers to understand key insights and make informed decisions.\nIdentifying Trends and Patterns: Using visualization, patterns and trends can be easily identified in the data, which further help to identify correlation between variables.\nImproving Communication: Visualization is very effective to communicate findings from the data to stakehloders in a simple and understandway way. Therefore, users who lack technical expertise can easily understand the main point of data.\nSupporting Effective Decision Making: Visualization emphasizes important insights, thus helping organizations make data-driven decisions. Moreover, visualization makes the data more understandable and accessible, thus supporting strategic planning and operational improvements.\nDetecting Outliers and Anomalies: Effective visualization tools and techniques make it easy to spot outliers and anomalies in the data. These unusual data points can be important for identifying errors, fraud, or opportunities for further investigation.\nEngaging Audience: Interactive and visually appealing visualization can engage audience and enhance effective delivery of main points. Visualization tools like interactive dashboards allow users to explore the data themselves, thus leading to deeper understanding of the data.\nEncouraging Exploration: Data visualization tools often allow for interactive exploration of data. Users can drill down into specific data points, filter relevant data, and view different dimensions of the data, thus leading to new iinsights and discoveries.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#visualization-packages-in-r-and-python",
    "href": "visualization.html#visualization-packages-in-r-and-python",
    "title": "4  Data Visualization",
    "section": "4.3 Visualization Packages in R and Python",
    "text": "4.3 Visualization Packages in R and Python\n\nRPython\n\n\n\n     ggplot2 is a powerful package for visualization in R. In addition, some other packages enhance the functionalities of ggplot2. These packages include - gganimate, ggthemes, ggpubr, ggridges, ggmap, ggrepel, ggextra, ggpattern, ggcorrplot and so on.\n\n\n# Loading tidyverse package\nlibrary(tidyverse)\n# Loading dataset \ntips = read_csv(\n    'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv'\n)\n\n\n\n\n     In Python, matplotlib and seaborn are two of the powerful packages for visualization. Additionally, plotly, plotnine, altair, and bokeh are some other python packages that enhances visualization in python.\n\n\n# Loading Necessary Python Packages \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\n# ggplot style \nplt.style.use('ggplot')\n# Loading dataset\ntips = sns.load_dataset('tips')",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#ggplot---grammar-of-graphics",
    "href": "visualization.html#ggplot---grammar-of-graphics",
    "title": "4  Data Visualization",
    "section": "4.4 ggplot - Grammar of Graphics",
    "text": "4.4 ggplot - Grammar of Graphics\n\n     In ggplot, a plot consists of at least four elements -\n\nData - the data frame\nAesthetic Mappings - aesthetic mappings map variable from the data frame to different kinds of aesthetics such as x coordinate, y coordinate, color, shape, size and so on.\nCoordinate System - the positioning of points\nGeom - geoms are geometirc objects such as points or lines.\n\n     You can also use cheatsheet of ggplot to know more about the ggplot. Another good source to learn more about visualization in R is The R Graph Library. Similarly, for Python, you can use The Python Graph Library.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#types-of-visualization",
    "href": "visualization.html#types-of-visualization",
    "title": "4  Data Visualization",
    "section": "4.5 Types of Visualization",
    "text": "4.5 Types of Visualization\n\n4.5.1 Bar Diagram (Bar Plot)\n\n4.5.1.1 One Categorical Variable\n\nRPython\n\n\n\ntips |&gt; \n    count (sex) |&gt;\n    ggplot(mapping = aes(x = sex, y = n))+\n    geom_bar(stat = 'identity', width = 0.5, fill = \"orangered3\") + \n    labs(x = 'Sex', y = 'Total Observations')\n\n\n\n\nBar Plot of Gender (geom_bar)\n\n\n\n\n      Either of the the following code will also produce the same visualization.\n\ntips |&gt; \n    ggplot(mapping = aes(x = sex))+\n    geom_bar(width = 0.5, fill = \"maroon\") + \n    labs(x = 'Sex', y = 'Total Observations')\n\n\ntips |&gt; \n    ggplot(mapping = aes(x = sex))+\n    stat_count(width = 0.5, fill = \"maroon\") + \n    labs(x = 'Sex', y = 'Total Observations')\n\n\n\n\nsns.countplot(data = tips, x = \"sex\", width=0.5)\nplt.xlabel('Sex')\nplt.ylabel('Total Observations')\n\n\n\n\nBar Plot of Gender (sns.countplot)\n\n\n\n\n\n\n\n\n\n4.5.1.2 One Categorical Variable and One Continuous Variable\n\n     Barplot can also be used for two variables - both discrete (categorical) variables or one discrete (categorical) and one continuous variable. Below is bar plot for one discrete (categorical) and one continuous variable.\n\n\nRPython\n\n\n\ntips |&gt; \n    group_by(sex) |&gt;\n    summarize(total_bill = mean(total_bill)) |&gt;\n    ggplot(aes(x = sex, y = total_bill)) + \n    geom_col(width =0.6, fill = \"pink\") + \n    labs(x = \"Sex\", y = \"Total Bill\") + \n    geom_text(aes(label = round(total_bill,2)), vjust = -0.2)\n\n\n\n\n\n\n\n\n     The following code will produce the same results.\n\ntips |&gt; \n    ggplot(mapping = aes(x = sex, y = total_bill))+\n    geom_bar(stat = 'summary', fun = \"mean\", position = \"dodge\",\n    width = 0.60, fill = \"pink\") + \n    labs(x = \"Sex\", y = \"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\nsns.barplot(data = tips, x = \"sex\", y = \"total_bill\",\n            width= 0.5, \n            errorbar= None)\nplt.xlabel('Sex')\nplt.ylabel('Total Bill')\n\n\n\n\nBar Plot of Gender (sns.barplot)\n\n\n\n\n     The following code will add text value on the bars in barplot.\n\nax = sns.barplot(data = tips, x = \"sex\", y = \"total_bill\",\n            width= 0.5, \n            errorbar= None)\n\nfor i in ax.containers:\n    ax.bar_label(i,)\n\nplt.xlabel('Sex')\nplt.ylabel('Total Bill')\n\n\n\n\n\n\n4.5.1.3 Two Categorical Variables\n     Below is a bar plot for both discrete (categorical) variables.\n\nRPython\n\n\n\ntips |&gt; \n    count (sex, day) |&gt;\n    ggplot(mapping = aes(x = sex, y = n, fill = day))+\n    geom_bar(stat = 'identity', position = \"dodge\") + \n    labs(x = \"Sex\", y = \"Total Observations\")\n\n\n\n\nBar Plot of Gender (geom_bar - unstacking bar)\n\n\n\n\n     The following code will also produce the same visualization.\n\ntips |&gt; \n    #count (sex, day) |&gt;\n    ggplot(mapping = aes(x = sex, fill = day))+\n    geom_bar(stat = 'count', position = \"dodge\") + \n    labs(x = \"Sex\", y = \"Total Observations\"\n         ,fill = \"Day\"\n    )\n\n\n\n\n\n\n\n\n\ntips |&gt; \n    count (sex, day) |&gt;\n    ggplot(mapping = aes(x = sex, y = n, fill = day))+\n    geom_bar(stat = 'identity', position = \"stack\") + # position = \"fill\"\n    labs(x = \"Sex\", y = \"Total Observations\")\n\n\n\n\n\n\n\n\n     The following code will also produce the same visualization.\n\ntips |&gt; \n    #count (sex, day) |&gt;\n    ggplot(mapping = aes(x = sex, fill = day))+\n    geom_bar(stat = 'count', position = \"stack\") + # position = \"fill\"\n    labs(x = \"Sex\", y = \"Total Observations\"\n         ,fill = \"Day\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nsns.countplot(data = tips, x = \"sex\", hue = \"day\")\nplt.xlabel('Sex')\nplt.ylabel('Total Observations')\n\n\n\n\nBar Plot of Gender (sns.countplot - unstacking bar)\n\n\n\n\n     Stacked barchart cannot be created using seaborn. So, we use alternatives -\n\ntips[['sex', 'day']].value_counts().reset_index() \\\n    .pivot(index = \"sex\", columns = \"day\", values = 'count') \\\n    .plot(kind = \"bar\", stacked = True)\nplt.xticks(rotation = 360)\n\n(array([0, 1]), [Text(0, 0, 'Male'), Text(1, 0, 'Female')])\n\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Total Observations\")\nplt.legend(loc = \"upper right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.2 Histogram\n\n4.5.2.1 One Continuous Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill))+\n    geom_histogram(binwidth = 2.25, fill = \"orangered3\") + \n    labs(x = \"Total Bill\", y = \"Count\") \n\n\n\n\n\n\n\n\n     The following code will generate the same results with a little modification -\n\ntips |&gt;\n    ggplot(aes(x = total_bill))+\n    geom_histogram(binwidth = 2.25, fill = \"orangered3\", col = \"white\") + \n    labs(x = \"Total Bill\", y = \"Count\")\n\n\n\n\nsns.histplot(data = tips, x = \"total_bill\", binwidth=2.25)\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.2.2 One Continuous and One Categorical Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, fill = sex))+\n    geom_histogram(binwidth = 2.25)+\n    labs(x = \"Total Bill\")\n\n\n\n\n\n\n\n\n     The following code will generate the same results -\n\ntips |&gt;\n    ggplot(aes(x = total_bill, color = sex))+\n    geom_histogram(binwidth = 2.25)\n\n\n\n\nsns.histplot(data = tips, x = \"total_bill\", hue = \"sex\", binwidth=2.25)\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Count\")\n\n\n\n\n\n\n\n\n\nsns.FacetGrid(data=tips, col=\"sex\") \\\n    .map(sns.histplot, \"total_bill\", binwidth = 2.25)\n\n\n\n\n\n\n\n4.5.3 Density Plot\n\n4.5.3.1 One Continuous Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill))+\n    geom_density( size = 1, color = \"orangered3\"\n        #adjust = 0.2\n    ) + \n    labs(x = \"Total Bill\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data = tips, x = \"total_bill\"\n            #,bw_adjust = 0.20\n            )\nplt.xlabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3.2 Two Continuous Variables\n\nRPython\n\n\n\ntips |&gt;\n    select(1:2) |&gt;\n    pivot_longer(cols = everything(), names_to = \"types\", values_to = \"values\") |&gt;\n    ggplot(aes(x = values, col = types))+\n    geom_density(size = 1)\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data = tips[['total_bill', 'tip']])\nplt.xlabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3.3 One Continuous Variable and One Categorical Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, fill = sex))+\n    geom_density(\n        #adjust = 0.2\n    )+ \n    labs(x = \"Total Bill\", y = \"Density\")\n\n\n\n\n\n\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, color = sex))+\n    geom_density(size = 1\n        #adjust = 0.2\n    )+ \n    labs(x = \"Total Bill\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data = tips, x = \"total_bill\", hue = \"sex\")\nplt.xlabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data = tips, x = \"total_bill\", hue = \"sex\", multiple = \"stack\")\nplt.xlabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.4 Point Plot\n\n4.5.4.1 One Categorical and One Continuous Variable\n\nRPython\n\n\n\ntips |&gt; \n    ggplot(aes(x = sex, y = total_bill, group = 1)) + \n    stat_summary(aes(sex, total_bill), geom = \"point\", fun.y = mean, size = 2, col = \"red\")+\n    stat_summary(aes(sex, total_bill), geom = \"line\", fun.y = mean, size = 1.5, col = \"red\",size = 2.1) + \n    labs(x = \"Sex\", y = \"Total Bill\")\n\n\n\n\nLine Plot of Gender (geom_line - mean)\n\n\n\n\n     The following code will also produce the same visualization.\n\ntips |&gt; \n    group_by(sex) |&gt;\n    summarize(total_bill = mean(total_bill)) |&gt;\n    ggplot(aes(x = sex, y = total_bill, group = 1)) + \n    geom_point(col = \"red\", size = 2)+\n    geom_line(col = \"red\", size = 2.1) + \n    labs(x = \"Sex\", y = \"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\nsns.pointplot(data = tips, x = \"sex\", y = \"total_bill\", errorbar=None)\nplt.xlabel('Sex')\nplt.ylabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.4.2 Two Categorical Variables and One Continuous Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = sex, y = total_bill, group = smoker, color = smoker)) + \n    stat_summary(aes(x = sex, y = total_bill), geom = \"point\", fun.y = mean) + \n    stat_summary(aes(x = sex, y = total_bill), geom = \"line\", fun.y = mean, size = 1.1) + \n    labs(x = \"Sex\", y = \"Total Bill\" #, color = \"Smoker\"\n    )\n\n\n\n\n\n\n\n\n     The following code will also produce the same visualization.\n\ntips |&gt;\n    group_by(sex, smoker) |&gt;\n    summarize( total_bill = mean(total_bill)) |&gt;\n    ggplot(aes(x = sex, y = total_bill, group = smoker , color = smoker)) + \n    geom_point()+\n    geom_line(size = 1.1)+\n    labs(x = \"Sex\", y = \"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\nsns.pointplot(data = tips, x = \"sex\", y = \"total_bill\", \n              hue = \"smoker\", errorbar= None)\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.5 Box Plot\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = sex, y = total_bill))+\n    geom_boxplot(fill = \"pink\") + \n    labs (x = \"Sex\", y = \"Total Bill\")\n\n\n\n\n\n\n\n\n\ntips |&gt;\n    ggplot(aes(x = sex, y = total_bill))+\n    geom_boxplot(fill = \"pink\") + \n    labs (x = \"Sex\", y = \"Total Bill\") + \n    facet_wrap(~smoker)\n\n\n\n\n\n\n\ntips |&gt;\n    ggplot(aes(x = sex, y = total_bill))+\n    geom_boxplot(fill = \"pink\") + \n    labs (x = \"Sex\", y = \"Total Bill\") + \n    facet_grid(time~smoker)\n\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(data = tips, x = \"sex\", y = \"total_bill\", color = \"pink\")\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\nsns.catplot(data = tips, x = \"sex\", y = \"total_bill\", \n            color = \"pink\", kind = \"box\", row = \"smoker\"\n           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsns.catplot(data = tips, x = \"sex\", y = \"total_bill\", \n            color = \"pink\", kind = \"box\", row = \"smoker\"\n            ,col = \"time\"\n           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.6 Scatter Plot\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, y = tip))+\n    geom_point(col = \"blue\")+\n    labs(x = \"Total Bill\", y = \"Tip\")\n\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = tips, x = \"total_bill\", y = \"tip\")\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.7 Regression Plot\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, y = tip))+\n    geom_point(col = \"blue\")+\n    geom_smooth(method = \"lm\", col = \"orange\") + \n    labs(x = \"Total Bill\", y = \"Tip\")\n\n\n\n\n\n\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, y = tip, col = sex))+\n    geom_point(col = \"blue\")+\n    geom_smooth(method = \"lm\") + \n    labs(x = \"Total Bill\", y = \"Tip\")\n\n\n\n\n\n\n\n\n\n\n\nsns.lmplot(data = tips, x = \"total_bill\", y = \"tip\")\n\n\n\n\n\n\n\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")\n\n\n\n\n\n\n\n\n\nsns.regplot(data = tips, x = \"total_bill\", y = \"tip\")\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")\n\n\n\n\n\n\n\n\n\nsns.lmplot(data = tips, x = \"total_bill\", y = \"tip\", hue = \"sex\")\n\n\n\n\n\n\n\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#exercises-01",
    "href": "visualization.html#exercises-01",
    "title": "4  Data Visualization",
    "section": "4.6 Exercises # 01",
    "text": "4.6 Exercises # 01\n\nDownload student data from the url and create a pointplot (lineplot) of students average math score (math.grade) of gender (gender). Please note that the variable gender includes a label called other in addition to M and F; you should filter out obsevations of the label other before you create visualization.\nFrom the dataset in above (question 1), compare, using pointplot (lineplot), the average math (math.grade) and science score (sciences.grade) of different students based on gender (gender). You might need to use pivot_longer function to reshape the data frame before visualizing the relation.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#interactive-visualization",
    "href": "visualization.html#interactive-visualization",
    "title": "4  Data Visualization",
    "section": "4.7 Interactive Visualization",
    "text": "4.7 Interactive Visualization\n\n     Interactive Visualization involves graphical presentation of data that permits users to engage with the visual elements directly. Unlike static visulization, interactive visualization allows users to manipulate data, explore different aspects, and customize the visualization in real time. The primary objective of interactive visualization is to make data exploration more intuititve and dynamic. The benefits of interactive visualzation include - enhaned engagement, deeper insights, customization, and exploration and discovery.\n\n\nRPythonPlotnine\n\n\n\nlibrary(plotly)\n\n\np = ggplot(data = tips, aes(x = sex)) + \n    geom_bar(width = 0.5, fill = \"orangered3\") + \n    labs(x = \"Gender\", y = \"Total Observations\")\n\nggplotly(p)\n\n\n\n\n\n\np2 = tips |&gt;\n    ggplot(aes(x = time, y = total_bill, group = smoker, color = smoker))+\n    stat_summary(aes(x = time, y = total_bill), geom = \"point\", fun.y = mean) + \n    stat_summary(aes(x = time, y = total_bill), geom = \"line\", fun.y = mean, size = 1.1) + \n    labs (x = \"Time\", y = \"Total Bill\")\n\nggplotly(p2)\n\n\n\n\n\n\n\n\nimport plotly.express as px\n\n\nfig = px.histogram(tips, x = \"sex\") \\\n    .update_traces(marker_color = \"orangered\") \\\n    .update_xaxes(title = \"Sex\") \\\n    .update_yaxes(title = \"Count\")\nfig.show()\n\n                        \n                                            \n\n\n\npx.histogram(tips, x = \"sex\", y = \"total_bill\",histfunc='avg') \\\n    .update_traces(marker_color = \"orangered\") \\\n    .update_xaxes(title = \"Sex\") \\\n    .update_yaxes(title = \"Average Total Bill\") \\\n    .show()\n\n                        \n                                            \n\n\n\npx.histogram(tips, x=\"total_bill\",histnorm='probability density',\n             width=600, height=400) \\\n                .update_xaxes(title = \"Total Bill\") \\\n                .update_yaxes(title =\"Density\")\n\n                        \n                                            \n\n\n\n\n\n#import plotnine as p9\nfrom plotnine import *\nimport plotly.tools as tls\n\ndf = tips.groupby([\"sex\"])[\"total_bill\"].agg('mean').reset_index()\n\n\n(\n    ggplot(df, aes(x = \"sex\", y = \"total_bill\", group = 1)) + \n    geom_point(color = \"blue\")+\n    geom_line(color = \"orange\", size = 1.1) + \n    labs(x = \"Sex\", y = \"Average Total Bill\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\nplotly_fig = (\n    ggplot(df, aes(x = \"sex\", y = \"total_bill\", group = 1)) + \n    geom_point(color = \"blue\")+\n    geom_line(color = \"orange\", size = 1.1)\n)\ntls.mpl_to_plotly(plotly_fig.draw()).show()\n\n                        \n                                            \n\n\n\ndf2 = tips.groupby([\"sex\", \"smoker\"])[\"total_bill\"] \\\n    .agg('mean') \\\n    .round(2) \\\n    .reset_index()\n\n(\n    ggplot(df2, aes(x = \"sex\", y = \"total_bill\",  group = \"smoker\", color = \"smoker\")) + \n    geom_point()+\n    geom_line(size = 1.1) + \n    labs(x = \"Sex\", y = \"Average Total Bill\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#exercises-02",
    "href": "visualization.html#exercises-02",
    "title": "4  Data Visualization",
    "section": "4.8 Exercises # 02",
    "text": "4.8 Exercises # 02",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html",
    "href": "dashboard.html",
    "title": "5  Dashboard for Visualization",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#what-is-dashboard",
    "href": "dashboard.html#what-is-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.1 What is Dashboard?",
    "text": "5.1 What is Dashboard?\n\n     A dashboard for visualization is a user interface that displays a collection of visual data representations, such as charts, graphs, tables, and metrics, to provide users with an interactive and comprehensive overview of key information. Dashboards are commonly used in business, data science, finance, healthcare, and other fields to monitor performance, track metrics, and explore data trends in real time.\n     The main goal of a dashboard is to present complex data in an easy-to-understand format, enabling users to quickly grasp insights and make data-driven decisions. Dashboards often combine multiple visual elements into a single screen or page, allowing users to view different aspects of the data simultaneously. They typically include interactive features like filters, drill-downs, and tooltips, which allow users to interact with the data and explore deeper insights without needing to understand the underlying data structures. Some benefits of dashboards are -\n\nData Aggregation: Dashboards bring together data from various sources, providing a unified view of different datasets.\nVisualization Elements: They use visual elements such as bar charts, line graphs, pie charts, heatmaps, and more to represent data in a visually appealing and informative way.\nInteractivity: Users can interact with the visual elements, applying filters, adjusting time frames, or drilling down into specific data points to gain more detailed insights.\nReal-Time Data: Dashboards can display real-time data, updating visualizations dynamically to reflect the latest information, which is especially useful for monitoring live systems or business performance.\nCustomization: Dashboards are highly customizable, allowing users to tailor the layout, visualizations, and data to meet their specific needs.\n\n     Dashboards for visualization are widely used for performance monitoring (e.g., tracking KPIs), data exploration (e.g., identifying trends), and decision-making (e.g., comparing metrics). They make data analysis more accessible to a wider audience by simplifying the complexity of raw data into clear and actionable insights.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#importance-of-dashboard",
    "href": "dashboard.html#importance-of-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.2 Importance of Dashboard",
    "text": "5.2 Importance of Dashboard",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#interactive-dashboard",
    "href": "dashboard.html#interactive-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.3 Interactive Dashboard",
    "text": "5.3 Interactive Dashboard\n     For interactive visualization, see Figure 5.3.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#quarto---r-and-python-dashboard",
    "href": "dashboard.html#quarto---r-and-python-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.4 Quarto - R and Python Dashboard",
    "text": "5.4 Quarto - R and Python Dashboard\n\n     Quarto Dashboard is a powerful and flexible open-source tool to create interactive dashboard in R or Python. Quarto dashboards are easy to create and support a wide variety of visualization and interactive components1. More about quarto dashboard can be learned from Quarto Dashboard Website. To learn more about interactivity on quarto dashboard using Shiny, please visit webpages for R and Python. However, it is recommended to use Python for quarto dashboard if you want to include interactive applications on your dashboard.\n     There are several components of quarto dashboard:\n\nNavigation Bar - Top page bar with icon, title of the dashboard, name of author and links to sub-pages\nPages, Rows, Columns, and Tabsets - Using markdown headings (#) - pages, rows and columns are defined. Tabsets are used to further divide the content within a row or column\nCards, Sidebar, and Toolbars - Cards are containers for text, images, charts, and interactive elements and useful for organizing information into distinct sections within a dashboard. Typically, the contents of cards map to cells in the dashboard. Sidebar is another layout component of quarto dashboard, which contain navigation menus, filters, and controls that allow users to adjust or explore the data presented in the main content. Toolbars ….\n\n     The first step to create a quarto dashboard is to structure YAML in .qmd file. A quarto dashboard YAML look like -\n\n\n\n\n\n\nFigure 5.1: Quarto Dashboard YAML\n\n\n\n     In quarto dashboard, each level 1 header (#) introduces a new page, each level 2 header (##) introduces a new row within the given page, and each code chunk within a given row introduces a new column.\n\n\n\n\n\n\nFigure 5.2: Quarto Dashboard Structure\n\n\n\n     Some other attributes (Table 5.1) that can be added to the quarto dashboard’s rows or columns include -\n\n\n\nTable 5.1: Some Additional Attributes\n\n\n\n\n\n\n\n\n\nAttribute\nExplanation\n\n\n\n\n{width=} and {height=}\nSet the size of columns, rows, and boxes\n\n\n{orientation=}\nsets the dashboard layout to either rows or columns. This is a global option set in the YAML. However, if your dashboard has multiple pages and you want to specify the orientation for each page, remove orientation: from the YAML and use this attribute instead\n\n\n{.tabset}\ndivide columns, rows, or charts into tabs\n\n\n{.sidebar}\ntypically creates a sidebar on the page\n\n\n{.hidden}\nexcludes a specific page from the navigation bar",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#vizro---python-dashboard",
    "href": "dashboard.html#vizro---python-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.5 Vizro - Python Dashboard",
    "text": "5.5 Vizro - Python Dashboard\n\n     Built on top of Dash and Plottly, Vizro is a powerful python module to create a dashboard. A vizro dashboard consists of several objects. The first object is Page. Each page contains several other sub-objects such as Comonents, which can include Graphs and Tables, Filters, which can be sliders, dropdown boxes and other buttons, and optional Actions. To learn more about Vizro, we can explore Vizro document website and developer website. The key benefits of Vizro include:\n\nLow-code and Configuration - Vizro only needs a few lines of code code, thus replacing thousand lines of codes and saving valuable time\nIn-built Best Practices - Vizro already incorporates standards for visual design and software development.\nSelf-service Visualization - Vizro readily assemble dashboards without advanced design or coding experience\nOptional High-code Extensions - Vizro enables limitless customization for advanced users with complex needs\nModularity - Vizro leverage components that are simple to swap, reuse, maintain, share and scale\nFlexibilit and Scalability - Vizro enables data science ready and to develop python based data visualization applications\n\n\n\n# Dashboard Creation \nfrom vizro import Vizro\nimport vizro.models as vm\nimport vizro.plotly.express as px\n\n# Data Visualization \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\n# loading dataset \nimport palmerpenguins\ndf = palmerpenguins.load_penguins()\n\n\n5.5.1 Standalone Page on Vizro Dashboard (Example # 01)\n\nVizro._reset()\n\nfirst_page = vm.Page(\n    title= \" \", \n    components= [\n        vm.Graph(\n            id = 'boxplot', \n            figure = px.box (df, x = 'species', y = 'bill_length_mm', \n            color = 'species', \n            labels={'species':'Species', 'bill_length_mm':'Bill Length (mm)'})\n        ),\n    ],\n)\n\ndashboard = vm.Dashboard(pages=[first_page])\nVizro().build(dashboard).run()\n\n\n\n\n        \n        \n\n\nFigure 5.3: A Sample of Vizro Dashboard\n\n\n\n\n\n\n5.5.2 Standalone Page on Vizro Dashboard (Example # 02)\n\n# loading dataset \ndf = px.data.gapminder()\ngapminder_data = (\n        df.groupby(by=[\"continent\", \"year\"]).\n            agg({\"lifeExp\": \"mean\", \"pop\": \"sum\", \"gdpPercap\": \"mean\"}).reset_index()\n    )\n\n\nVizro._reset()\nsecond_page = vm.Page(\n    title=\"First Page\",\n    components=[\n        vm.Card(\n            text=\"\"\"\n                # First dashboard page\n                This pages shows the inclusion of markdown text in a page and how components\n                can be structured using Layout.\n            \"\"\",\n        ),\n        vm.Graph(\n            figure=px.box(gapminder_data, x=\"continent\", y=\"lifeExp\", color=\"continent\",\n                            labels={\"lifeExp\": \"Life Expectancy\", \"continent\": \"Continent\"}),\n        ),\n        vm.Graph(\n            figure=px.line(gapminder_data, x=\"year\", y=\"gdpPercap\", color=\"continent\",\n                            labels={\"year\": \"Year\", \"continent\": \"Continent\",\n                            \"gdpPercap\":\"GDP Per Cap\"}, title=''),\n        ),\n\n    ],\n)\n\ndashboard2 = vm.Dashboard(pages=[second_page])\nVizro().build(dashboard2).run()\n\n\n\n5.5.3 Multiple Pages on Vizro Dashboard\n\nVizro._reset()\n\nthird_page = vm.Page(\n    title=\"First Page\",\n    layout=vm.Layout(grid=[[0, 0], [1, 2], [1, 2], [1, 2]]),\n    components=[\n        vm.Card(\n            text=\"\"\"\n                # First dashboard page\n                This pages shows the inclusion of markdown text in a page and how components\n                can be structured using Layout.\n            \"\"\",\n        ),\n        vm.Graph(\n            figure=px.box(gapminder_data, x=\"continent\", y=\"lifeExp\", color=\"continent\",\n                            labels={\"lifeExp\": \"Life Expectancy\", \"continent\": \"Continent\"}),\n        ),\n        vm.Graph(\n            figure=px.line(gapminder_data, x=\"year\", y=\"gdpPercap\", color=\"continent\",\n                            labels={\"year\": \"Year\", \"continent\": \"Continent\",\n                            \"gdpPercap\":\"GDP Per Cap\"}),\n            ),\n    ],\n)\n\ndashboard3 = vm.Dashboard(pages=[third_page])\nVizro().build(dashboard3).run()\n\n\nVizro._reset()\n\nsecond_page = vm.Page(\n    title=\"Second Page\",\n    components=[\n        vm.Card(\n            text=\"\"\"\n                # First dashboard page\n                This pages shows the inclusion of markdown text in a page and how components\n                can be structured using Layout.\n            \"\"\",\n        ),\n        vm.Graph(\n            figure=px.box(gapminder_data, x=\"continent\", y=\"lifeExp\", color=\"continent\",\n                            labels={\"lifeExp\": \"Life Expectancy\", \"continent\": \"Continent\"}),\n        ),\n        vm.Graph(\n            figure=px.line(gapminder_data, x=\"year\", y=\"gdpPercap\", color=\"continent\",\n                            labels={\"year\": \"Year\", \"continent\": \"Continent\",\n                            \"gdpPercap\":\"GDP Per Cap\"}, title=''),\n        ),\n\n    ],\n)\n\nthird_page = vm.Page(\n    title=\"Third Page\",\n    layout=vm.Layout(grid=[[0, 0], [1, 2], [1, 2], [1, 2]]),\n    components=[\n        vm.Card(\n            text=\"\"\"\n                # First dashboard page\n                This pages shows the inclusion of markdown text in a page and how components\n                can be structured using Layout.\n            \"\"\",\n        ),\n        vm.Graph(\n            figure=px.box(gapminder_data, x=\"continent\", y=\"lifeExp\", color=\"continent\",\n                            labels={\"lifeExp\": \"Life Expectancy\", \"continent\": \"Continent\"}),\n        ),\n        vm.Graph(\n            figure=px.line(gapminder_data, x=\"year\", y=\"gdpPercap\", color=\"continent\",\n                            labels={\"year\": \"Year\", \"continent\": \"Continent\",\n                            \"gdpPercap\":\"GDP Per Cap\"}),\n            ),\n    ],\n)\n\ndashboard4 = vm.Dashboard(pages=[second_page, third_page])\nVizro().build(dashboard4).run()",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#conclusions",
    "href": "dashboard.html#conclusions",
    "title": "5  Dashboard for Visualization",
    "section": "5.6 Conclusions",
    "text": "5.6 Conclusions",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#exercises",
    "href": "dashboard.html#exercises",
    "title": "5  Dashboard for Visualization",
    "section": "5.7 Exercises",
    "text": "5.7 Exercises\n\nCreate a dashboard from Adidas (Ticker:ADR) sales data (Adidas US Sales Datasets.csv).",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#footnotes",
    "href": "dashboard.html#footnotes",
    "title": "5  Dashboard for Visualization",
    "section": "",
    "text": "Shiny widgets and functionality can be incorporated in the quarto dashboard. Therefore, quarto dashboard is a powerful tool for creating interactive visualization.↩︎",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "data_management.html",
    "href": "data_management.html",
    "title": "6  Data Management in Accounting",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#data-management",
    "href": "data_management.html#data-management",
    "title": "6  Data Management in Accounting",
    "section": "6.1 Data Management",
    "text": "6.1 Data Management\n\n     Data management is the practice of collecting, keeping, and using data securely, efficiently, and cost-effectively. Data management is important for a variety of data-driven use cases including end-to-end business process execution, regulatory compliance, accurate analytics and AI, data migration, and digital transformation.\n     Managing digital data in an organization involves a broad range of tasks, policies, procedures, and practices. The work of data management has a wide scope, covering factors such as how to:\n\nCreate, access, and update data across a diverse data tier\nStore data across multiple clouds and on premises\nProvide high availability and disaster recovery\nUse data in a growing variety of apps, analytics, and algorithms\nEnsure data privacy and security\nArchive and destroy data in accordance with retention schedules and compliance requirements\n\n     Data management systems are built on data management platforms and can include databases, data lakes and data warehouses, big data management systems, data analytics, and more.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#relational-database",
    "href": "data_management.html#relational-database",
    "title": "6  Data Management in Accounting",
    "section": "6.2 Relational Database",
    "text": "6.2 Relational Database\n\n     Relational databases organize data into rows and columns, which form a table and different tables are connected to each other usign either primary key or foreign key. Here’s a simple example of two tables a small business might use to process orders for its products. The first table is a customer info table, so each record includes a customer’s name, address, shipping and billing information, phone number, and other contact information. Each bit of information (each attribute) is in its own column, and the database assigns a unique ID (a key) to each row. In the second table—a customer order table—each record includes the ID of the customer that placed the order, the product ordered, the quantity, the selected size and color, and so on—but not the customer’s name or contact information.\n     These two tables have only one thing in common: the ID column (the key). But because of that common column, the relational database can create a relationship between the two tables. Then, when the company’s order processing application submits an order to the database, the database can go to the customer order table, pull the correct information about the product order, and use the customer ID from that table to look up the customer’s billing and shipping information in the customer info table. The warehouse can then pull the correct product, the customer can receive timely delivery of the order, and the company can get paid.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#relational-database-management-systems-rdbms",
    "href": "data_management.html#relational-database-management-systems-rdbms",
    "title": "6  Data Management in Accounting",
    "section": "6.3 Relational Database Management Systems (RDBMS)",
    "text": "6.3 Relational Database Management Systems (RDBMS)\n\n     While a relational database organizes data based off a relational data model, a relational database management system (RDBMS) is a more specific reference to the underlying database software that enables users to maintain it. These programs allow users to create, update, insert, or delete data in the system, and they provide:\n\nData structure\nMulti-user access\nPrivilege control\nNetwork access\n\n     Examples of popular RDBMS systems include MySQL, PostgreSQL, and IBM DB2. Additionally, a relational database system differs from a basic database management system (DBMS) in that it stores data in tables while a DBMS stores information as files.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#creating-a-relational-database-using-postgresql",
    "href": "data_management.html#creating-a-relational-database-using-postgresql",
    "title": "6  Data Management in Accounting",
    "section": "6.4 Creating a Relational Database Using PostgreSQL",
    "text": "6.4 Creating a Relational Database Using PostgreSQL\n\n     PostgreSQL is a widely used open-source Relational Database Management System (RDBMS). Since its release in 1996, PostgreSQL has carved a niche as one of the most robust and reliable database systems in the Tech industry. PostgreSQL is known for its rich feature set, reliability, scalability, and strong community support network.\n\n\n6.4.1 Installing PostgreSQL\n\n     To install PostgreSQL locally on your computer, visit the installer by EDB, and download the newest version compatible with your operating system. To install PostgreSQL on Windows, you need to have administrator privileges. Below, we delineate the steps to install the PostgreSQL in windows.\n     Step 01: When the downloading is complete, double click the downloaded file and an installation wizard will appear and guide you through multiple steps. Click “Next” to specify directory.\n\n\n\nInstallation Wizard\n\n\n     Step 02: You can specify the location of PostgreSQL, or go with the default choice\n\n\n\nInstallation Directory\n\n\n     Step 03: To use PostgreSQL, you need to install PostgreSQL server. For our installation and uses purposes, we select the following servers - PostgreSQL Server, pgAdmin4, and Command Line Tools.\n\n\n\nServer Selection\n\n\n     Step 04: Then, we need to choose the storage directory. Alternatively, we can go with the default directory.\n\n\n\nStorage Directory\n\n\n     Step 05: At this point, you need to choose your passowrd. It is important to note that the password will be used to connect to the database. Therefore, you must remember your password.\n\n\n\nPassword Selection\n\n\n     Step 06: Next you need to select the port. The default port number is 5432. If you change the port, you also need to remember the port number because like passowrd, you need to use the port number to connect to the database.\n\n\n\nPort Selection\n\n\n     Step 07: We also need to select the geographical location of the database server. We can go with the default choice.\n\n\n\nLocale Selection\n\n\nThen if everything looks ok, you can move forward by clicking ‘Next’. However, it is recommended that you copy and save the information from the “Pre Installation Summary” in some safe place so that you can access them in future, if necessary.\n\n\n\nPre Installation Summary\n\n\n     Step 08: Then you can click “Next” in ‘Ready to Install’ windows and the installation process will start and it will take a while to finish the installation. Once the installing is done, please click ‘Finish’ button on ‘Complete the PostgreSQL Setup Wizard’ window.\n\n\n\nReady to Install\n\n\n\n\n\nInstalling\n\n\n\n\n\nComplete the PostgreSQL Setup Wizard\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSome good sources to learn more about PostgreSQL -\n\nW3 Schools\nPostgreSQL Tutorial",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#connecting-to-the-relational-database",
    "href": "data_management.html#connecting-to-the-relational-database",
    "title": "6  Data Management in Accounting",
    "section": "6.5 Connecting to the Relational Database",
    "text": "6.5 Connecting to the Relational Database\n\n     In Section 6.4.1, we described the steps of installing PostgreSQL database; now, we will try to connect to that database. There are several ways to connect the database we created. These ways include -\n\nSQL Shell (psql)\nCommand Shell (cmd)\npgAdmin 4\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo connect to PostgreSQL Database using pgAdmin 4, pleae use the following link -\n\nConnecting using pgAdmin 4\n\n\n\n\n\n6.5.1 Connection using SQL Shell (psql)\n\n      First, we will discuss SQL Shell (psql) method. Basically, SQL Shell (psql) is a terminal based program where you can write and execute SQL syntax in the command-line terminal. You will find the SQL Shell (psql) in the start menu under PostgreSQL. If you do not find it please search “psql” on windows search box.\n\n\n\nSQL Shell on Start Menu\n\n\n     Once the program is open, you should be able to see a window like below. Usually, the name of the server is localhost as shown below; however, if you choose a different server while installing the PostgreSQL, you must mention your server name. Then, click “enter” on your keyboard.\n\n\n\nServer\n\n\n     The default name of your database is postgres as shown below. If you choose a different name, please enter that name. Then, click “enter” on your keyboard.\n\n\n\nDatabase\n\n\n     The default port number is 5432. If you use the default port number, then please click “enter” on your keyboard. Otherwise, use the port number that you used while installing PostgreSQL and click “enter” on your keyboard.\n\n\n\nPort\n\n\n     Similarly, the default user name is postgres. Please click “enter” on your keyboard if your default name is the same.\n\n\n\nUsername\n\n\n     Next, you need to provide password that you set while installing the database.\n\n\n\nPassword\n\n\n\n\n\n6.5.2 Connection using Command Shell (cmd)\n\n     To use the command prompt (cmd) to connect to PostgreSQL database, you need to use the following steps -\n\nStep # 01 - Open command prompt by writing cmd in windows search bar as shown below. Then command prompt will open.\n\n\n\n\nSearch Bar on Windows\n\n\n\nStep # 02 - type cd C:\\Program Files\\PostgreSQL\\16\\bin on the command prompt and hit enter on the keyboard\nStep # 03 - type psql -h localhost -p 5432 -d postgres -U postgres\nStep # 04 - Now provide the password that you set while installing PostgreSQL\n\nThese steps should ensure that you are connected to the database.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#sec-dealdatabase",
    "href": "data_management.html#sec-dealdatabase",
    "title": "6  Data Management in Accounting",
    "section": "6.6 Dealing with the Database",
    "text": "6.6 Dealing with the Database\n\n     Once the database is created, new tables can be created. Also, different kinds of data manipulation can be performed. For example, after being connected to the database, one should check the database available by running the following code -\n\n\\l \n\n# or \n\n\\list\n\n     To connect to a database, one should run the following code -\n\n\\c name_of_the_database;\n\n     To see all tables in a database, one should run the following code -\n\n\\dt;\n\n\n\n6.6.1 Creating a Database and Tables inside the Database\n\n     To create a database (e.g., my_database), one can run the following code -\n\nCREATE DATABASE my_database;\n\n     Then, you need to connect to the database (here my_database) by running the following code -\n\n\\c my_database;\n\n     Once you run the above code, you will be connected to my_database.\n     Next, in order to create a table, following code can be run -\n\n# we are going to create a table called - People\n\nCREATE TABLE People (\n pid int not null,\n prefix text,\n firstName text,\n lastName text,\n suffix text,\n homeCity text,\n DOB date,\n primary key(pid)\n);\n\n     Similarly, four more tables are created by running the following code -\n\n# Table - Customer \n\nCREATE TABLE Customers (\n pid int not null references People(pid),\n paymentTerms text,\n discountPct decimal(5,2),\n primary key(pid)\n);\n\n# Table - Agent \n\nCREATE TABLE Agents (\n pid int not null references People(pid),\n paymentTerms text,\n commissionPct decimal(5,2),\n primary key(pid)\n);\n\n# Table - Products \n\nCREATE TABLE Products (\n prodId char(3) not null,\n name text,\n city text,\n qtyOnHand int,\n priceUSD numeric(10,2),\n primary key(prodId)\n);\n\n# Table - Orders \n\nCREATE TABLE Orders (\n orderNum int not null,\n dateOrdered date not null,\n custId int not null references Customers(pid),\n agentId int not null references Agents(pid),\n prodId char(3) not null references Products(prodId),\n  quantityOrdered integer,\n totalUSD numeric(12,2),\n primary key(orderNum)\n);\n\n     Next, we will populate the tables with different kinds of data.\n\n# Inserting records into table - People \n\nINSERT INTO People (pid, prefix, firstName, lastName, suffix, homeCity, DOB)\nVALUES\n (001, 'Dr.', 'Neil', 'Peart', 'Ph.D.', 'Toronto', '1952-09-12'),\n (002, 'Ms.', 'Regina', 'Schock', NULL, 'Toronto', '1957-08-31'),\n (003, 'Mr.', 'Bruce', 'Crump', 'Jr.', 'Jacksonville', '1957-07-17'),\n (004, 'Mr.', 'Todd', 'Sucherman', NULL, 'Chicago', '1969-05-02'),\n (005, 'Mr.', 'Bernard', 'Purdie', NULL, 'Teaneck', '1939-06-11'),\n (006, 'Ms.', 'Demetra', 'Plakas', 'Esq.', 'Santa Monica', '1960-11-09'),\n (007, 'Ms.', 'Terri Lyne', 'Carrington', NULL, 'Boston', '1965-08-04'),\n (008, 'Dr.', 'Bill', 'Bruford', 'Ph.D.', 'Kent', '1949-05-17'),\n (009, 'Mr.', 'Alan', 'White', 'III', 'Pelton', '1949-06-14')\n;\n\n# Inserting records into table - Customers \n\nINSERT INTO Customers (pid, paymentTerms, discountPct)\nVALUES\n (001, 'Net 30' , 21.12),\n (004, 'Net 15' , 4.04),\n (005, 'In Advance', 5.50),\n (007, 'On Receipt', 2.00),\n (008, 'Net 30' , 10.00)\n;\n\n# Inserting records into table - Agents \n\nINSERT INTO Agents (pid, paymentTerms, commissionPct)\nVALUES\n (002, 'Quarterly', 5.00),\n (003, 'Annually', 10.00),\n (005, 'Monthly', 2.00),\n (006, 'Weekly', 1.00)\n;\n\n# Inserting records into table - Products \n\nINSERT INTO Products( prodId, name, city, qtyOnHand, priceUSD )\nVALUES\n('p01', 'Heisenberg Compensator', 'Dallas', 47,  67.50),\n('p02', 'Universal Translator', 'Newark', 2399, 5.50 ),\n('p03', 'Commodore PET', 'Duluth', 1979, 65.02 ),\n('p04', 'LCARS module', 'Duluth', 3, 47.00 ),\n('p05', 'Remo drumhead', 'Dallas', 8675309, 16.61 ),\n('p06', 'Trapper Keeper', 'Dallas', 1982, 2.00 ),\n('p07', 'Flux Capacitor', 'Newark', 1007, 1.00 ),\n('p08', 'HAL 9000 memory core', 'Newark', 200, 1.25 ),\n('p09', 'Red Barchetta',  'Toronto', 1, 379000.47 )\n;\n\n\n# Inserting records into table - Orders \n\nINSERT INTO Orders(orderNum, dateOrdered, custId, agentId, prodId, quantityOrdered, totalUSD)\nVALUES\n(1011, '2020-01-23', 001, 002, 'p01', 1100, 58568.40),\n(1012, '2020-01-23', 004, 003, 'p03', 1200, 74871.83),\n(1015, '2020-01-23', 005, 003, 'p05', 1000, 15696.45),\n(1016, '2020-01-23', 008, 003, 'p01', 1000, 60750.00),\n(1017, '2020-02-14', 001, 003, 'p03', 500, 25643.88),\n(1018, '2020-02-14', 001, 003, 'p04', 600, 22244.16),\n(1019, '2020-02-14', 001, 002, 'p02', 400, 1735.36),\n(1020, '2020-02-14', 004, 005, 'p07', 600, 575.76),\n(1021, '2020-02-14', 004, 005, 'p01', 1000, 64773.00),\n(1022, '2020-03-15', 001, 003, 'p06', 450, 709.92),\n(1023, '2020-03-15', 001, 002, 'p05', 500, 6550.984),\n(1024, '2020-03-15', 005, 002, 'p01', 880, 56133.00),\n(1025, '2020-04-01', 008, 003, 'p07', 888, 799.20),\n(1026, '2020-05-01', 008, 005, 'p03', 808, 47282.54)\n;\n\n     Now to see whether the database contains the tables we just created above, you can run the following code -\n\n\\dt\n\n\n\n\n6.6.2 Querying the Database\n\n     Once the database is created, we need to retrieve data from the database for which we need to write queries. For example, we want retireve prodid, name, and city from the table products. Then, we need to write the following query -\n\n# SQL Query = \nSELECT prodid, name, city FROM products;\n\n# Output \n\n prodid |          name          |  city\n--------+------------------------+---------\n p01    | Heisenberg Compensator | Dallas\n p02    | Universal Translator   | Newark\n p03    | Commodore PET          | Duluth\n p04    | LCARS module           | Duluth\n p05    | Remo drumhead          | Dallas\n p06    | Trapper Keeper         | Dallas\n p07    | Flux Capacitor         | Newark\n p08    | HAL 9000 memory core   | Newark\n p09    | Red Barchetta          | Toronto\n(9 rows)\n\n     Similarly, if we want to collect all fields from products table for the city Dallas, then we can write the following queries -\n\n# SQL Query = \nSELECT * FROM products WHERE city = 'Dallas';\n\n\n# Output \n\n prodid |          name          |  city  | qtyonhand | priceusd\n--------+------------------------+--------+-----------+----------\n p01    | Heisenberg Compensator | Dallas |        47 |    67.50\n p05    | Remo drumhead          | Dallas |   8675309 |    16.61\n p06    | Trapper Keeper         | Dallas |      1982 |     2.00\n(3 rows)\n\n     If you need to quit the database, then run the following line of code -\n\n\\q\n\n\n\n\n6.6.3 Saving a Database from PostgreSQL\n\n     In order to share a database with stakeholders, we need to save the a particular database. Therefore, knowing how to save the database is very important. We must use the Command Prompt (cmd) to save a database. First, we need to open a command prompt following step # 01 in Section 6.5.2. Then we need to write the following codes on command prompt -\n\n# First line of code and then hit enter on the Keyboard\ncd ..\n# Second line of code and then hit enter on the Keyboard\ncd ..\n# Third line of code and then hit enter on the Keyboard\ncd \\Program Files\\PostgreSQL\\16\\bin\n# Fourth line of code and then hit enter on the Keyboard\npg_dump -h localhost -d name_of_your_database -U postgres -p 5433 -F tar &gt;K:\\name_of_your_database.tar\n# Fifth Line of code - provide your password\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can watch the youtube video to learn more about how to save a database from PostgreSQL - https://www.youtube.com/watch?v=sa5VXDG_aW8\n\n\n\n     In the fourth line of code above - pg_dump -h localhost -d name_of_your_database -U postgres -p 5433 -F tar &gt;K:\\name_of_your_database.tar- the “name_of_your_database” is the database name that you want to save to share and “K:\\\\” is the directory (folder) address (path) on which you want to save your database and “name_of_your_database” is the name of the database that you would like to assign to the saved database and “.tar” is the file extension. It is important to note that when you run the fourth line of code above, it might show the message - “Access is denied”. In such situation, you should change the path (address) of the directory in which you would like to save the database.\n\n\n\n6.6.4 Uploading a Database to PostgreSQL\n\n     Sometimes, you might need to upload a database to PostgreSQL. Therefore, you should know how you can upload the database to PostgreSQL. First, you should create a database following the process as described in Section 6.6.1. For example, we create a database called my_database in PostgreSQL. Assume that the database that we want to upload to PostgreSQL is in the path - K:\\ and the name of the database is my_database_upload. Therefore, the complete path of the database to be uploaded is - “K:\\my_database_upload.tar”. Then, you should open a command prompt as described in Section 6.5.2. Then run the following code -\n\n# First line of code and then hit enter on the Keyboard\ncd ..\n# Second line of code and then hit enter on the Keyboard\ncd ..\n# Third line of code and then hit enter on the Keyboard\ncd \\Program Files\\PostgreSQL\\16\\bin\n# Fourth line of code and then hit enter on the Keyboard\npg_restore -h localhost -p 5433 -d my_database -U postgres -v \"K:\\my_database_upload.tar\"\n# Fifth Line of code - provide your password\n\n     In the fourth line of code above - pg_restore -h localhost -p 5433 -d my_database -U postgres -v \"K:\\my_database_upload.tar\"- “my_database” is the name of the database that you created on the PostgreSQL and “K:\\my_database_upload.tar” is the path of the database to be uploaded.\n     Once you upload the database to the PostgreSQL , you can check whether the database is uploaded by following Section 6.6.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#querying-postgresql-database-using-r",
    "href": "data_management.html#querying-postgresql-database-using-r",
    "title": "6  Data Management in Accounting",
    "section": "6.7 Querying PostgreSQL Database Using R",
    "text": "6.7 Querying PostgreSQL Database Using R\n\n# Importing Necessary Packages\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(askpass)\nlibrary(tidyverse)\n\n     In order to connect to the local postgreSQL database, you can run the following code. However, there is a problem that if you publish this connection code, your password will be shared; therefore, it is better not to share the password.\n\n# Connecting to PostgreSQL Database \ncon = dbConnect(RPostgres::Postgres()\n                 , host='localhost'\n                 , port='5433'\n                 , dbname='my_database',\n                 , user='postgres'\n                 , password=\"YourPassword\") \n\n     For the below code, first we set the password in environment variable in R environment. Since I use IDE Positron (or VS Code), I get access to the environment by running the following code. Then, the password is stored by running the code - POSTGRESS_PASSWORD=\"YOURPASSWORD\". Replace, the word “YOURPASSWORD” with your own password and save the file.\n\nlibrary(usethis)\nusethis::edit_r_environ()\n\n\n# Connecting to PostgreSQL Database \ncon = dbConnect(RPostgres::Postgres()\n                 , host='localhost'\n                 , port='5433'\n                 , dbname='my_database',\n                 , user='postgres'\n                 , password= Sys.getenv(\"POSTGRESS_PASSWORD\")\n                 )\n\n     Alternatively, you can use askpass package to input your password without sharing your password with third party.\n\n# Connecting to PostgreSQL Database \ncon = dbConnect(RPostgres::Postgres()\n                 , host='localhost'\n                 , port='5433'\n                 , dbname='my_database',\n                 , user='postgres'\n                 , password=askpass::askpass(\"Enter your database password (R):\")\n                 )\n\n\n# Checking all Tables in the Database \ndbListTables(con)\n\n[1] \"agents\"    \"customers\" \"orders\"    \"people\"    \"products\" \n\n\n\n# To see a table \ncon %&gt;%\n  tbl ('orders') \n\n# Source:   table&lt;\"orders\"&gt; [?? x 7]\n# Database: postgres  [postgres@localhost:5433/my_database]\n   ordernum dateordered custid agentid prodid quantityordered totalusd\n      &lt;int&gt; &lt;date&gt;       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;            &lt;int&gt;    &lt;dbl&gt;\n 1     1011 2020-01-23       1       2 p01               1100   58568.\n 2     1012 2020-01-23       4       3 p03               1200   74872.\n 3     1015 2020-01-23       5       3 p05               1000   15696.\n 4     1016 2020-01-23       8       3 p01               1000   60750 \n 5     1017 2020-02-14       1       3 p03                500   25644.\n 6     1018 2020-02-14       1       3 p04                600   22244.\n 7     1019 2020-02-14       1       2 p02                400    1735.\n 8     1020 2020-02-14       4       5 p07                600     576.\n 9     1021 2020-02-14       4       5 p01               1000   64773 \n10     1022 2020-03-15       1       3 p06                450     710.\n# ℹ more rows\n\n\n\ncon %&gt;%\n  tbl ('orders') %&gt;%\n  collect ()\n\n# A tibble: 14 × 7\n   ordernum dateordered custid agentid prodid quantityordered totalusd\n      &lt;int&gt; &lt;date&gt;       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;            &lt;int&gt;    &lt;dbl&gt;\n 1     1011 2020-01-23       1       2 p01               1100   58568.\n 2     1012 2020-01-23       4       3 p03               1200   74872.\n 3     1015 2020-01-23       5       3 p05               1000   15696.\n 4     1016 2020-01-23       8       3 p01               1000   60750 \n 5     1017 2020-02-14       1       3 p03                500   25644.\n 6     1018 2020-02-14       1       3 p04                600   22244.\n 7     1019 2020-02-14       1       2 p02                400    1735.\n 8     1020 2020-02-14       4       5 p07                600     576.\n 9     1021 2020-02-14       4       5 p01               1000   64773 \n10     1022 2020-03-15       1       3 p06                450     710.\n11     1023 2020-03-15       1       2 p05                500    6551.\n12     1024 2020-03-15       5       2 p01                880   56133 \n13     1025 2020-04-01       8       3 p07                888     799.\n14     1026 2020-05-01       8       5 p03                808   47283.\n\n\n\n# Selecting some columns \ncon %&gt;%\n  tbl ('orders') %&gt;%\n  select (ordernum, dateordered, custid, agentid)\n\n# Source:   SQL [?? x 4]\n# Database: postgres  [postgres@localhost:5433/my_database]\n   ordernum dateordered custid agentid\n      &lt;int&gt; &lt;date&gt;       &lt;int&gt;   &lt;int&gt;\n 1     1011 2020-01-23       1       2\n 2     1012 2020-01-23       4       3\n 3     1015 2020-01-23       5       3\n 4     1016 2020-01-23       8       3\n 5     1017 2020-02-14       1       3\n 6     1018 2020-02-14       1       3\n 7     1019 2020-02-14       1       2\n 8     1020 2020-02-14       4       5\n 9     1021 2020-02-14       4       5\n10     1022 2020-03-15       1       3\n# ℹ more rows\n\n\n\n# Filtering some rows\ncon %&gt;% \n  tbl ('products') %&gt;%\n  filter (city == \"Dallas\")\n\n# Source:   SQL [3 x 5]\n# Database: postgres  [postgres@localhost:5433/my_database]\n  prodid name                   city   qtyonhand priceusd\n  &lt;chr&gt;  &lt;chr&gt;                  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;\n1 p01    Heisenberg Compensator Dallas        47     67.5\n2 p05    Remo drumhead          Dallas   8675309     16.6\n3 p06    Trapper Keeper         Dallas      1982      2  \n\n\n\n# showing the SQL query\ncon %&gt;% \n  tbl ('products') %&gt;%\n  filter (city == \"Dallas\") %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT \"products\".*\nFROM \"products\"\nWHERE (\"city\" = 'Dallas')\n\n\n\n# Disconnecting the database \ncon %&gt;% dbDisconnect ()",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#querying-postgresql-database-using-python",
    "href": "data_management.html#querying-postgresql-database-using-python",
    "title": "6  Data Management in Accounting",
    "section": "6.8 Querying PostgreSQL Database Using Python",
    "text": "6.8 Querying PostgreSQL Database Using Python\n\n# Importing Necessary Python Packages \nimport ibis\nimport getpass\nimport os \n\n\n# To know more about the next line of code\n# see - https://ibis-project.org/tutorials/ibis-for-dplyr-users\nibis.options.interactive = True\n\n\n# to connect to the database \nconn = ibis.postgres.connect(\n    user='postgres', \n    password = 'YourPassword',\n    host = \"localhost\",\n    port = 5433,\n    database = \"my_database\" # here our database name will be 'dvdrental'\n)\n\n     Like before, if you do not share your password, please get the password from environment.\n\n# to connect to the database \nconn = ibis.postgres.connect(\n    user='postgres', \n    password = os.getenv(\"POSTGRESS_PASSWORD\"),\n    host = \"localhost\",\n    port = 5433,\n    database = \"dvdrental\"\n)\n\n     If you do not know how to set the password in python environment, please follow these steps -\n\nCrate a .env file in your working directory. You can create the .env file using notepad, but not that the extension of the file .txt must be deleted. Once the .env file is created, then save your password as you did in R enviornment. For example - save the line - POSTGRESS_PASSWORD=\"YOURPASSWORD\", where replace the word YOURPASSWORD with your own password.\nThen, if necessary, please install python package python-dotenv by running the code in terminal - pip install python-dotenv. Once the package is installed, please run the following code to get connected to your local postgreSQL database.\n\n\nfrom dotenv import load_dotenv\n# load environment variable from .env file \nload_dotenv()\n\n# Connect to your PostgreSQL database \nconn = ibis.postgres.connect(\n    user='postgres',\n    password=os.getenv('POSTGRESS_PASSWORD'),\n    host='localhost',\n    port=5433,\n    database='my_database'\n)\n\n     Alternatively, you can use getpass python package, which is very similar to askpass package in R.\n\n# to connect to the database \nconn = ibis.postgres.connect(\n    user='postgres', \n    password = getpass.getpass(\"Enter your database password (Python): \"),\n    host = \"localhost\",\n    port = 5433,\n    database = \"dvdrental\"\n)\n\n\n# See all tables in a database \nconn.list_tables()\n\n['actor', 'actor_info', 'address', 'category', 'city', 'country', 'customer', 'customer_list', 'film', 'film_actor', 'film_category', 'film_list', 'inventory', 'language', 'nicer_but_slower_film_list', 'payment', 'rental', 'sales_by_film_category', 'sales_by_store', 'staff', 'staff_list', 'store']\n\n\n\n# To know the total number of rows \nconn.table('actor').count() # total rows \n\n\n\n\n\nconn.table('actor').columns # name of the columns \n\n['actor_id', 'first_name', 'last_name', 'last_update']\n\n\n\n# To see the SQL command \nconn.table('actor').compile()\n\n'SELECT * FROM \"actor\"'\n\n\n\n# Dealing with a table \nconn.table('country')\n\n┌────────────┬────────────────┬─────────────────────┐\n│ country_id │ country        │ last_update         │\n├────────────┼────────────────┼─────────────────────┤\n│ !int32     │ !string        │ !timestamp(6)       │\n├────────────┼────────────────┼─────────────────────┤\n│          1 │ Afghanistan    │ 2006-02-15 09:44:00 │\n│          2 │ Algeria        │ 2006-02-15 09:44:00 │\n│          3 │ American Samoa │ 2006-02-15 09:44:00 │\n│          4 │ Angola         │ 2006-02-15 09:44:00 │\n│          5 │ Anguilla       │ 2006-02-15 09:44:00 │\n│          6 │ Argentina      │ 2006-02-15 09:44:00 │\n│          7 │ Armenia        │ 2006-02-15 09:44:00 │\n│          8 │ Australia      │ 2006-02-15 09:44:00 │\n│          9 │ Austria        │ 2006-02-15 09:44:00 │\n│         10 │ Azerbaijan     │ 2006-02-15 09:44:00 │\n│          … │ …              │ …                   │\n└────────────┴────────────────┴─────────────────────┘\n\nconn.table('country').head(5).execute()\n\n   country_id         country         last_update\n0           1     Afghanistan 2006-02-15 09:44:00\n1           2         Algeria 2006-02-15 09:44:00\n2           3  American Samoa 2006-02-15 09:44:00\n3           4          Angola 2006-02-15 09:44:00\n4           5        Anguilla 2006-02-15 09:44:00\n\n\n\n## describe () or columns () equivalent to glimpse ()\nconn.table('country').describe\n\n&lt;bound method Table.describe of ┌────────────┬────────────────┬─────────────────────┐\n│ country_id │ country        │ last_update         │\n├────────────┼────────────────┼─────────────────────┤\n│ !int32     │ !string        │ !timestamp(6)       │\n├────────────┼────────────────┼─────────────────────┤\n│          1 │ Afghanistan    │ 2006-02-15 09:44:00 │\n│          2 │ Algeria        │ 2006-02-15 09:44:00 │\n│          3 │ American Samoa │ 2006-02-15 09:44:00 │\n│          4 │ Angola         │ 2006-02-15 09:44:00 │\n│          5 │ Anguilla       │ 2006-02-15 09:44:00 │\n│          6 │ Argentina      │ 2006-02-15 09:44:00 │\n│          7 │ Armenia        │ 2006-02-15 09:44:00 │\n│          8 │ Australia      │ 2006-02-15 09:44:00 │\n│          9 │ Austria        │ 2006-02-15 09:44:00 │\n│         10 │ Azerbaijan     │ 2006-02-15 09:44:00 │\n│          … │ …              │ …                   │\n└────────────┴────────────────┴─────────────────────┘&gt;\n\n\n\n# filtering rows    \nconn.table('country')[conn.table('country')['country']==\"Zambia\"] \\\n    .execute()\n\n   country_id country         last_update\n0         109  Zambia 2006-02-15 09:44:00\n\n\n\n# Selecting Columns \nconn.table('country')[[\"country_id\", \"country\"]]\n\n┌────────────┬────────────────┐\n│ country_id │ country        │\n├────────────┼────────────────┤\n│ !int32     │ !string        │\n├────────────┼────────────────┤\n│          1 │ Afghanistan    │\n│          2 │ Algeria        │\n│          3 │ American Samoa │\n│          4 │ Angola         │\n│          5 │ Anguilla       │\n│          6 │ Argentina      │\n│          7 │ Armenia        │\n│          8 │ Australia      │\n│          9 │ Austria        │\n│         10 │ Azerbaijan     │\n│          … │ …              │\n└────────────┴────────────────┘\n\n\n\n# Arrange \n\nconn.table('rental') \\\n    .order_by(['inventory_id'])\n\n┌───────────┬─────────────────────┬──────────────┬─────────────┬───┐\n│ rental_id │ rental_date         │ inventory_id │ customer_id │ … │\n├───────────┼─────────────────────┼──────────────┼─────────────┼───┤\n│ !int32    │ !timestamp(6)       │ !int32       │ !int16      │ … │\n├───────────┼─────────────────────┼──────────────┼─────────────┼───┤\n│      4863 │ 2005-07-08 19:03:15 │            1 │         431 │ … │\n│     11433 │ 2005-08-02 20:13:10 │            1 │         518 │ … │\n│     14714 │ 2005-08-21 21:27:43 │            1 │         279 │ … │\n│       972 │ 2005-05-30 20:21:07 │            2 │         411 │ … │\n│      2117 │ 2005-06-17 20:24:00 │            2 │         170 │ … │\n│      4187 │ 2005-07-07 10:41:31 │            2 │         161 │ … │\n│      9449 │ 2005-07-30 22:02:34 │            2 │         581 │ … │\n│     15453 │ 2005-08-23 01:01:01 │            2 │         359 │ … │\n│     10126 │ 2005-07-31 21:36:07 │            3 │          39 │ … │\n│     15421 │ 2005-08-22 23:56:37 │            3 │         541 │ … │\n│         … │ …                   │            … │           … │ … │\n└───────────┴─────────────────────┴──────────────┴─────────────┴───┘\n\n\n\n# Group By\nconn.table('payment')[['payment_id', 'customer_id', 'staff_id', 'amount']] \\\n    .execute() \\\n    .groupby('customer_id').agg({'amount': 'mean'}) \\\n    .reset_index() \n\n     customer_id    amount\n0              1  3.823333\n1              2  4.759231\n2              3  5.448333\n3              4  3.717273\n4              5  3.847143\n..           ...       ...\n594          595  3.817586\n595          596  3.353636\n596          597  3.816087\n597          598  3.808182\n598          599  4.378889\n\n[599 rows x 2 columns]\n\n\n\n# To disconnect the database \nconn.disconnect()",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#online-analytical-processing-olap-database-management-system",
    "href": "data_management.html#online-analytical-processing-olap-database-management-system",
    "title": "6  Data Management in Accounting",
    "section": "6.9 Online Analytical Processing (OLAP) Database Management System",
    "text": "6.9 Online Analytical Processing (OLAP) Database Management System",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#conclusion",
    "href": "data_management.html#conclusion",
    "title": "6  Data Management in Accounting",
    "section": "6.10 Conclusion",
    "text": "6.10 Conclusion",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "predictive.html",
    "href": "predictive.html",
    "title": "7  Predictive Modeling - Linear Regression",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling - Linear Regression</span>"
    ]
  },
  {
    "objectID": "predictive.html#introduction",
    "href": "predictive.html#introduction",
    "title": "7  Predictive Modeling - Linear Regression",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\n     Discussion on predictive analytics.",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling - Linear Regression</span>"
    ]
  },
  {
    "objectID": "predictive.html#regression-analysis",
    "href": "predictive.html#regression-analysis",
    "title": "7  Predictive Modeling - Linear Regression",
    "section": "7.2 Regression Analysis",
    "text": "7.2 Regression Analysis\n\n7.2.1 Simple Linear Regression\n     Regression analysis is one of very useful predictive modeling techniques that identify the relationship between two or more variables. The objective of linear regression is to identify a linear line of best fit that can predict the outcome variable (target variable/dependent variable/response variable) for one or more independent variables(predictors). For example, we can draw a scatter diagram to see the relation between tip and total bill in restaurant. Figure 7.1 and Figure 7.2 show the relationship between tip and total bill in R and Python respectively. It is clear that there exists a positive relationship between tip and total bill and it makes sense because higher bill generates higher tips. Similarly, there is positive relationship between size of the diners and tips because when there are more people in a group to dine, the higher the bill and tips (Figure 7.3 and Figure 7.4). The equation of a simple linear regression is - \\[y= mX + C\\]\n     where \\(y\\) = Target variable (Dependent variable); \\(m\\) = slope or rate of change; \\(X\\) = predictor or independent variable; and \\(C\\) = intercept or constant.",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling - Linear Regression</span>"
    ]
  },
  {
    "objectID": "predictive.html#time-series-analysis",
    "href": "predictive.html#time-series-analysis",
    "title": "7  Predictive Modeling - Linear Regression",
    "section": "7.3 Time Series Analysis",
    "text": "7.3 Time Series Analysis",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling - Linear Regression</span>"
    ]
  },
  {
    "objectID": "predictive.html#exercises",
    "href": "predictive.html#exercises",
    "title": "7  Predictive Modeling - Linear Regression",
    "section": "Exercises",
    "text": "Exercises\n\nCalculate a linear model between PepsiCo stock returns and S&P 500 Market index (the Ticker of S&P 500 Index is ^GSPC)",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling - Linear Regression</span>"
    ]
  },
  {
    "objectID": "prescriptive.html",
    "href": "prescriptive.html",
    "title": "8  Prescriptive Analytics in Accounting",
    "section": "",
    "text": "8.1 Decision Analysis",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prescriptive Analytics in Accounting</span>"
    ]
  },
  {
    "objectID": "prescriptive.html#optimization-techniques",
    "href": "prescriptive.html#optimization-techniques",
    "title": "8  Prescriptive Analytics in Accounting",
    "section": "8.2 Optimization Techniques",
    "text": "8.2 Optimization Techniques",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prescriptive Analytics in Accounting</span>"
    ]
  },
  {
    "objectID": "prescriptive.html#scenario-planning",
    "href": "prescriptive.html#scenario-planning",
    "title": "8  Prescriptive Analytics in Accounting",
    "section": "8.3 Scenario Planning",
    "text": "8.3 Scenario Planning",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Prescriptive Analytics in Accounting</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html",
    "href": "advanced_analytics.html",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#what-is-web-scraping",
    "href": "advanced_analytics.html#what-is-web-scraping",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.1 What is Web Scraping?",
    "text": "9.1 What is Web Scraping?\n\n     Web scraping refers to the techniques of accessing websites and collecting information from them. Having web scraping knowledge is important nowadays because a vast amount of data is available on websites and in many occasions we need to access, collect, and analyze those data. Web scraping is also called “web harvesting” or “web data extraction”.\n     Web scraping is employed in different kinds of practical applications. For example, companies scrape websites of their competitors to keep track of their pricing, which can help companies to form a competitive pricing strategy. Moreover, marketers and analysts scrape different social media platforms to analyze public sentiment about their products, brands, or events, which help them to gauge public opinions and ultimately tailor their products or services to meet or exceed customers’ expectations.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#legal-and-ethical-consideration-of-web-scraping",
    "href": "advanced_analytics.html#legal-and-ethical-consideration-of-web-scraping",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.2 Legal and Ethical Consideration of Web Scraping",
    "text": "9.2 Legal and Ethical Consideration of Web Scraping\n\n     As good citizens on the internet, it is incumbent on us to respect the policies of the websites we plan to scrape. Therefore, before we decide to scrape a website, we must take into consideration the legal and ethical aspects of scraping.\n\n\n9.2.1 Legal Framework of Web Scraping\n\n     Before scraping a website, we must evaluate the following legal considerations -\n\nTerms of Service: Please check the terms of service of the website because some sites explicitly prohibit scraping and violating terms of service might result in legal action.\nCopyright Law: In most cases, data published online is protected by copyright. As such, it is important to know beforehand what you can legally collect from the website by scraping and how you can use the scraped data.\nComputer Fraud and Abuse Act (CFAA): In the US, the Computer Fraud and Abuse Act (CFAA) was enacted in 1986. The CFAA prohibits intentionally accessing a computer without authorization or in excess of authorization. You might violate CFAA if a website has taken steps to block scraping and you circumvent those measures.\nData Protection Law: Beacause of different kinds of data protection law such as General Data Protection Regulation (GDPR) in Europe or similar law in other jurisdictions, it has become very critical to deal with personal data. If you scrape personal data, you must comply with such laws, which typically include requirements for consent, data minimization, and secure handling of the data.\n\n\n\n\n9.2.2 Ethical Considerations of Web Scraping\n\n     In addition to legal considerations, you should also behave ethically when you try to scrape a website. Ethical considerations though aligns with legal considerations, they extend to the idea of good citizenship on the web. Some important ethical considerations during webscraping include -\n\nWeb scraping might be equivalent to Distriubted of Denial of Service (DDOS) attack if too many requests are sent to the targeted websites, thus disrupting the regular functioning of the website. Therefore, while web scraping, we should scrape in such a way so that it does not disrupt the usage of the website by other legitimate users. Further, you should not try to scrape a website if it prohibits web scraping. Some websites have robots.txt file, which defines what can be scraped from the website. So please invesitage a website well before you decide to scrape it.\nHow the scraped data will be used is an important considerations even if the data are publicly available. Using the web scraped data in a way that is detrimental to individual or businesses is unethical. Further, you should also consider the ramifications of publishing or sharing the scraped data.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#understanding-html-and-css-selectors",
    "href": "advanced_analytics.html#understanding-html-and-css-selectors",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.3 Understanding HTML and CSS Selectors",
    "text": "9.3 Understanding HTML and CSS Selectors\n\n     Websites are usually created by using HTML - HyperText Markup Language, which describes the structure of a web page and includes cues for the apperance of a website. Therefore, having some knowledge on HTML will help you to scrape a website. HTML document uses different kinds of tags to identify or refer to different elements. A typical HTML document has following elements -\n&lt;!DOCTYPE&gt; : Defines the document type\n&lt;html&gt; : Defines the HTML document\n&lt;head&gt; : Contains metadata or information for the document\n&lt;body&gt; : Defines the document body such as text, images, and other media\n     More about HTML tags can be found here. Here is an example of a basic HTML structure -\n\n&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;\n&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were\n&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and\n&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;\nand they lived at the bottom of a well.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\n     In addition to HTML tags, CSS (Cascading Style Sheets) selectors are used to style different elements in the website. In web scraping, we use CSS selectors to identify the data we want to extract. There are different types of CSS selectors:\n\nElement Selector: Selects all elements of a specific type. For example, p selects all &lt;p&gt; elements.\nID Selector: Selects a single element with a specific id. The ID selector is defined with a hash (#). For example, #navbar selects the element with id=\"navbar\".\nClass Selector: Selects all elements with a specific class. The class selector is defined with a dot (.). For example, .menu-item selects all elements with class=\"menu-item\".\nAttribute Selector: Selects elements with a specific attribute or attribute value. For example, [href] selects all elements with an href attribute.\n\n     Below is an example of CSS selectors -\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;style&gt;\n        #header {\n            background-color: #f2f2f2;\n        }\n        .highlight {\n            font-weight: bold;\n        }\n        a[href^=\"https\"] {\n            color: green;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div id=\"header\"&gt;This is the header&lt;/div&gt;\n    &lt;p class=\"highlight\"&gt;This paragraph is highlighted.&lt;/p&gt;\n    &lt;a href=\"https://example.com\"&gt;This link is green because it uses HTTPS.&lt;/a&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n     In the above code, #header selects the &lt;div&gt; with the ID of “header,” .highlight selects any element with the “highlight” class, and a[href^=\"https\"] selects anchor tags (&lt;a&gt;) whose href attribute value begins with “https”. Understanding how to use these CSS selectors are very important while web scraping websites.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#an-overview-of-beautiful-soup",
    "href": "advanced_analytics.html#an-overview-of-beautiful-soup",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.4 An Overview of Beautiful Soup",
    "text": "9.4 An Overview of Beautiful Soup\n\n     Beautifulsoup is a python module that is widely used to scrape and parse websites. Beautifulsoup has many useful functions that can be easily used to extract data from HTML. Figure 9.1 shows the basic work process Beautifulsoup uses. It is clear from Figure 9.1 that using Beautifulsoup, we can extract data by finding HTML tag names, by CSS class names, and so on.\n\n\n\n\n\n\nFigure 9.1: Beautiful Soup Process\n\n\n\n     The following python code can be run to install and import Beautifulsoup module.\n\n# installing beautifulsoup \npip install beautifulsoup4\n\n# importing beautifulsoup\nfrom bs4 import BeautifulSoup\n\n     When we use BeautifulSoup to scrape a website, one of the most critical tasks is to identify the tags or CSS selectors from which we want to extract text or data. These targets are called Document Object Model (DOM). The DOM is a programming interface for web documents. Visualize HTML code of a webpage as an upside-down tree. Each HTML element - headings, paragraphs, and links - is a node in the tree. Figure 9.2 shows a basic tree structure of an HTML page.\n\n\n\n\n\n\nFigure 9.2: Tree Structure of HTML Page\n\n\n\n\n\n9.4.1 An Example of Web Scraping\n\n     Below we provide a small example of webscraping. We create a webpage called html, which includes different tags and CSS selectors.\n\n# an HTML file data \n\nhtml = \"\"\"\n&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;\n&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were\n&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and\n&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;\nand they lived at the bottom of a well.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\"\"\"\n\n     Then we import BeautifulSoup from beautifulsoup.\n\n# importing beautiful soup \nfrom bs4 import BeautifulSoup\n\n     Next, we convert the html into beautifulsoup object and name it soup. In BeautifulSoup ()function, we use the built-in parser called html.parser. We can also use other parsers such as lxml or html5lib. Each of these parsers has their own pros and cons. For example, lxml is the fastest and html.parser does not need extra dependencies.\n\n# Converting HTML data into Beautiful Soup Object \nsoup = BeautifulSoup(html, \"html.parser\")\n\n     The prettify() function will turn a soup object into a nicely formatted Unicode string, witha a separate line for each tag and each string.\n\nsoup.prettify()\n\n'&lt;html&gt;\\n &lt;head&gt;\\n  &lt;title&gt;\\n   The Dormouse\\'s story\\n  &lt;/title&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n  &lt;p class=\"title\"&gt;\\n   &lt;b&gt;\\n    The Dormouse\\'s story\\n   &lt;/b&gt;\\n  &lt;/p&gt;\\n  &lt;p class=\"story\"&gt;\\n   Once upon a time there were three little sisters; and their names were\\n   &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;\\n    Elsie\\n   &lt;/a&gt;\\n   ,\\n   &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;\\n    Lacie\\n   &lt;/a&gt;\\n   and\\n   &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;\\n    Tillie\\n   &lt;/a&gt;\\n   ;\\nand they lived at the bottom of a well.\\n  &lt;/p&gt;\\n &lt;/body&gt;\\n&lt;/html&gt;\\n'\n\n\n     We can use get_text() function to see the text element of the tags. text is a property (attribute) of soup object, which calls get_text function.\n\nsoup.get_text()\n\n\"\\nThe Dormouse's story\\n\\nThe Dormouse's story\\nOnce upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.\\n\"\n\n\n\nsoup.text\n\n\"\\nThe Dormouse's story\\n\\nThe Dormouse's story\\nOnce upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.\\n\"\n\n\n\nprint(soup.text)\n\n\nThe Dormouse's story\n\nThe Dormouse's story\nOnce upon a time there were three little sisters; and their names were\nElsie,\nLacie and\nTillie;\nand they lived at the bottom of a well.\n\n\n\n     To see the title of the document, we run the following codes -\n\n# Navigating to Specific Tags \nsoup.head.title\n\n&lt;title&gt;The Dormouse's story&lt;/title&gt;\n\n\n\n# Getting Text from a Specific Tag\nsoup.head.title.text\n\n\"The Dormouse's story\"\n\n\n     To see the text, from a tag, we run the following code -\n\nsoup.body.a.text\n\n'Elsie'\n\n\n     To see the text, from p tag, we run the following code -\n\nsoup.body.p.text\n\n\"The Dormouse's story\"",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#searching-the-elements-of-tags",
    "href": "advanced_analytics.html#searching-the-elements-of-tags",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.5 Searching the Elements of Tags",
    "text": "9.5 Searching the Elements of Tags\n\n     The find_all() function from beautifulsoup takes an HTML tag as an string argument and returns the list of elements that match the tag. For example, if we want to have all a tags in html data above, we will run the following code. Please note that there is another similar function called find(), which will return the first tag element.\n\nsoup.find_all('a')\n\n[&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n\n\n\nsoup.find('a')\n\n&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n\n\n     We can also search for tags of a specific class as well by providing class_ argument. Beasutiful soup uses class_ because class is a reserved keyword in python. For example, let’s search for p tags that have element story.\n\nsoup.find_all(\"p\", class_ = \"title\")\n\n[&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;]\n\n\n\nsoup.find(\"p\", class_=\"story\")\n\n&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were\n&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt; and\n&lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;;\nand they lived at the bottom of a well.&lt;/p&gt;\n\n\n\nsoup.find(\"p\", class_=\"story\").get_text()\n\n'Once upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.'",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#sec-usebeautifulsoup",
    "href": "advanced_analytics.html#sec-usebeautifulsoup",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.6 Scrape a Website Using BeautifulSoup",
    "text": "9.6 Scrape a Website Using BeautifulSoup\n\n     We have mastered some basic knowledge of Beautifulsoup. Therefore, it is now time to put our knowledge into practice. We are going to parse a website, which includes information about books. We would like to extract some data from the website. The data include - book url, title of the book, ratings of the book, price, and availability of the book. Before we start scraping the website, we need to identify the tags or CSS selectors that are relevant for our targeted data. Figure 9.3 shows how we can identify the tags or selectors relevant for our search. We should hover our cursor over the information that we plan to extract and then click right button of the mouse (on Windows) and click \"inspect\". Then we can see all tags and CSS selectors and other tags of the website. Figure 9.3 visualizes the whole process.\n\n\n\n\n\n\nFigure 9.3: How to Find the HTML tags and CSS Class\n\n\n\n     First, we need to import necessary python modules. We use requests module to get the website information.\n\n# importing requests \nimport requests\n# importing beautifulsoup\nfrom bs4 import BeautifulSoup\n# importing pandas \nimport pandas as pd\n\n     Then, we convert the data into soup object.\n\n# Fetch the website page \nurl = 'https://books.toscrape.com/catalogue/page-1.html'\nhtml = requests.get(url)\npage = html.text\n# Converting it into Soup Object \nsoup = BeautifulSoup(page, \"html.parser\")\n\n     After inspecting the tags and CSS selectors, we identify that article tag and product_pod class contains the information that we would like to extract. We use the find function from beautifulsuop to see our expected data. As noted before, find function identifies the first instance of the elements whereas find_all identifies all elements of the parsed HTML.\n\nsoup.find(\"article\", class_=\"product_pod\")\n\n&lt;article class=\"product_pod\"&gt;\n&lt;div class=\"image_container\"&gt;\n&lt;a href=\"a-light-in-the-attic_1000/index.html\"&gt;&lt;img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"../media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;p class=\"star-rating Three\"&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;/p&gt;\n&lt;h3&gt;&lt;a href=\"a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\"&gt;A Light in the ...&lt;/a&gt;&lt;/h3&gt;\n&lt;div class=\"product_price\"&gt;\n&lt;p class=\"price_color\"&gt;Â£51.77&lt;/p&gt;\n&lt;p class=\"instock availability\"&gt;\n&lt;i class=\"icon-ok\"&gt;&lt;/i&gt;\n    \n        In stock\n    \n&lt;/p&gt;\n&lt;form&gt;\n&lt;button class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\" type=\"submit\"&gt;Add to basket&lt;/button&gt;\n&lt;/form&gt;\n&lt;/div&gt;\n&lt;/article&gt;\n\n\n\nsoup.find_all(\"article\", class_=\"product_pod\")\n\n     Next, we check the url of each book. The a tag defines a hyperlink and the href is an attribute of a tag. Below, we use a tag to identify the link of each book.\n\nbooks = soup.find_all(\"article\", class_=\"product_pod\")\n\n\nsource_url = \"https://books.toscrape.com/catalogue\"\n\n\n# Book url \nfor h in soup.find_all(\"article\", class_=\"product_pod\"):\n    print(source_url+\"/\"+h.find('a')['href'])\n\nhttps://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\nhttps://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\nhttps://books.toscrape.com/catalogue/soumission_998/index.html\nhttps://books.toscrape.com/catalogue/sharp-objects_997/index.html\nhttps://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\nhttps://books.toscrape.com/catalogue/the-requiem-red_995/index.html\nhttps://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\nhttps://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\nhttps://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\nhttps://books.toscrape.com/catalogue/the-black-maria_991/index.html\nhttps://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html\nhttps://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html\nhttps://books.toscrape.com/catalogue/set-me-free_988/index.html\nhttps://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\nhttps://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html\nhttps://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\nhttps://books.toscrape.com/catalogue/olio_984/index.html\nhttps://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\nhttps://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html\nhttps://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html\n\n\n\n# Book url (Alternative) \nfor h in soup.find_all(\"article\", class_=\"product_pod\"):\n    print(h.h3.find('a')['href'])\n\na-light-in-the-attic_1000/index.html\ntipping-the-velvet_999/index.html\nsoumission_998/index.html\nsharp-objects_997/index.html\nsapiens-a-brief-history-of-humankind_996/index.html\nthe-requiem-red_995/index.html\nthe-dirty-little-secrets-of-getting-your-dream-job_994/index.html\nthe-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\nthe-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\nthe-black-maria_991/index.html\nstarving-hearts-triangular-trade-trilogy-1_990/index.html\nshakespeares-sonnets_989/index.html\nset-me-free_988/index.html\nscott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\nrip-it-up-and-start-again_986/index.html\nour-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\nolio_984/index.html\nmesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\nlibertarianism-for-beginners_982/index.html\nits-only-the-himalayas_981/index.html\n\n\n\n# Book Title \nfor h in soup.find_all(\"article\", class_=\"product_pod\"):\n    print(h.h3.find('a')['title'])\n\nA Light in the Attic\nTipping the Velvet\nSoumission\nSharp Objects\nSapiens: A Brief History of Humankind\nThe Requiem Red\nThe Dirty Little Secrets of Getting Your Dream Job\nThe Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\nThe Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\nThe Black Maria\nStarving Hearts (Triangular Trade Trilogy, #1)\nShakespeare's Sonnets\nSet Me Free\nScott Pilgrim's Precious Little Life (Scott Pilgrim #1)\nRip it Up and Start Again\nOur Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\nOlio\nMesaerion: The Best Science Fiction Stories 1800-1849\nLibertarianism for Beginners\nIt's Only the Himalayas\n\n\n\n# ratings \nsoup.find('p', class_='star-rating')['class'][1]\n\n'Three'\n\n\n\n# price \nsoup.find('p', class_='price_color').get_text().replace(\"Â\",'')\n\n'£51.77'\n\n\n\n# availability \nsoup.find('p', class_='instock availability').get_text().replace('\\n','').strip()\n\n'In stock'\n\n\n\n9.6.1 Putting All of the Above Actions Together\n     In Section 9.6, we identify and extract individual tags and data that we want to extract. Now, we will put all of them together and create a data frame. For this purpose, we will use for loop.\n\n# Fetch the Page \nurl = 'https://books.toscrape.com/catalogue/page-1.html'\nhtml = requests.get(url)\npage = html.text\n# Parse HTML Content\nsoup = BeautifulSoup(page, \"html.parser\")\n\n# Information We need \n\nbook_url = []\ntitle = []\nratings = []\nprice = []\navailability = []\n\n# Extract listings from the page\nbooks = soup.find_all(\"article\", class_=\"product_pod\")\nsource_url = \"https://books.toscrape.com/catalogue\"\n\nfor book in books:\n    # extract book url \n    book_url_text = source_url+\"/\"+book.find('a')['href']\n    book_url.append(book_url_text)\n\n    # extract title \n    title_text = book.h3.find('a')['title']\n    title.append(title_text)\n\n    # extract ratings \n    ratings_text = book.find('p', class_='star-rating')['class'][1]\n    ratings.append(ratings_text)\n\n    # extract price \n    price_text = book.find('p', class_='price_color').get_text().replace(\"Â\",'')\n    price.append(price_text)\n\n    # extract availability \n    availability_text = book.find('p', class_='instock availability').get_text().replace('\\n','').strip()\n    availability.append(availability_text)\n\n# Creating the Data Frame \n\npd.DataFrame({\n    'book_url':book_url,\n    'title':title,\n    'ratings':ratings,\n    'price':price,\n    'availability':availability\n})\n\n\n\n\n\n\n\n\n\nbook_url\ntitle\nratings\nprice\navailability\n\n\n\n\n0\nhttps://books.toscrape.com/catalogue/a-light-i...\nA Light in the Attic\nThree\n£51.77\nIn stock\n\n\n1\nhttps://books.toscrape.com/catalogue/tipping-t...\nTipping the Velvet\nOne\n£53.74\nIn stock\n\n\n2\nhttps://books.toscrape.com/catalogue/soumissio...\nSoumission\nOne\n£50.10\nIn stock\n\n\n3\nhttps://books.toscrape.com/catalogue/sharp-obj...\nSharp Objects\nFour\n£47.82\nIn stock\n\n\n4\nhttps://books.toscrape.com/catalogue/sapiens-a...\nSapiens: A Brief History of Humankind\nFive\n£54.23\nIn stock\n\n\n5\nhttps://books.toscrape.com/catalogue/the-requi...\nThe Requiem Red\nOne\n£22.65\nIn stock\n\n\n6\nhttps://books.toscrape.com/catalogue/the-dirty...\nThe Dirty Little Secrets of Getting Your Dream...\nFour\n£33.34\nIn stock\n\n\n7\nhttps://books.toscrape.com/catalogue/the-comin...\nThe Coming Woman: A Novel Based on the Life of...\nThree\n£17.93\nIn stock\n\n\n8\nhttps://books.toscrape.com/catalogue/the-boys-...\nThe Boys in the Boat: Nine Americans and Their...\nFour\n£22.60\nIn stock\n\n\n9\nhttps://books.toscrape.com/catalogue/the-black...\nThe Black Maria\nOne\n£52.15\nIn stock\n\n\n10\nhttps://books.toscrape.com/catalogue/starving-...\nStarving Hearts (Triangular Trade Trilogy, #1)\nTwo\n£13.99\nIn stock\n\n\n11\nhttps://books.toscrape.com/catalogue/shakespea...\nShakespeare's Sonnets\nFour\n£20.66\nIn stock\n\n\n12\nhttps://books.toscrape.com/catalogue/set-me-fr...\nSet Me Free\nFive\n£17.46\nIn stock\n\n\n13\nhttps://books.toscrape.com/catalogue/scott-pil...\nScott Pilgrim's Precious Little Life (Scott Pi...\nFive\n£52.29\nIn stock\n\n\n14\nhttps://books.toscrape.com/catalogue/rip-it-up...\nRip it Up and Start Again\nFive\n£35.02\nIn stock\n\n\n15\nhttps://books.toscrape.com/catalogue/our-band-...\nOur Band Could Be Your Life: Scenes from the A...\nThree\n£57.25\nIn stock\n\n\n16\nhttps://books.toscrape.com/catalogue/olio_984/...\nOlio\nOne\n£23.88\nIn stock\n\n\n17\nhttps://books.toscrape.com/catalogue/mesaerion...\nMesaerion: The Best Science Fiction Stories 18...\nOne\n£37.59\nIn stock\n\n\n18\nhttps://books.toscrape.com/catalogue/libertari...\nLibertarianism for Beginners\nTwo\n£51.33\nIn stock\n\n\n19\nhttps://books.toscrape.com/catalogue/its-only-...\nIt's Only the Himalayas\nTwo\n£45.17\nIn stock\n\n\n\n\n\n\n\n\n\n\n9.6.2 Doing the Same Things for All Pages\n     In Section 9.6.1, we scrape the first page of the website, but now we would like to scrape all pages of the website.\n\nurl1 = 'https://books.toscrape.com/catalogue/page-'\npages = range(51)\nurl2 = '.html'\n\n# Information We need \nbook_url = []\ntitle = []\nratings = []\nprice = []\navailability = []\n# Some other Information \nsource_url = \"https://books.toscrape.com/catalogue\"\n\nfor page in pages:\n    url = url1+str(page)+url2\n    r = requests.get(url)\n    soup = BeautifulSoup(r.text, 'html.parser')\n    books = soup.find_all(\"article\", class_=\"product_pod\")\n\n    for book in books:\n        # extract book url \n        book_url_text = source_url+\"/\"+book.find('a')['href']\n        book_url.append(book_url_text)\n\n        # extract title \n        title_text = book.h3.find('a')['title']\n        title.append(title_text)\n\n        # extract ratings \n        ratings_text = book.find('p', class_='star-rating')['class'][1]\n        ratings.append(ratings_text)\n\n        # extract price \n        price_text = book.find('p', class_='price_color').get_text().replace(\"Â\",'')\n        price.append(price_text)\n\n        # extract availability \n        availability_text = book.find('p', class_='instock availability').get_text().replace('\\n','').strip()\n        availability.append(availability_text)\n    \n\n\n\n# Creating the Data Frame \npd.DataFrame({\n    'book_url':book_url,\n    'title':title,\n    'ratings':ratings,\n    'price':price,\n    'availability':availability\n})\n\n\n\n\n\n\n\n\n\nbook_url\ntitle\nratings\nprice\navailability\n\n\n\n\n0\nhttps://books.toscrape.com/catalogue/a-light-i...\nA Light in the Attic\nThree\n£51.77\nIn stock\n\n\n1\nhttps://books.toscrape.com/catalogue/tipping-t...\nTipping the Velvet\nOne\n£53.74\nIn stock\n\n\n2\nhttps://books.toscrape.com/catalogue/soumissio...\nSoumission\nOne\n£50.10\nIn stock\n\n\n3\nhttps://books.toscrape.com/catalogue/sharp-obj...\nSharp Objects\nFour\n£47.82\nIn stock\n\n\n4\nhttps://books.toscrape.com/catalogue/sapiens-a...\nSapiens: A Brief History of Humankind\nFive\n£54.23\nIn stock\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\nhttps://books.toscrape.com/catalogue/alice-in-...\nAlice in Wonderland (Alice's Adventures in Won...\nOne\n£55.53\nIn stock\n\n\n996\nhttps://books.toscrape.com/catalogue/ajin-demi...\nAjin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)\nFour\n£57.06\nIn stock\n\n\n997\nhttps://books.toscrape.com/catalogue/a-spys-de...\nA Spy's Devotion (The Regency Spies of London #1)\nFive\n£16.97\nIn stock\n\n\n998\nhttps://books.toscrape.com/catalogue/1st-to-di...\n1st to Die (Women's Murder Club #1)\nOne\n£53.98\nIn stock\n\n\n999\nhttps://books.toscrape.com/catalogue/1000-plac...\n1,000 Places to See Before You Die\nFive\n£26.08\nIn stock\n\n\n\n\n1000 rows × 5 columns",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#conclusion",
    "href": "advanced_analytics.html#conclusion",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.7 Conclusion",
    "text": "9.7 Conclusion\n     On this chapter, we discuss about how to collect unstructured data, particularly collecting data from websites. The structure of HTML taggings and CSS selectors are highlighted because they are very important to understand how to scrape a website. Moreover, we scrape a page of a website and finally we demonstrated how to extract data from multiple pages of a website and convert them into a data frame.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "10  Natural Language Processing (NLP)",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#introduction",
    "href": "nlp.html#introduction",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\n\n    In today’s data driven world, a significant amount data is produced each day. For example, Google processes 24 peta bytes of data every day; 10 million photos are uploaded every hour on Facebook; and 400 million tweets are posted on X (formerly Twitter). Of these amount of data, a significant portion consists of text data. Therefore, it it important to gain insights from text data.\n    Natural Language Processing (NLP), according to IBM, is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language. Specifically, NLP involves understanding, interpreting, and extracting insights from human language. Businesses use NLP for many purposes such as processing and analyzing large volume of documents, analyzing customer reviews, and scaling customer services (like developing chatbots or virtual assistants).",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#python-libraries-for-nlp",
    "href": "nlp.html#python-libraries-for-nlp",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.2 Python Libraries for NLP",
    "text": "10.2 Python Libraries for NLP\n\n    Python has built a rich and efficient ecosystem for NLP. Some of the most popular python modules (libraries) for NLP include - nltk (Natural Language Toolkit); SpaCy; gensim; and TextBlob.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#steps-in-nlp",
    "href": "nlp.html#steps-in-nlp",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.3 Steps in NLP",
    "text": "10.3 Steps in NLP\n\n    Like other data science tools or techniques, NLP involves several steps because most the time text data is not readily available or even if they are available, we need to clean the data and make it ready for next step processing. In this section, several important steps, which are called preprocessing, of NLP will be discussed.\n\n\n10.3.1 Preprocessing\n\n    Before applying NLP techniques, it is necessary to preprosess and clean the text data. Therefore, the processes involving cleaning and preparing text data to get them ready for NLP models are called preprocessing. Preprocessing is very important in NLP to get effective and accurate insights from the data. Below we will discuss several important concepts of preprocessing.\n\n\n# An exmaple of a text data \nmy_text = \"\"\"\nAccounting is the systematic process of recording, analyzing, and reporting financial \\\ntransactions. It helps businesses track their income, expenses, and overall financial \\\nhealth. Accountants use various financial statements, such as balance sheets and income \\\nstatements, to summarize a company's financial position. Double-entry bookkeeping is a \\\nfundamental principle in accounting, ensuring that every transaction affects at least two \\\naccounts. Financial accounting focuses on providing information to external stakeholders, \\\nsuch as investors and creditors, while managerial accounting provides information to \\\ninternal stakeholders, like managers, to aid in decision-making. Auditing is an essential \\\naspect of accounting, involving the examination of financial records to ensure accuracy \\\nand compliance. Tax accounting deals with preparing tax returns and planning for \\\nfuture tax obligations. Forensic accounting involves investigating financial discrepancies \\\nand fraud. Accounting software, like QuickBooks and Xero, has revolutionized the way \\\nbusinesses manage their finances, making the process more efficient and accurate. \\\nOverall, accounting plays a crucial role in the financial management and transparency \\\nof businesses and organizations.\n\"\"\"\n\n\n10.3.1.1 Tokenization\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download(\"punkt_tab\")\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n\n# sentence tokenize\nmy_text_sent = sent_tokenize(my_text)\nmy_text_sent[0:5]\n\n['\\nAccounting is the systematic process of recording, analyzing, and reporting financial transactions.',\n 'It helps businesses track their income, expenses, and overall financial health.',\n \"Accountants use various financial statements, such as balance sheets and income statements, to summarize a company's financial position.\",\n 'Double-entry bookkeeping is a fundamental principle in accounting, ensuring that every transaction affects at least two accounts.',\n 'Financial accounting focuses on providing information to external stakeholders, such as investors and creditors, while managerial accounting provides information to internal stakeholders, like managers, to aid in decision-making.']\n\n\n\n# word tokenize\nmy_text_word = word_tokenize(my_text)\nmy_text_word[0:5]\n\n['Accounting', 'is', 'the', 'systematic', 'process']\n\n\n\n\n10.3.1.2 Removing Punctuation\n\n    It is evident that in our word tokens, punctuations like comma (,), full stop (.) are also included, but they are unncessary. Therefore, we need to eliminate them from the token list.\n\n\nimport string\nmy_text_nopunc = [x for x in my_text_word if x not in string.punctuation]\nmy_text_nopunc[:11]\n\n['Accounting',\n 'is',\n 'the',\n 'systematic',\n 'process',\n 'of',\n 'recording',\n 'analyzing',\n 'and',\n 'reporting',\n 'financial']\n\n\n\n\n10.3.1.3 Filtering Stop Words\n\n    Stop words are the words that we want to ignore. Words like “in”, “an”, “the” we want to ignore. Therefore, in this step, we want to filter out these kinds of words.\n\n\nnltk.download(\"stopwords\") # to download the stopwords from NLTK repository\nfrom nltk.corpus import stopwords # imports the module \nstop_words = set(stopwords.words(\"english\")) # access the stopwords for english \n# print(stop_words)\n\n\nmy_text_nostopwords = [x for x in my_text_nopunc if x.lower() not in stop_words]\nmy_text_nostopwords[0:11]\n\n['Accounting',\n 'systematic',\n 'process',\n 'recording',\n 'analyzing',\n 'reporting',\n 'financial',\n 'transactions',\n 'helps',\n 'businesses',\n 'track']\n\n\n\n    Still we can see there are some unnessary words in the list. So, we need to eliminate them. For example, “’s” is in the my_text_nostopwords. We need to get rid of it.\n\n\n\"'s\" in my_text_nostopwords\nmy_text_nostopwords = [x for x in my_text_nostopwords if \"'s\" not in x]\n\"'s\" in my_text_nostopwords\n\nFalse\n\n\n\n\n10.3.1.4 Stemming\n\n    Stemming is the process of reducing the words to their base or root form. For example, the token list contains words like recording, reporting, analyzing and so on. The base form of those words are record, report, and analyze respectively. Therefore, we need to reduce those words to base form. Stemming will help to do so. For this purpose, there are several types of stemmers such as Porter stemmer, Lovins stemmer, Dawson stemmer, Krovetz stemmer, and Xerox stemmer.\n\n\nfrom nltk.stem import PorterStemmer,SnowballStemmer, LancasterStemmer\nporter = PorterStemmer()\nsnowball = SnowballStemmer(\"english\")\nlancaster = LancasterStemmer()\n[porter.stem(x) for x in my_text_nostopwords]\n[snowball.stem(x) for x in my_text_nostopwords]\n[lancaster.stem(x) for x in my_text_nostopwords][0:11]\n\n['account',\n 'system',\n 'process',\n 'record',\n 'analys',\n 'report',\n 'fin',\n 'transact',\n 'help',\n 'busy',\n 'track']\n\n\n\n\n10.3.1.5 Lemmatization\n\n    Lemmatization, like stemming, is the process of reducing a word to its base form, but, unlike stemming, it considers the context of the word.\n\n\nfrom nltk.stem import WordNetLemmatizer\nwordnet = WordNetLemmatizer()\nmy_text_lemmatized = [wordnet.lemmatize(x) for x in my_text_nostopwords]\nmy_text_lemmatized[:11]\n\n['Accounting',\n 'systematic',\n 'process',\n 'recording',\n 'analyzing',\n 'reporting',\n 'financial',\n 'transaction',\n 'help',\n 'business',\n 'track']\n\n\n\n\n10.3.1.6 Other Steps in Preprocessing\n\n    In addition to the above preprocessing, we might need to remove many other special characters from the text. These special characters include - hastags, HTML tags, links. For this purpose, knowledge about “regular expression” might be useful. Python built-in package re could be handy for regular expression. To learn more about regular expression - https://www.w3schools.com/python/python_regex.asp.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#visualization-of-words",
    "href": "nlp.html#visualization-of-words",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.4 Visualization of Words",
    "text": "10.4 Visualization of Words\n\n10.4.1 Word Cloud\n\n    Figure 10.1 shows a word cloud of our tokenized text.\n\n\nfrom wordcloud import WordCloud\n# We need a single string; So, it is tranformed below\nmy_text_lemmatizedSstring = ' '.join(my_text_lemmatized)\n# Word Cloud \nword_cloud = WordCloud(collocations = False, background_color = 'white').generate(my_text_lemmatizedSstring)\nimport matplotlib.pyplot as plt\nplt.imshow(word_cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\nFigure 10.1: Word Cloud of the Words\n\n\n\n\n\n\n    Now we are going to present a word cloud of customer reviews about many products of a company.\n\n\ncustomer_reviews = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/Customer_Reviews2.csv\")\n\ncustomer_reviews['ProductId'].value_counts().reset_index()\n\nproduct1_reviews = customer_reviews[customer_reviews['ProductId'] == \"B003VXFK44\"]['Text'].to_list()\n\n# Combine the text into a single string \ncombined_product1_reviews = \" \".join(product1_reviews)\n# Generate the word cloud\nword_cloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(combined_product1_reviews)\n# Display the word cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Word Cloud\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n10.4.2 Bar Diagram of Word Frequency\n\n    Figure 10.2 shows the bar diagram of the words in tokenized list.\n\n\nfrom collections import Counter\n# calculate word frequencies \nword_freq = Counter(my_text_lemmatized)\n# extract word and their frequencies \nwords = list(word_freq.keys())\nfrequencies = list(word_freq.values())\n# create a data frame \nimport pandas as pd\nimport seaborn as sns\nword_df = pd.DataFrame(word_freq.items(), columns = ['Word', \"Frequency\"])\nword_df = word_df.sort_values(by='Frequency', ascending=False)\n# Create the bar diagram \nplt.figure(figsize=(10, 5)) \nsns.barplot(y='Word', x='Frequency', data=word_df[word_df['Frequency']&gt;1], palette='viridis') \nplt.ylabel('Words') \nplt.xlabel('Frequencies') \nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\nFigure 10.2: Bar Diagram of Word Frequency",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#vectorization-of-text",
    "href": "nlp.html#vectorization-of-text",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.5 Vectorization of Text",
    "text": "10.5 Vectorization of Text\n\n    Using word clouds or bar diagrams of most frequent words is easy to use and helps to quickly explore datasets. However, they suffer from some shortcomings. For example, they ignore context and are unable to capture the relations between words within the data. Therefore, sometimes we need convert the texts into numbers.\n    The process of converting text data into numeric vectors is called text vectorization. Text vectorization actually helps us to capture semantic relationships between words, thus allowing us to understand the meaning of text beyond just keywords. Further, it enhances processing of data. Moreover, vectorized text can be used for various NLP tasks such as document classification, sentiment analysis, and topic modeling. There are several techniques of text vectorization, spanning from simple to complex technieques.\n\n10.5.1 Bag of Words (BoW)\n    Bag of Words is the simplest text vectorization technique. BoW counts the frequency of words in text documents. BoW does not consider the order of the words or syntax. “Dog toy” or “toy dog” have equal importance in BoW. In BoW, each document is represented as a vector of word frequencies. Actually, BoW generates document-term matrix, which is a m×n matrix, where m is the document and n is the term (word). So, the cell in the matrix contains raw count of the number of times the \\(j\\)-word appear in the $i%-th document.\n\n\n10.5.2 Term Frequency-Inverse Document Frequency (TF-IDF)\n    TF-IDF is another text vectorization technique. It is basically an extension of BoW model and considers the importance or significance of words in the texts. Term Frequecny (TF) refers to the frequency at which a particular word appears in a document and IDF measures how rare a word is across a collection of documents. IDF assigns more weights to words that are frequent in a document, but rare across all documents1. Since TF-IDF takes into consideration the significance of the words, it is sometimes called Latent Semantic Analysis (LSA). Unlike BoW, the cell in TF-IDF matrix contains tf-idf score, which is calculated in Equation 10.12.\n\\[\nw_{i,j} = tf_{i,j} × log\\frac{N}{df_{i}}\n\\tag{10.1}\\]\n    In Equation 10.1, \\(tf_{i,j}\\) refers to the occurrences of term \\(j\\) in document \\(i\\); \\(N\\) is total number of documents; \\(df_{i}\\) is the number of documents that contain the term \\(j\\); and \\(w_{i,j}\\) is the tf-idf score in the TF-IDF matrix.\n\nsklearngensim\n\n\n\n# using sklearn package \n# preprocessing function \nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Download necessary NLTK data\nnltk.download('stopwords')\nnltk.download('wordnet')\n\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n    \n    return text\n\ndef preprocess_documents(documents): \n    return [preprocess_text(doc) for doc in documents]\n\n\n# Bag of Words (BoW)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntexts = [\"I love Accounting\", \"Accounting is called language of Business\", \"I will graduate with an Accounting degree\"]\ntexts_processed = preprocess_documents(texts)\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(texts_processed)\n# Get the feature names (terms) \nfeature_names = vectorizer.get_feature_names_out()\nfeature_names\n\narray(['accounting', 'business', 'called', 'degree', 'graduate',\n       'language', 'love'], dtype=object)\n\n\n\ntexts_processed\n\n['love accounting',\n 'accounting called language business',\n 'graduate accounting degree']\n\n\n\nprint(X.toarray())\n\n[[1 0 0 0 0 0 1]\n [1 1 1 0 0 1 0]\n [1 0 0 1 1 0 0]]\n\n\n\npd.DataFrame(X.toarray(), columns = feature_names)\n\n\n\n\n\n\n\n\n\naccounting\nbusiness\ncalled\ndegree\ngraduate\nlanguage\nlove\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n0\n1\n0\n\n\n2\n1\n0\n0\n1\n1\n0\n0\n\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(texts_processed)\nprint(X.toarray())\n\n[[0.50854232 0.         0.         0.         0.         0.\n  0.861037  ]\n [0.32274454 0.54645401 0.54645401 0.         0.         0.54645401\n  0.        ]\n [0.38537163 0.         0.         0.65249088 0.65249088 0.\n  0.        ]]\n\n\n\npd.DataFrame(X.toarray(), \ncolumns = vectorizer.get_feature_names_out())\n\n\n\n\n\n\n\n\n\naccounting\nbusiness\ncalled\ndegree\ngraduate\nlanguage\nlove\n\n\n\n\n0\n0.508542\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.861037\n\n\n1\n0.322745\n0.546454\n0.546454\n0.000000\n0.000000\n0.546454\n0.000000\n\n\n2\n0.385372\n0.000000\n0.000000\n0.652491\n0.652491\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n# using gensim package\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom gensim import corpora, models, matutils\nimport pandas as pd\n\n# Download necessary NLTK data\nnltk.download('stopwords')\nnltk.download('wordnet')\n\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    \n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n    \n    return text\n\ndef preprocess_documents(documents):\n    return [preprocess_text(doc).split() for doc in documents]\n\n# Sample documents\ndocuments = [\"I love Accounting\", \"Accounting is called language of Business\", \"I will graduate with an Accounting degree\"]\n\n# Preprocess the documents\ntexts_processed = preprocess_documents(documents)\n\n# Create a dictionary from the tokenized texts\ndictionary = corpora.Dictionary(texts_processed)\n\n# Create a corpus using the dictionary\ncorpus = [dictionary.doc2bow(text) for text in texts_processed]\n\n\n# Create a TF-IDF model from the corpus\ntfidf_model = models.TfidfModel(corpus)\n\n# Transform the corpus using the TF-IDF model\ncorpus_tfidf = tfidf_model[corpus]\n\n# Convert the TF-IDF weighted corpus to a dense matrix\ntfidf_matrix = matutils.corpus2dense(corpus_tfidf, num_terms=len(dictionary)).T\n\n# Convert the dense matrix to a DataFrame for easier inspection\ndf_tfidf_matrix = pd.DataFrame(tfidf_matrix, columns=[dictionary[i] for i in range(len(dictionary))])\n\ndf_tfidf_matrix\n\n\n\n\n\n\n\n\n\naccounting\nlove\nbusiness\ncalled\nlanguage\ndegree\ngraduate\n\n\n\n\n0\n0.0\n1.0\n0.00000\n0.00000\n0.00000\n0.000000\n0.000000\n\n\n1\n0.0\n0.0\n0.57735\n0.57735\n0.57735\n0.000000\n0.000000\n\n\n2\n0.0\n0.0\n0.00000\n0.00000\n0.00000\n0.707107\n0.707107\n\n\n\n\n\n\n\n\n\n# Convert the corpus to a document-term matrix (DTM)\ndtm_matrix = matutils.corpus2dense(corpus, num_terms=len(dictionary)).T\n# Convert the DTM to a DataFrame for easier inspection\ndf_dtm_matrix = pd.DataFrame(dtm_matrix, columns=[dictionary[i] for i in range(len(dictionary))])\ndf_dtm_matrix\n\n\n\n\n\n\n\n\n\naccounting\nlove\nbusiness\ncalled\nlanguage\ndegree\ngraduate\n\n\n\n\n0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n\n\n2\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.5.3 Word Embeddings\n    Word embeddings are dense vector representations of words (not documents) that capture semantic relationships. Popular models include Word2Vec, GloVe, and FastText.\n\nfrom gensim.models import Word2Vec\n\n# Sample tokenized texts\ntexts_processed = [\n    ['love', 'accounting'],\n    ['accounting', 'called', 'language', 'business'],\n    ['graduate', 'accounting', 'degree']\n]\n\n# Create the Word2Vec model\nmodel = Word2Vec(sentences=texts_processed, vector_size=10, window=5, min_count=1, workers=4)\n\n# Get the word vectors for all terms in the vocabulary\nword_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n\n# Create a Data Frame from the word vectors \ndf_word_vectors = pd.DataFrame(word_vectors).T\n# Print the word vectors\ndf_word_vectors\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\naccounting\n-0.005362\n0.002364\n0.051033\n0.090093\n-0.093029\n-0.071168\n0.064589\n0.089730\n-0.050154\n-0.037634\n\n\ndegree\n0.073805\n-0.015335\n-0.045366\n0.065541\n-0.048602\n-0.018160\n0.028766\n0.009919\n-0.082852\n-0.094488\n\n\ngraduate\n0.073118\n0.050703\n0.067577\n0.007629\n0.063509\n-0.034054\n-0.009464\n0.057686\n-0.075216\n-0.039361\n\n\nbusiness\n-0.075116\n-0.009300\n0.095381\n-0.073192\n-0.023338\n-0.019377\n0.080774\n-0.059309\n0.000452\n-0.047537\n\n\nlanguage\n-0.096036\n0.050073\n-0.087596\n-0.043918\n-0.000351\n-0.002962\n-0.076612\n0.096147\n0.049821\n0.092331\n\n\ncalled\n-0.081579\n0.044958\n-0.041371\n0.008245\n0.084986\n-0.044622\n0.045175\n-0.067870\n-0.035485\n0.093985\n\n\nlove\n-0.015777\n0.003214\n-0.041406\n-0.076827\n-0.015080\n0.024698\n-0.008880\n0.055337\n-0.027430\n0.022601\n\n\n\n\n\n\n\n\n\n\n10.5.4 Doc2Vec\n    Doc2Vec is an extension of Word2Vec that generates vector representations for entire documents, capturing the context of the document.\n\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\n\ntexts_processed = [['love', 'accounting'],\n ['accounting', 'called', 'language', 'business'],\n ['graduate', 'accounting', 'degree']]\n# Create tagged documents\ntagged_data = [TaggedDocument(words=text, tags=[str(i)]) for i, text in enumerate(texts)]\n# Create Doc2Vec Model \nmodel = Doc2Vec(tagged_data, vector_size=10, window=5, min_count=1, workers=4)\n# Get the document vectors for all documents\ndoc_vectors = {str(i): model.dv[str(i)] for i in range(len(texts_processed))} \n# Create a DataFrame for the document vectors \ndf_doc_vectors = pd.DataFrame(doc_vectors).T \n# Print the DataFrame \ndf_doc_vectors\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n-0.052996\n-0.059291\n-0.099730\n0.085218\n0.036740\n0.001727\n-0.098661\n-0.050780\n-0.099170\n0.019854\n\n\n1\n0.026782\n0.047362\n-0.044838\n-0.031941\n-0.028554\n-0.089483\n0.022004\n0.094703\n-0.100429\n-0.035253\n\n\n2\n-0.039851\n0.027446\n-0.059339\n0.025528\n0.061253\n-0.084000\n-0.082807\n-0.096986\n0.042936\n-0.091991\n\n\n\n\n\n\n\n\n\n\n10.5.5 BERT (Bidirectional Encoder Representations from Transformers)\n    BERT is a transformer-based model that generates contextualized word embeddings. It captures the context of words in a sentence, making it powerful for various NLP tasks.\n\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ntext = \"I love programming\"\ninputs = tokenizer(text, return_tensors='pt')\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nprint(last_hidden_states)",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#topic-modeling",
    "href": "nlp.html#topic-modeling",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.6 Topic Modeling",
    "text": "10.6 Topic Modeling\n\n    Topic Modeling is a technique in NLP that tries to identify or extract semantic patterns or topic from a collection of documents. In other words, topic modeling helps to idenfity the cluster of words that appear together often and ultimately, they can be grouped into topics. For example, imagine an auditor is reveiwing important contracts of the client to test some management assertions, but before starting collecting evident to test the assertion, the auditor needs to understand the main themes of those contracts. Topic modeling can be used to figure out the themes (topics) based on the words used in the contracts. Figure 10.3 shows the general process of how topic modeling works.\n\n\n\n\n\n\nFigure 10.3: Topic Modeling\n\n\n\n    There are two commonly used methods for topic modeling. They include - Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). LSA assumes that words with similar meaning will appear in similar documents. LSA uses mathmatical techniques to reduce the dimensionality of the data, but LDA is a proabilistic technique, which assumes documents contain topics and topics contain words.\n    In LSA, a term-document matrix is created in which rows represent terms and columns represent documents. The values (cells) of the matrix indicates the frequency of each term in each document. Then Singular Value Decomposition (SVD) is applied on the term-document matrix. The SVD technique converts the term-document matrix into three matrices - U (term-topic matrx), \\(\\sum\\) (diagonal matrix of singualr values), and V (document-topic matrix). This breakdown helps to reduce the dimensionality of the data while retaining the most improtant relationships between the data. Then, we select the top \\(k\\) singualr values and their associated vectors from \\(U\\) and \\(V\\) matrices to form a reduced dimensional representation of the data.\n    In LDA, on the other hand, it is assumed each document is a mixture of topics and that each topic is a mixture of words. Therefore, in LDA documents are mapped to a list of topics by assigning words in the document to different topics. LDA uses Bayesian inferences to find the underlying topics in a corpus of documents. Of the two methods, it is recommended to use LDA because of its probabilistic nature, interpretability and scalability.\n\nimport pandas as pd\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim, spacy, logging, warnings\nfrom gensim import corpora, models, matutils, utils\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport pyLDAvis \nimport pyLDAvis.gensim_models as gensimvis\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(\n    ['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come']\n    )\n\n# reading the dataset\ndf= pd.read_csv('DATA/tripadvisor_hotel_reviews.csv')\ndf_500= df[0:500]\ndef sent_to_words(sentences):\n    for sent in sentences:\n        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n        sent = utils.simple_preprocess(str(sent), deacc=True) \n        yield(sent)  \n\n# Convert to list\ndata = df_500.Review.values.tolist()\ndata_words = list(sent_to_words(data))\n\n    In topic modeling, sometimes we use bigrams or trigrams. Specifically, bigrams and trigrams arte sequences of two or three words respectively that appear consecutively in texts. They are also called n-grams, where \\(n\\) represents the number of words in the sequence. In topic modeling, bigrams or trigrams help to capture the context and meaning of texts better than individual words. For example, by considering the triplets such as “machine learning algorithm”, we can identify patterns, relationships, and meanings better. Therefore, bigrams or trigrams enhances the quality and coherence of the topics generated by LDA.\n\n# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) \n# higher threshold fewer phrases\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# Before running the process_words function, please run the following line of code in the terminal - \n# python -m spacy download en_core_web_sm\ndef process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n    texts = [bigram_mod[doc] for doc in texts]\n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = spacy.load(\"en_core_web_sm\")\n    #nlp = spacy.load('en', disable=['parser', 'ner'])\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n    return texts_out\n\ndata_ready = process_words(data_words)  # processed Text Data!\n\n\n# turn our tokenized documents into a term  &lt;-&gt; id dictionary)\ndictionary = corpora.Dictionary(data_ready)\n# convert tokenized documents into a document-term matrix)\ncorpus = [dictionary.doc2bow(text) for text in data_ready]\n# Convert the corpus to a document-term matrix \ndoc_term_matrix = matutils.corpus2csc(corpus).transpose()\n# Convert the document-term matrix to a DataFrame \ndf_doc_term_matrix = pd.DataFrame(doc_term_matrix.toarray(), \ncolumns=[dictionary[i] for i in range(len(dictionary))])\n# Print the DataFrame \ndf_doc_term_matrix\n\n\n\n\n\n\n\n\n\nadvantage\nadvice\nanniversary\narrive\naveda\nbang\nbed\ncheck\nclosing\ncomfortable\n...\ncushy\ngouge\nhumor\nlast\nnavigating\noxygen\nreplacement\nextraordinarily\nranier\nseperate\n\n\n\n\n0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n2.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n2.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n496\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n497\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n498\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n499\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n\n\n\n\n500 rows × 4230 columns\n\n\n\n\n\n# Create a TF-IDF model from the corpus \ntfidf_model = models.TfidfModel(corpus)\n# Transform the corpus using the TF-IDF model \ncorpus_tfidf = tfidf_model[corpus]\n# Convert the TF-IDF weighted corpus to a document-term matrix \ndoc_term_matrix_tfidf = matutils.corpus2csc(corpus_tfidf).transpose()\n# Convert the document-term matrix to a DataFrame \ndf_doc_term_matrix_tfidf = pd.DataFrame(doc_term_matrix_tfidf.toarray(), \ncolumns=[dictionary[i] for i in range(len(dictionary))])\n# Print the DataFrame \ndf_doc_term_matrix_tfidf\n\n\n\n\n\n\n\n\n\nadvantage\nadvice\nanniversary\narrive\naveda\nbang\nbed\ncheck\nclosing\ncomfortable\n...\ncushy\ngouge\nhumor\nlast\nnavigating\noxygen\nreplacement\nextraordinarily\nranier\nseperate\n\n\n\n\n0\n0.160135\n0.173497\n0.179099\n0.085600\n0.23166\n0.23166\n0.038655\n0.060582\n0.23166\n0.056518\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n1\n0.000000\n0.000000\n0.140405\n0.033553\n0.00000\n0.00000\n0.015152\n0.000000\n0.00000\n0.022154\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.040298\n0.063157\n0.00000\n0.029460\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n3\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.000000\n0.000000\n0.00000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n4\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.000000\n0.026962\n0.00000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.000000\n0.000000\n0.00000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n496\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.065851\n0.000000\n0.00000\n0.048142\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n497\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.029146\n0.000000\n0.00000\n0.042615\n...\n0.196603\n0.196603\n0.196603\n0.196603\n0.196603\n0.196603\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n498\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.075078\n0.000000\n0.00000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.253216\n0.000000\n0.000000\n0.000000\n\n\n499\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.000000\n0.000000\n0.00000\n0.067615\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.311936\n0.311936\n0.311936\n\n\n\n\n500 rows × 4230 columns\n\n\n\n\n\n# Create an LDA model from the corpus\nlda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n# Print the topics\nfor idx, topic in lda_model.print_topics(-1):\n    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n\nTopic: 0 \nWords: 0.018*\"room\" + 0.015*\"stay\" + 0.013*\"hotel\" + 0.008*\"night\" + 0.005*\"service\" + 0.005*\"day\" + 0.005*\"leave\" + 0.005*\"bathroom\" + 0.004*\"water\" + 0.004*\"small\"\n\nTopic: 1 \nWords: 0.038*\"hotel\" + 0.037*\"room\" + 0.021*\"stay\" + 0.015*\"great\" + 0.011*\"staff\" + 0.010*\"location\" + 0.010*\"night\" + 0.009*\"bed\" + 0.009*\"place\" + 0.007*\"service\"\n\nTopic: 2 \nWords: 0.027*\"room\" + 0.011*\"stay\" + 0.011*\"hotel\" + 0.011*\"night\" + 0.007*\"desk\" + 0.007*\"time\" + 0.006*\"bed\" + 0.005*\"elevator\" + 0.005*\"experience\" + 0.005*\"check\"\n\n\n\n\n# Topic Membership Likelihood \n# Create a DataFrame with topic vectors for each document\ntopic_vectors = []\nfor doc in corpus:\n    topic_vector = lda_model.get_document_topics(doc, minimum_probability=0)\n    topic_vectors.append([prob for _, prob in topic_vector])\ndf_topic_vectors = pd.DataFrame(topic_vectors)\n# Print the DataFrame of the probabilities\ndf_topic_vectors\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.005699\n0.575330\n0.418971\n\n\n1\n0.002079\n0.995860\n0.002061\n\n\n2\n0.002548\n0.623119\n0.374333\n\n\n3\n0.005680\n0.989345\n0.004976\n\n\n4\n0.002452\n0.994975\n0.002572\n\n\n...\n...\n...\n...\n\n\n495\n0.978581\n0.011443\n0.009976\n\n\n496\n0.005756\n0.988378\n0.005866\n\n\n497\n0.004771\n0.550551\n0.444678\n\n\n498\n0.007069\n0.985660\n0.007271\n\n\n499\n0.013484\n0.593313\n0.393204\n\n\n\n\n500 rows × 3 columns\n\n\n\n\n\n# Create the visualization \nvis_data = gensimvis.prepare(lda_model, corpus, dictionary) \npyLDAvis.display(vis_data)\n\n\n\n\n\n\n\n\n\n\n# Dominat topics and its percentage contribution in each document\n\ndef format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # =&gt; dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                # Create a new row as a DataFrame and concatenate it\n                new_row = pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords])\n                sent_topics_df = pd.concat([sent_topics_df, new_row.to_frame().T], ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(5)\n\n\n\n\n\n\n\n\n\nDocument_No\nDominant_Topic\nTopic_Perc_Contrib\nKeywords\nText\n\n\n\n\n0\n0\n1\n0.5753\nhotel, room, stay, great, staff, location, nig...\n[hotel, expensive, parking, deal, stay, hotel,...\n\n\n1\n1\n1\n0.9959\nhotel, room, stay, great, staff, location, nig...\n[special, charge, diamond, member, decide, cha...\n\n\n2\n2\n1\n0.6231\nhotel, room, stay, great, staff, location, nig...\n[room, experience, hotel, level, positive, lar...\n\n\n3\n3\n1\n0.9893\nhotel, room, stay, great, staff, location, nig...\n[unique, great, stay, wonderful, time, hotel, ...\n\n\n4\n4\n1\n0.995\nhotel, room, stay, great, staff, location, nig...\n[great, stay, great, stay, game, awesome, down...\n\n\n\n\n\n\n\n\n    There are two goodness of fit (gof) measures to evaluate topic modeling. They are - coherence score and perplexity. Coherence score measures the coherence between topics. Higher coherence score indicates more meaningful and interpretable topics. Coherent scores range from 0 to 1. The higher coherence scores indicate more interpretable and meaningful topics.\n    Perplexity score evaluates how well the model represents the data. Lower perplexity score indicates a better fit. Perplexity might not always correlate with human interpretability of the topics. Unlike, coherence socres, there is no good or bad perplexity score, but it is useful to compare different models on the same dataset.\n\n# Compute Coherence Score\nfrom gensim.models import CoherenceModel\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready,dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n\n\n\n\nCoherence Score:  0.3579669980731333\n\n\n\n# Compute Perplexity\nprint('\\nPerplexity : ', lda_model.log_perplexity(corpus)) \nlda_model.log_perplexity(corpus)\n\n\nPerplexity :  -7.088801568052299\n\n\n-7.088803211863139\n\n\n    To optimize topic modeling performance, we can tune some hyperparameters (also called hyperparameter tuning). Some of the important hyperparameters of topic modeling include - number of topics, document topic density, topic word density, number of iterations and so on.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#sentiment-analysis",
    "href": "nlp.html#sentiment-analysis",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.7 Sentiment Analysis",
    "text": "10.7 Sentiment Analysis\n\n    Sentiment analysis involves converting text into sentiments such as positive, neutral, and negative. Texts are widely used to express emotion, feelings, opinion and so on. Therefore, sometimes sentiment analysis is also called “Opinion Mining.” Identifying sentiment from texts could provide valuable insights to make strategic decisions such as improving product features, launching new products, identifying strengths or weaknesses of product or service offerings. Before, we perform the sentiment analysis, we need to do the preprocessing as described in Section 10.3.1 first.\n    Below we use texblob python module for seniment analysis of our text about Accounting. texblob is simple for sentiment analysis because the function accepts text as input and return sentiment score. There are two types of sentiment scores - polarity and subjectivity. Polarity score actually measures the sentiment of the text and it values are between -1 and +1, where -1 indicates high negative sentiment and +1 indicates very positive sentiment. On the other hand, subjectivity score measures whether the text contaiins factual information or personal opinion. Subjectivity scores range from 0 to 1, where 0 indicates factual information and 1 indicates personal opinion.\n\nfrom textblob import TextBlob\n\n\n# Determining Polarity \nTextBlob(my_text).sentiment.polarity\n\n0.028571428571428574\n\n\n\n# Determining Subjectivity \nTextBlob(my_text).sentiment.subjectivity\n\n0.21706349206349207\n\n\n    In the above analysis, we see the polarity score is 0.02857, which is very close to zero. Therefore, we can say our text is neutral. On the other hand, subjectivity score is 0.21706, which is close to 0, indicating that our text is factual information (not personal opinion).\n    Now, we are going to measure the polarity and subjectivity of our customer rieveiw data.\n\nproduct1 = customer_reviews[customer_reviews['ProductId'] == \"B003VXFK44\"].reset_index()\nproduct1['polarity'] = product1['Text'].apply(lambda x: TextBlob(x).sentiment.polarity)\nproduct1['subjectivity'] = product1['Text'].apply(lambda x: TextBlob(x).sentiment.subjectivity)",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#readability-index",
    "href": "nlp.html#readability-index",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.8 Readability Index",
    "text": "10.8 Readability Index\n\n    Flesch Readability\n    Kincaid Readability\n    Gunning Fog Index Readability\n    Smog Index Readability\n\n\n    We are going to collect Item 7 - Management Discussion & Analysis (MD&A) and Item 1A - Risk Factors from 10-K for some companies and calculate their readability\n\n# edgartools - my version is 3.13.0\n# pip install edgartools # latest version on 03.12.2025 is 3.13.0\n# pip install edgartools==2.31.1 # to install a specific version\n# pip list - shows all packages with their versions \nfrom edgar import *\n\n\n# Tell the SEC who you are\nset_identity(\"YOUREMAIL@outlook.com\")\n\n\nfilings2 = get_filings(form='10-K', amendments=False, filing_date=\"2024-12-01:2024-12-05\")\nfilings2_df = filings2.to_pandas()\n\n# Create a list to store the Item 1c text\nitems_texts = []\n\nfor n, filing in enumerate(filings2):\n    url = filing.url\n    cik = filing.cik\n    filing_date = filing.header.filing_date,\n    reporting_date = filing.header.period_of_report,\n    comn = filing.company\n\n    # Extract the text for Item 1c\n    TenK = filing.obj()\n    \n    # Bypass None values\n    try:\n        item7_text = TenK['Item 7'] # Management Discussion & Analysis (MDA)\n        item1a_text = TenK['Item 1A'] # Risk Factors \n    except:\n        item7_text = None\n        item1a_text = None\n\n    # Append the data to the list\n    # item1c_text = TenK['Item 1C']\n    items_texts.append({\n        'CIK': cik,\n        'Filing Date': str(filing_date),\n        'Item 7 Text': item7_text,\n        'Item 1a Text': item1a_text,\n        'url': url,\n        'reporting_date': str(reporting_date),\n        'comn': comn\n    })\n\n# Create a DataFrame from the Item 1c text data\nitems_df = pd.DataFrame(items_texts)\n\n\nimport readability # for readability \nimport textstat # for readability \n# Flesch Readability \nitems_df['Item7_Flesch_read'] = items_df['Item 7 Text'].apply (lambda x: textstat.flesch_reading_ease(x))\nitems_df['Item7_Flesch2_read'] = items_df['Item 7 Text'].apply (lambda x: readability.getmeasures(x)['readability grades']['FleschReadingEase'])\n# Kincaid Readability \nitems_df['Item7_kincaid_read'] = items_df['Item 7 Text'].apply (lambda x: textstat.flesch_kincaid_grade(x))\nitems_df['Item7_kincaid2_read'] = items_df['Item 7 Text'].apply (lambda x: readability.getmeasures(x)['readability grades']['Kincaid'])\n# Gunning Fog Index\nitems_df['Item7_GunningFogIndex_read'] = items_df['Item 7 Text'].apply (lambda x: textstat.gunning_fog(x))\nitems_df['Item7_GunningFogIndex2_read'] = items_df['Item 7 Text'].apply (lambda x: readability.getmeasures(x)['readability grades']['GunningFogIndex'])\n# Smog Index\nitems_df['Item7_SmogIndex_read'] = items_df['Item 7 Text'].apply (lambda x: textstat.smog_index(x))\nitems_df['Item7_SmogIndex2_read'] = items_df['Item 7 Text'].apply (lambda x: readability.getmeasures(x)['readability grades']['SMOGIndex'])\n\n# Total words and sentences \nitems_df['Item7_TotalWords'] = items_df['Item 7 Text'].apply (lambda x: readability.getmeasures(x)['sentence info']['words'])\nitems_df['Item7_TotalSentences'] = items_df['Item 7 Text'].apply (lambda x: readability.getmeasures(x)['sentence info']['sentences'])",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#text-similarity",
    "href": "nlp.html#text-similarity",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.9 Text Similarity",
    "text": "10.9 Text Similarity\n\n    Jaccard Similarity\n\n# Jaccard Similarity \ndef jaccard_similarity(list1, list2):\n    s1 = set(list1)\n    s2 = set(list2)\n    return float(len(s1.intersection(s2)) / len(s1.union(s2)))\n\njaccard_similarity(items_df['Item 7 Text'][0].split(),\n                items_df['Item 7 Text'][1].split())\n\njaccard_similarity(items_df['Item 7 Text'][0].split(),\n                items_df['Item 1a Text'][5].split())\n\n0.1627382146439318\n\n\n    Cosine Similarity\n\nimport math\nfrom collections import Counter\n\ndef counter_cosine_similarity(c1, c2):\n    terms = set(c1).union(c2)\n    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n    return dotprod / (magA * magB)\n\n\ncounter_cosine_similarity(Counter(items_df['Item 7 Text'][0]),\n                Counter(items_df['Item 7 Text'][5]))\n\n0.9004806355027185\n\n\n\n# Alternatively, using sklearn we can compute cosine similiarity\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Sample texts\ntext1 = items_df['Item 7 Text'][0]\ntext2 = items_df['Item 7 Text'][5]\n\n# Convert text to vectorized representation\nvectorizer = CountVectorizer()\nvectors = vectorizer.fit_transform([text1, text2])\n\n# Compute cosine similarity\ncos_sim = cosine_similarity(vectors[0], vectors[1])\nprint(\"Cosine Similarity:\", cos_sim[0][0])\n\nCosine Similarity: 0.8997538050603816",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#conclusion",
    "href": "nlp.html#conclusion",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.10 Conclusion",
    "text": "10.10 Conclusion",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#footnotes",
    "href": "nlp.html#footnotes",
    "title": "10  Natural Language Processing (NLP)",
    "section": "",
    "text": "The collection of documents is called corpus.↩︎\nYou might find some discrepencies between Equation 10.1 tf-idf scores and tf-idf scores generated by different packages because the packages uses a slightly different formula that might include some kinds of smoothing.↩︎",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "11  Machine Learning",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#introduction",
    "href": "machine_learning.html#introduction",
    "title": "11  Machine Learning",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\n\n     Machine Learning (ML), which is a subset of Artificial Intelligence (AI)1, has been woven into the fabric of our daily lives (Reddi 2025). Since waking up from the sleep until we go to the bed, we keep interacting with ML based products or services. For example, while commuting to the office, our GPS suggests the best route; at work, email filters spam (Reddi 2025). Therefore, it is dubbed AI is everywhere. While 18th and 19th centuries are characterized by Industrial Revolution, and 20th century is by Computer and Internet - the 21st century seems to be dominated by ML and AI.\n     It goes without saying that AI/ML has potential to significantly transform accounting and auditing profession in many ways. For example, repetitive tasks such as data entry, reconiliation, verification can be subordinated to AI, which allows accountants and auditors to get involved more with stratgeic decision making. Even in strategic decision making, AI facilitates effective and efficient decision making by identifying and processing vast amount of the unstructured data. Moreover, ML systems allow continuous montioring of finanical activites and real time insights of potential risks, thus enhancing the accuracy of financial reporting. Therefore, understanding how ML systems work is important for the future generation of accountants and auditors.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#importance-of-machine-learning-in-accounting",
    "href": "machine_learning.html#importance-of-machine-learning-in-accounting",
    "title": "11  Machine Learning",
    "section": "11.2 Importance of Machine Learning in Accounting",
    "text": "11.2 Importance of Machine Learning in Accounting\n\n     Accountants and auditors have already started leveraging the power of AI in their day to day business. For example, auditors are using AI/ML to review and extract information from contracts, thus reducing time and human errors (Schatsky, Muraskin, and Ragu 2015). The auditing process is being automated gradually, reducing the time spent on reviewing the audit documentations and allowing more time to spend on professional judgment (Schatsky, Muraskin, and Ragu 2015; Persico, Sidhu, et al. 2017). For example, audit procedure - confirmation - is handled by AI-enabled system, ultimately providing the auditors with the relevant documentation for professional judgment. AI-enabled drone using computer vision is being used in accounting profession to count inventory and communicate the data with the auditors in real time, thus allowing more data to be captured and allowing auditors to focus on risk management (Vien 2018). Academic research also supports that AI/ML is enhancing the performance of accountants and auditors (Fedyk et al. 2022; Estep, Griffith, and MacKenzie 2024).\n     Recently, accounting firms have turned their focus on Generative Artificial Intelligence (GenAI). For example, PricewaterhouseCoopers dedicated $1 billion investment in GenAI, partnering with Microsoft and OpenAI to automate aspects of its tax, audit and consulting services (Loten 2023). Other accounting firms started following the same feat (Deloitte 2024). Moreover, big four accounting firms are developing their own chatbot like ChatGPT. For example, EY rolled out EY.ai EYQ - a large language model - to aid EY employees in ideation, research, and creation; and to aid clients to capture the transformative power of AI to transform their businesses through confident and responsible adoption of AI (EY 2023). To sum up, all these developments highlight the importance of AI/ML in accounting. Therefore, future generations of accountants should prepare themselves to embrace these technologies, develop relevant skills, and stay updated with the latest advancements in AI and ML. This will enable them to leverage AI/ML tools effectively, enhance their professional capabilities, and contribute to the evolving landscape of the accounting profession.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#types-of-machine-learning",
    "href": "machine_learning.html#types-of-machine-learning",
    "title": "11  Machine Learning",
    "section": "11.3 Types of Machine Learning",
    "text": "11.3 Types of Machine Learning\n\n     Machine learning can be categorized into different types based on the the nature of learning process and the type of data it used. Usually, ML can be categorized into three types - supervised, unsupervised, and reinforcement learning. Figure 11.1 shows different types of machine learning.\n\n\n\n\n\n\nFigure 11.1: Machine Learning Types\n\n\n\n\n11.3.1 Supervised Machine Learning\n\n\n11.3.2 Unsupervised Machine Learning\n\n\n11.3.3 Reinforcement Learning",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#roadmap-for-building-ml-systems",
    "href": "machine_learning.html#roadmap-for-building-ml-systems",
    "title": "11  Machine Learning",
    "section": "11.4 Roadmap for Building ML Systems",
    "text": "11.4 Roadmap for Building ML Systems\n\n     In the previous sections, we discuss about machine learning and different types of ML. In this section, we will discuss typical workflow (also called roadmap) for using machine systems (Raschka, Liu, and Mirjalili 2022). Figure 11.2 shows the typical workflow when ML systems are developed.\n\n\n\n\n\n\nFigure 11.2: Machine Learning Roadmap",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#example-of-a-machine-learning-system",
    "href": "machine_learning.html#example-of-a-machine-learning-system",
    "title": "11  Machine Learning",
    "section": "11.5 Example of a Machine Learning System",
    "text": "11.5 Example of a Machine Learning System\n\n     Using a dataset, machine learning roadmap or workflow is illustrated below. A supervised machine learning for classification is used.\n\n11.5.1 Preprocessing Data\n     Since data rarely come into the form ready to feed into an ML system, preprocessing is an important step before feeding the data into an ML system. Preprocessing invloves getting the data ready into shape so that data become ready to feed into an ML system. Based on the condition or circumstances of the data, we need to decide what kinds of preprocessing we will perform on the dataset. Usually, in preprocessing, we clean and transform the dataset. Some of the cleaning includes - fixing the name of the fixtures, removing missing rows, changing the types of the features and so on.\n\n# importing necessary python packages \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization purposes\nimport seaborn as sns # for data visualization\nimport sklearn # for Machine Learning\n\n\n# importing Dataset \ndf = pd.read_csv(\"DATA/breast-cancer-wisconsin.txt\", header=None)\n\n\n# meta data \ndf.shape\ndf.info()\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 699 entries, 0 to 698\nData columns (total 11 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   0       699 non-null    int64 \n 1   1       699 non-null    int64 \n 2   2       699 non-null    int64 \n 3   3       699 non-null    int64 \n 4   4       699 non-null    int64 \n 5   5       699 non-null    int64 \n 6   6       699 non-null    object\n 7   7       699 non-null    int64 \n 8   8       699 non-null    int64 \n 9   9       699 non-null    int64 \n 10  10      699 non-null    int64 \ndtypes: int64(10), object(1)\nmemory usage: 60.2+ KB\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n0\n1000025\n5\n1\n1\n1\n2\n1\n3\n1\n1\n2\n\n\n1\n1002945\n5\n4\n4\n5\n7\n10\n3\n2\n1\n2\n\n\n2\n1015425\n3\n1\n1\n1\n2\n2\n3\n1\n1\n2\n\n\n3\n1016277\n6\n8\n8\n1\n3\n4\n3\n7\n1\n2\n\n\n4\n1017023\n4\n1\n1\n3\n2\n1\n3\n1\n1\n2\n\n\n\n\n\n\n\n\n\n# assigning column names \ncol_names = ['Id', 'Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape', 'Marginal_Adhesion','Single_Epithelial_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class']\ndf.columns = col_names\ndf.columns\ndf.info()\ndf.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 699 entries, 0 to 698\nData columns (total 11 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Id                           699 non-null    int64 \n 1   Clump_thickness              699 non-null    int64 \n 2   Uniformity_Cell_Size         699 non-null    int64 \n 3   Uniformity_Cell_Shape        699 non-null    int64 \n 4   Marginal_Adhesion            699 non-null    int64 \n 5   Single_Epithelial_Cell_Size  699 non-null    int64 \n 6   Bare_Nuclei                  699 non-null    object\n 7   Bland_Chromatin              699 non-null    int64 \n 8   Normal_Nucleoli              699 non-null    int64 \n 9   Mitoses                      699 non-null    int64 \n 10  Class                        699 non-null    int64 \ndtypes: int64(10), object(1)\nmemory usage: 60.2+ KB\n\n\n\n\n\n\n\n\n\n\nId\nClump_thickness\nUniformity_Cell_Size\nUniformity_Cell_Shape\nMarginal_Adhesion\nSingle_Epithelial_Cell_Size\nBare_Nuclei\nBland_Chromatin\nNormal_Nucleoli\nMitoses\nClass\n\n\n\n\n0\n1000025\n5\n1\n1\n1\n2\n1\n3\n1\n1\n2\n\n\n1\n1002945\n5\n4\n4\n5\n7\n10\n3\n2\n1\n2\n\n\n2\n1015425\n3\n1\n1\n1\n2\n2\n3\n1\n1\n2\n\n\n3\n1016277\n6\n8\n8\n1\n3\n4\n3\n7\n1\n2\n\n\n4\n1017023\n4\n1\n1\n3\n2\n1\n3\n1\n1\n2\n\n\n\n\n\n\n\n\n\n# Dropping redundant columns \ndf.drop('Id', axis=1, inplace=True)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 699 entries, 0 to 698\nData columns (total 10 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   Clump_thickness              699 non-null    int64 \n 1   Uniformity_Cell_Size         699 non-null    int64 \n 2   Uniformity_Cell_Shape        699 non-null    int64 \n 3   Marginal_Adhesion            699 non-null    int64 \n 4   Single_Epithelial_Cell_Size  699 non-null    int64 \n 5   Bare_Nuclei                  699 non-null    object\n 6   Bland_Chromatin              699 non-null    int64 \n 7   Normal_Nucleoli              699 non-null    int64 \n 8   Mitoses                      699 non-null    int64 \n 9   Class                        699 non-null    int64 \ndtypes: int64(9), object(1)\nmemory usage: 54.7+ KB\n\n\n\n# changing the types of variables \ndf['Bare_Nuclei'] = pd.to_numeric(df['Bare_Nuclei'], errors='coerce')\ndf.dtypes\n\nClump_thickness                  int64\nUniformity_Cell_Size             int64\nUniformity_Cell_Shape            int64\nMarginal_Adhesion                int64\nSingle_Epithelial_Cell_Size      int64\nBare_Nuclei                    float64\nBland_Chromatin                  int64\nNormal_Nucleoli                  int64\nMitoses                          int64\nClass                            int64\ndtype: object\n\n\n\n# Checking missing observations \ndf.isnull().sum() # Checking missing values in variables\ndf.isna().sum() # Checking missing values in the dataframe \n\nClump_thickness                 0\nUniformity_Cell_Size            0\nUniformity_Cell_Shape           0\nMarginal_Adhesion               0\nSingle_Epithelial_Cell_Size     0\nBare_Nuclei                    16\nBland_Chromatin                 0\nNormal_Nucleoli                 0\nMitoses                         0\nClass                           0\ndtype: int64\n\n\n\n# summary statistics \nround(df.describe(),2).transpose()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nClump_thickness\n699.0\n4.42\n2.82\n1.0\n2.0\n4.0\n6.0\n10.0\n\n\nUniformity_Cell_Size\n699.0\n3.13\n3.05\n1.0\n1.0\n1.0\n5.0\n10.0\n\n\nUniformity_Cell_Shape\n699.0\n3.21\n2.97\n1.0\n1.0\n1.0\n5.0\n10.0\n\n\nMarginal_Adhesion\n699.0\n2.81\n2.86\n1.0\n1.0\n1.0\n4.0\n10.0\n\n\nSingle_Epithelial_Cell_Size\n699.0\n3.22\n2.21\n1.0\n2.0\n2.0\n4.0\n10.0\n\n\nBare_Nuclei\n683.0\n3.54\n3.64\n1.0\n1.0\n1.0\n6.0\n10.0\n\n\nBland_Chromatin\n699.0\n3.44\n2.44\n1.0\n2.0\n3.0\n5.0\n10.0\n\n\nNormal_Nucleoli\n699.0\n2.87\n3.05\n1.0\n1.0\n1.0\n4.0\n10.0\n\n\nMitoses\n699.0\n1.59\n1.72\n1.0\n1.0\n1.0\n1.0\n10.0\n\n\nClass\n699.0\n2.69\n0.95\n2.0\n2.0\n2.0\n4.0\n4.0\n\n\n\n\n\n\n\n\n\n# visualization \n\n# Set the figure size\nplt.figure(figsize=(15, 20))\n\n# Create subplots\nfor i, column in enumerate(df.drop(columns = ['Class']).columns, 1):\n    plt.subplot(5, 2, i)\n    sns.histplot(df[column], bins=10, color = \"orangered\")\n    plt.title(f'Histogram of {column}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Discovering pattern and relation \ncorrelation = df.corr()\ncorrelation['Class'].sort_values(ascending=False)\n\nClass                          1.000000\nBare_Nuclei                    0.822696\nUniformity_Cell_Shape          0.818934\nUniformity_Cell_Size           0.817904\nBland_Chromatin                0.756616\nClump_thickness                0.716001\nNormal_Nucleoli                0.712244\nMarginal_Adhesion              0.696800\nSingle_Epithelial_Cell_Size    0.682785\nMitoses                        0.423170\nName: Class, dtype: float64\n\n\n     The correlation coefficient ranges from -1 to +1. When it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is a strong positive correlation between Class and Bare_Nuclei, Class and Uniformity_Cell_Shape, Class and Uniformity_Cell_Size. When it is close to -1, it means that there is a strong negative correlation. When it is close to 0, it means that there is no correlation. We can see that all the variables are positively correlated with Class variable. Some variables are strongly positive correlated while some variables are negatively correlated.\n\nplt.figure(figsize=(10,8))\nplt.title('Correlation of Attributes with Class variable')\na = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white')\na.set_xticklabels(a.get_xticklabels(), rotation=90)\na.set_yticklabels(a.get_yticklabels(), rotation=30)           \nplt.show()\n\n\n\n\n\n\n\n\n\n\n11.5.2 Declaring Feature Vector and Target Vector\n     In machine learning, the terms “feature vector” and “target vector” are fundamental concepts. A feature vector is a collection of input variables (features) that are used to make predictions. Each feature represents a measurable property or characteristic of the data. The feature vector is typically represented as a row in a dataset, where each column corresponds to a different feature. On the otherhand, The target vector, also known as the response vector or label, is the output variable that the model aims to predict. It represents the ground truth or actual values that correspond to the feature vectors. In our dataset, the target vector is Class and all other variables (features) are feature vector.\n\n# Feature Vector \nX = df.drop(['Class'], axis = 1) \n# Target Vector \ny = df[\"Class\"]\n\n\n\n11.5.3 Training and Testing Dataset\n     The training dataset is the portion of the data used to train the machine learning model. It contains input-output pairs where the input is the feature vector, and the output is the target vector. The model learns the relationship between the inputs and outputs during the training process. The testing dataset is the portion of the data used to evaluate the performance of the trained model. It contains input-output pairs that the model has not seen during training. The testing dataset helps assess how well the model generalizes to new, unseen data.\n     Splitting the dataset into training and testing sets is a very important step in building and evaluating machine learning models. We need to split the dataset into training and testing for the following reasons.\n\nModel Evaluation: The testing dataset is used to evaluate the model’s performance on new, unseen data. This helps determine how well the model generalizes to real-world scenarios. Overfitting occurs when a model performs well on the training data but poorly on new data. By evaluating the model on a separate testing set, we can detect and mitigate overfitting. If we use the same dataset for developing and evaluating model, then model performance will be overstated.\nHyperparameter Tuning: To optimize the performance of machine learning models, we need to identify the best parameters2 for the model. Hyperparameter tuning is the process of identifying the best parameters for optimizing the performance of machine learning models. During the training process, hyperparameters (e.g., value of \\(k\\) in KNN algorithm) are tuned to improve the model’s performance. The testing set helps validate that these parameters are optimized for generalization, not just for the training data.\nBias-Variance Tradeoff: Splitting the data helps in managing the bias-variance tradeoff. A model with high bias may underfit the data, while a model with high variance may overfit. Evaluating on a testing set helps find the right balance.\n\n\n# Split X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n\n# check the shape of X_train and X_test\nX_train.shape, X_test.shape\n\n((559, 9), (140, 9))\n\n\n\n\n11.5.4 Feature Engineering\n     Feature engineering is the process of creating new features or modifying existing ones to improve the performance of a machine learning model. It involves transforming raw data into meaningful features that better represent the underlying patterns and relationships in the data. Effective feature engineering can significantly enhance the accuracy and efficiency of machine learning models. Key Steps in Feature Engineering -\n\nData Cleaning:\n\nHandling Missing Values: Filling in or removing missing data points.\nRemoving Outliers: Identifying and eliminating data points that deviate significantly from the rest of the data.\n\nFeature Creation:\n\nCombining Features: Creating new features by combining existing ones (e.g., creating a “total sales” feature by summing individual sales columns).\nExtracting Features: Deriving new features from existing data (e.g., extracting the day of the week from a date column).\n\nFeature Transformation:\n\nScaling: Normalizing or standardizing features to ensure they have similar scales (e.g., using Min-Max scaling or Z-score normalization).\nEncoding Categorical Variables: Converting categorical variables into numerical representations (e.g., one-hot encoding or label encoding).\n\nFeature Selection:\n\nRemoving Redundant Features: Eliminating features that provide little or no additional information.\nSelecting Important Features: Identifying and retaining the most relevant features using techniques like correlation analysis, mutual information, or feature importance scores from models.\n\nDimensionality Reduction:\n\nPrincipal Component Analysis (PCA): Reducing the number of features while preserving the most important information.\nt-Distributed Stochastic Neighbor Embedding (t-SNE): Visualizing high-dimensional data in lower dimensions.\n\n\n\n# check missing values in numerical variables in X_train\nX_train.isnull().sum()\n\nClump_thickness                 0\nUniformity_Cell_Size            0\nUniformity_Cell_Shape           0\nMarginal_Adhesion               0\nSingle_Epithelial_Cell_Size     0\nBare_Nuclei                    13\nBland_Chromatin                 0\nNormal_Nucleoli                 0\nMitoses                         0\ndtype: int64\n\n\n\n# check missing values in numerical variables in X_test\nX_test.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    3\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\n\n# print percentage of missing values in the numerical variables in training set\nfor col in X_train.columns:\n    if X_train[col].isnull().mean()&gt;0:\n        print(col, round(X_train[col].isnull().mean(),4))\n\nBare_Nuclei 0.0233\n\n\n     It is assumed that the data are missing completely at random (MCAR). There are two methods which can be used to impute missing values. One is mean or median imputation and other one is random sample imputation. When there are outliers in the dataset, we should use median imputation. So, I will use median imputation because median imputation is robust to outliers.\n     I will impute missing values with the appropriate statistical measures of the data, in this case median. Imputation should be done over the training set, and then propagated to the test set. It means that the statistical measures to be used to fill missing values both in train and test set, should be extracted from the train set only. This is to avoid overfitting.\n\n# impute missing values in X_train and X_test with respective column median in X_train\nfor df in [X_train, X_test]:\n    for col in X_train.columns:\n        col_median=X_train[col].median()\n        df[col].fillna(col_median, inplace=True)  \n\n\n# check again missing values in numerical variables in X_train\nX_train.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    0\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\n\n# check missing values in numerical variables in X_test\nX_test.isnull().sum()\n\nClump_thickness                0\nUniformity_Cell_Size           0\nUniformity_Cell_Shape          0\nMarginal_Adhesion              0\nSingle_Epithelial_Cell_Size    0\nBare_Nuclei                    0\nBland_Chromatin                0\nNormal_Nucleoli                0\nMitoses                        0\ndtype: int64\n\n\n     Now, we will perform feature selection. Feature selection involves identifying the features that have the greatest explanatory power to predict the target variables. Therere are many techniques that can be used for feature selection. When there are many variables, using feature selection is very much important. Otherwise, noises might be introduced in the model.\n\n# feature selectiong using Chi-square test \nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\nchi_feature = SelectKBest(chi2, k = 4).fit(X_train, y_train)\nprint ('Score: ', chi_feature.scores_)\nprint ('Features: ', X_train.columns)\n\nScore:  [ 493.52822606 1144.54028237 1046.47720631  844.84992698  404.07603727\n 1364.87717094  532.42045225  951.992508    180.65784477]\nFeatures:  Index(['Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape',\n       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses'],\n      dtype='object')\n\n\n\n# feature selection using ANOVA \nanova_feature = SelectKBest (f_classif, k = 4).fit(X_train, y_train)\nprint ('Scores: ', anova_feature.scores_)\nprint ('Features: ', X_train.columns)\n\nScores:  [ 545.20592252 1142.16232995 1093.33969603  582.15032766  518.143572\n 1022.11887635  716.06901174  593.48561929  124.1776932 ]\nFeatures:  Index(['Clump_thickness', 'Uniformity_Cell_Size', 'Uniformity_Cell_Shape',\n       'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei',\n       'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses'],\n      dtype='object')\n\n\n     We can rank the features based on their scores. Higher scores indicate more relevant feature. Therefore, chi-square test above indicate that the four relevant features are - Clump_thickness, Uniformity_Cell_Size, Uniformity_Cell_Shape, and Marginal_Adhesion. Using ANOVA suggests the same results.\n     Now, we will perform feature scaling. Normalizing or standardizing features to ensure they have similar scales (e.g., using Min-Max scaling or Z-score normalization). Feature scaling is important because some algorimthms (such as KNN, Support Vector Machines) in ML are sensitive to the scale of features. Scaling ensures that all features contribute equally to the model’s predictions. Moreover, if features are not scaled, features with larger values tend to dominate learning process in ML, thus leading to bias in results. Therefore, scaling ensures that each feature contributes equally to the model. Moreover, distance-based alogrithms perform better when features are scaled.\n\n# Feature Scaling \ncols = X_train.columns\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])\nX_train.head()\n\n\n\n\n\n\n\n\n\nClump_thickness\nUniformity_Cell_Size\nUniformity_Cell_Shape\nMarginal_Adhesion\nSingle_Epithelial_Cell_Size\nBare_Nuclei\nBland_Chromatin\nNormal_Nucleoli\nMitoses\n\n\n\n\n0\n2.028383\n0.299506\n0.289573\n1.119077\n-0.546543\n1.858357\n-0.577774\n0.041241\n-0.324258\n\n\n1\n1.669451\n2.257680\n2.304569\n-0.622471\n3.106879\n1.297589\n-0.159953\n0.041241\n-0.324258\n\n\n2\n-1.202005\n-0.679581\n-0.717925\n0.074148\n-1.003220\n-0.104329\n-0.995595\n-0.608165\n-0.324258\n\n\n3\n-0.125209\n-0.026856\n-0.046260\n-0.622471\n-0.546543\n-0.665096\n-0.159953\n0.041241\n-0.324258\n\n\n4\n0.233723\n-0.353219\n-0.382092\n-0.274161\n-0.546543\n-0.665096\n-0.577774\n-0.283462\n-0.324258",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#machine-learning-algorithms",
    "href": "machine_learning.html#machine-learning-algorithms",
    "title": "11  Machine Learning",
    "section": "11.6 Machine Learning Algorithms",
    "text": "11.6 Machine Learning Algorithms\n     Now, the data is ready to feed to a Machine Learning algorithm. An algorithm is a step-by-step procedure or set of rules designed to perform a specific task or solve a particular problem. It is a sequence of instructions that takes an input, processes it, and produces an output. Machine learning algorithms are the backbone of machine learning systems. They are used to build models that can learn from data and make predictions or decisions. Different algorthims learn different ways from the data. For different ML, different kinds of algorithms are used. For example, for classification supervised machine learning, we can use KNN algorithm, logistic regression, decision trees, and support vector machines.\n\n11.6.1 K Nearnest Neighbor (KNN) Algorithm\n     KNN is a simple, yet powerful, machine learning supervised algorithm. KNN is an instance based ML algorithm because it uses similarity between data points to make a prediction. To measure the similarity between data points, KNN relies on distance metrics such as Euclidean distance, or Manhattan distance, or Minkowski distance. KNN is called lazy learner because all of its computation is done in prediction phase. The K in KNN represents the number of nearest neighbors to consider when making the prediction. For example, when K = 2, the number of neareast number that will be used is 2.\n     Assume you are trying to build a model that will classify reptiles into one of three classes (Rhys 2020). The classes are - Adder, Grass Snake, and Slow worm. We have two features of the reptiles - length of it and how aggressive (aggressiveness) the reptile is - which we will use to classify the reptiles. Further assume that a reptile expert helps you manually classify the observations so far, but you decide to build a kNN classifier to help you quickly classify future specimens you come across. Figure 11.3 plots the data before classification. Each of the reptile is plotted against body length and aggressiveness and we have three new reptiles that need to be classified.\n\n\n\n\n\n\nFigure 11.3: Reptile Characteristics\n\n\n\n     To predict the classifiction of the new reptiles, KNN will use distance based metrics as mentioned above and calculate the distance between each new unlabelled cases and all the labeled cases. Then, for each new unlabelled case, the algorithm ranks the neighbors from the nearest (most similar) to the furthest (the least similar) (Rhys 2020). Figure 11.4 shows the ranking of the neighbors. The algorithm identifies K-labeled cases (neighbors) nearest to each unlabeled cas. We actually determine the value of K. Each of the K-nearest neighbor cases votes to which class the unlabeled case belongs. Figure 11.5 shows the membership of new cases based on the value of K.\n\n\n\n\n\n\nFigure 11.4: Ranking the Neighbors\n\n\n\n\n\n\n\n\n\nFigure 11.5: Different K Values\n\n\n\n\n\n11.6.2 Building First KNN Model\n\n# import KNeighbors ClaSSifier from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n# instantiate the model\nknn = KNeighborsClassifier(n_neighbors=3)\n# fit the model to the training set\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\n\n# prediction using test data \ny_pred = knn.predict(X_test)\ny_pred\n\narray([2, 2, 4, 2, 4, 2, 4, 2, 4, 2, 2, 2, 4, 4, 4, 2, 2, 4, 4, 2, 4, 4,\n       2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2,\n       4, 4, 2, 4, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 4,\n       4, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 4, 2,\n       4, 4, 2, 2, 2, 4, 2, 2, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2,\n       4, 4, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 4, 2, 2, 4, 4, 4, 4, 4, 2,\n       2, 4, 4, 2, 2, 4, 2, 2], dtype=int64)\n\n\n     predict_proba () method gives the probabilities for the target variable(2 and 4) in this case, in array form. 2 is for probability of benign cancer and 4 is for probability of malignant cancer.\n\n# Prediction probability\nknn.predict_proba(X_test)[0:11]\n\narray([[1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.33333333, 0.66666667],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.66666667, 0.33333333],\n       [1.        , 0.        ]])\n\n\n\n# probability of getting output as 2 - benign cancer\nknn.predict_proba(X_test)[:,0]\n\narray([1.        , 1.        , 0.33333333, 1.        , 0.        ,\n       1.        , 0.        , 1.        , 0.        , 0.66666667,\n       1.        , 1.        , 0.        , 0.33333333, 0.        ,\n       1.        , 1.        , 0.        , 0.        , 1.        ,\n       0.        , 0.        , 1.        , 1.        , 1.        ,\n       0.        , 1.        , 1.        , 0.        , 0.        ,\n       1.        , 1.        , 1.        , 1.        , 1.        ,\n       0.66666667, 1.        , 0.        , 1.        , 1.        ,\n       1.        , 1.        , 1.        , 1.        , 0.        ,\n       0.        , 1.        , 0.        , 1.        , 0.        ,\n       0.        , 1.        , 1.        , 0.        , 1.        ,\n       1.        , 1.        , 1.        , 0.66666667, 1.        ,\n       0.        , 1.        , 1.        , 0.        , 0.        ,\n       0.33333333, 0.        , 1.        , 1.        , 0.        ,\n       1.        , 1.        , 0.        , 0.        , 1.        ,\n       1.        , 1.        , 1.        , 0.        , 1.        ,\n       1.        , 1.        , 0.        , 1.        , 1.        ,\n       1.        , 0.        , 1.        , 0.        , 0.        ,\n       1.        , 1.        , 0.66666667, 0.        , 1.        ,\n       1.        , 1.        , 0.        , 1.        , 0.        ,\n       0.        , 1.        , 1.        , 1.        , 0.        ,\n       1.        , 1.        , 1.        , 1.        , 1.        ,\n       0.        , 0.33333333, 0.        , 1.        , 1.        ,\n       1.        , 1.        , 1.        , 0.        , 0.        ,\n       0.        , 0.33333333, 1.        , 0.        , 1.        ,\n       1.        , 0.33333333, 0.33333333, 0.        , 0.        ,\n       0.        , 1.        , 1.        , 0.33333333, 0.        ,\n       1.        , 1.        , 0.        , 1.        , 1.        ])\n\n\n\n# probability of getting output as 4 - malignant cancer\nknn.predict_proba(X_test)[:,1]\n\narray([0.        , 0.        , 0.66666667, 0.        , 1.        ,\n       0.        , 1.        , 0.        , 1.        , 0.33333333,\n       0.        , 0.        , 1.        , 0.66666667, 1.        ,\n       0.        , 0.        , 1.        , 1.        , 0.        ,\n       1.        , 1.        , 0.        , 0.        , 0.        ,\n       1.        , 0.        , 0.        , 1.        , 1.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       0.33333333, 0.        , 1.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 0.        , 1.        ,\n       1.        , 0.        , 1.        , 0.        , 1.        ,\n       1.        , 0.        , 0.        , 1.        , 0.        ,\n       0.        , 0.        , 0.        , 0.33333333, 0.        ,\n       1.        , 0.        , 0.        , 1.        , 1.        ,\n       0.66666667, 1.        , 0.        , 0.        , 1.        ,\n       0.        , 0.        , 1.        , 1.        , 0.        ,\n       0.        , 0.        , 0.        , 1.        , 0.        ,\n       0.        , 0.        , 1.        , 0.        , 0.        ,\n       0.        , 1.        , 0.        , 1.        , 1.        ,\n       0.        , 0.        , 0.33333333, 1.        , 0.        ,\n       0.        , 0.        , 1.        , 0.        , 1.        ,\n       1.        , 0.        , 0.        , 0.        , 1.        ,\n       0.        , 0.        , 0.        , 0.        , 0.        ,\n       1.        , 0.66666667, 1.        , 0.        , 0.        ,\n       0.        , 0.        , 0.        , 1.        , 1.        ,\n       1.        , 0.66666667, 0.        , 1.        , 0.        ,\n       0.        , 0.66666667, 0.66666667, 1.        , 1.        ,\n       1.        , 0.        , 0.        , 0.66666667, 1.        ,\n       0.        , 0.        , 1.        , 0.        , 0.        ])\n\n\n\n\n11.6.3 Model Evaluation (Accuracy Score)\n\nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\nModel accuracy score: 0.9714\n\n\n     So, the ML model predicts the outcome variable with 97.14% accuracy.\n\n\n11.6.4 Overfitting and Underfitting\n     Overfitting and underfitting are two important sources of error in machine learning model building. In underfitting, the model is too simple or there are few predictors, thus failing to adequately describe the relationships/patterns in data. In underfitting, the model performs poorly on both training and testing data and is biased. On the other hand, in overfitting, the model is too complex or there are many predictors on the model. Thus, in overfitting, we are modeling not only the patterns in the data, but also the noise present in the dataset. In underfitting, the model performs very well on the trained data set, but performs poor on the testing data set. The problem with underfitting and obverfitting is that they reduce the generalizability of the model. Therefore, during model building process, we need to trade off between overfitting and underfitting. Figure 11.6 shows the tradeoff between overfitting and underfitting. Splitting the dataset into training and testing is one of the ways to deal with overfitting and underfitting. Splitting the data is calso called cross validation (CV), which is discussed further in Section 11.6.12.\n\n\n\n\n\n\nFigure 11.6: Overfitting vs Underfitting\n\n\n\n\ny_pred_train = knn.predict(X_train)\n# print the scores on training and test set\nprint('Training set score: {:.4f}'.format(knn.score(X_train, y_train)))\nprint('Test set score: {:.4f}'.format(knn.score(X_test, y_test)))\n\nTraining set score: 0.9821\nTest set score: 0.9714\n\n\n     The training-set accuracy score is 0.9821 while the test-set accuracy to be 0.9714. These two values are quite comparable. So, there is no question of overfitting.\n\n\n11.6.5 Comparing Model Accuracy with Null Accuracy\n     Null accuracy is a baseline measure used to evaluate the performance of a machine learning classification model. It represents the accuracy that could be achieved by always predicting the most frequent class in the dataset. Null accuracy is particularly useful for understanding how well a model performs compared to a simple, naive approach. If the model’s accuracy is lower than or equal to the null accuracy, it indicates that the model is not performing better than a naive approach of always predicting the most frequent class. In such cases, we may need to improve our model by feature engineering, or hyperparameter tuning, or using different algorithms. We can see that the occurences of most frequent class is 85. So, we can calculate null accuracy by dividing 85 by total number of occurences. We can see that our model accuracy score is 0.9714 but null accuracy score is 0.6071. So, we can conclude that our K Nearest Neighbors model is doing a very good job in predicting the class labels.\n\n# check class distribution in test set\ny_test.value_counts()\n\nClass\n2    85\n4    55\nName: count, dtype: int64\n\n\n\n# check null accuracy score\nnull_accuracy = (85/(85+55))\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))\n\nNull accuracy score: 0.6071\n\n\n\n\n11.6.6 Rebuild KNN Classification Model Using Different Values of K\n\n# instantiate the model with k=5\nknn_5 = KNeighborsClassifier(n_neighbors=5)\n# fit the model to the training set\nknn_5.fit(X_train, y_train)\n# predict on the test-set\ny_pred_5 = knn_5.predict(X_test)\nprint('Model accuracy score with k=5 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_5)))\n\nModel accuracy score with k=5 : 0.9714\n\n\n\n11.6.6.1 Rebuild KNN Classification Model Using K=6\n\n# instantiate the model with k=6\nknn_6 = KNeighborsClassifier(n_neighbors=6)\n# fit the model to the training set\nknn_6.fit(X_train, y_train)\n# predict on the test-set\ny_pred_6 = knn_6.predict(X_test)\nprint('Model accuracy score with k=6 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_6)))\n\nModel accuracy score with k=6 : 0.9786\n\n\n\n\n11.6.6.2 Rebuild KNN Classification Model Using K=7\n\n# instantiate the model with k=7\nknn_7 = KNeighborsClassifier(n_neighbors=7)\n# fit the model to the training set\nknn_7.fit(X_train, y_train)\n# predict on the test-set\ny_pred_7 = knn_7.predict(X_test)\nprint('Model accuracy score with k=7 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_7)))\n\nModel accuracy score with k=7 : 0.9786\n\n\n\n\n11.6.6.3 Rebuild KNN Classification Model Using K=8\n\n# instantiate the model with k=8\nknn_8 = KNeighborsClassifier(n_neighbors=8)\n# fit the model to the training set\nknn_8.fit(X_train, y_train)\n# predict on the test-set\ny_pred_8 = knn_8.predict(X_test)\nprint('Model accuracy score with k=8 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_8)))\n\nModel accuracy score with k=8 : 0.9786\n\n\n\n\n11.6.6.4 Rebuild KNN Classification Model Using K=9\n\n# instantiate the model with k=9\nknn_9 = KNeighborsClassifier(n_neighbors=9)\n# fit the model to the training set\nknn_9.fit(X_train, y_train)\n# predict on the test-set\ny_pred_9 = knn_9.predict(X_test)\nprint('Model accuracy score with k=9 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_9)))\n\nModel accuracy score with k=9 : 0.9714\n\n\n     Our original model accuracy score with k=3 is 0.9714. Now, we can see that we get same accuracy score of 0.9714 with k=5. But, if we increase the value of k further, this would result in enhanced accuracy. With k=6,7,8 we get accuracy score of 0.9786. So, it results in performance improvement. If we increase k to 9, then accuracy decreases again to 0.9714. Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels. But, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\n\n\n\n11.6.7 Automating the Calculation of Value K\n\nfrom sklearn import metrics\nmean_acc = np.zeros(20)\nfor i in range(1,21):\n#Train Model and Predict  \n    knn = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n    yhat= knn.predict(X_test)\n    mean_acc[i-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc\n\narray([0.95714286, 0.95      , 0.97142857, 0.96428571, 0.97142857,\n       0.97857143, 0.97857143, 0.97857143, 0.97142857, 0.97857143,\n       0.97857143, 0.97857143, 0.97142857, 0.97857143, 0.97857143,\n       0.97857143, 0.97857143, 0.97857143, 0.97857143, 0.97142857])\n\n\n\nloc = np.arange(1,21,step=1.0)\nplt.figure(figsize = (10, 6))\n# Create a line plot \nsns.lineplot(x=range(1, 21), y=mean_acc, marker='o', color= \"orangered\")\nplt.xticks(loc)\nplt.xlabel('Number of Neighbors ')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. Number of Neighbors')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n11.6.8 Hyperparameter Tuning\n     A hyperparameter is a parameter of the model that is set before the start of learning process. Different machine learning models have different hyperparameters. You can find out more about the different hyperparameters of k-NN in scikit-learn. For example, weights argument has two options - “uniform” or “distance”; p argument selects the either manhattn distance or euclidean distance.\n     Grid search and random search are two popular methods for hyperparameter tuning in machine learning. Both methods aim to find the best combination of hyperparameters that optimize the performance of a model. Grid search is an exhaustive search method that evaluates all possible combinations of hyperparameters from a predefined grid. It systematically explores the hyperparameter space by creating a grid of hyperparameter values and evaluating the model for each combination. One advantage of grid search is that it evaluates all possible combinations, ensuring that the best hyperparameters are found within the predefined grid. On the other hand, random search randomly samples hyperparameter combinations from a predefined distribution. It evaluates a fixed number of random combinations, rather than exhaustively searching the entire grid. One of the disadvantages of grid search is that it may miss the optimal combination of hyperparameters, as it does not evaluate all possible combinations.\n     We will use the Grid Search technique for hyperparameter optimization. An exhaustive grid search takes in as many hyperparameters as you would like, and tries every single possible combination of the hyperparameters as well as as many cross-validations as you would like it to perform. An exhaustive grid search is a good way to determine the best hyperparameter values to use, but it can quickly become time consuming with every additional parameter value and cross-validation that you add. Specifically, we will use three hyperparamters- n-neighbors, weights and metric.\n\nn_neighbors: Decide the best k based on the values we have computed earlier.\nweights: Check whether adding weights to the data points is beneficial to the model or not. ‘uniform’ assigns no weight, while ‘distance’ weighs points by the inverse of their distances meaning nearer points will have more weight than the farther points.\nmetric: The distance metric to be used will calculating the similarity.\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n     Since we have provided the class validation score as 3( cv= 3), Grid Search will evaluate the model 10 x 2 x 3 x 3 = 180 times with different hyperparameters.\n\ngrid_params = { 'n_neighbors' : [3,4,5,6,7,8,9,10,11,12],\n               'weights' : ['uniform','distance'],\n               'metric' : ['minkowski','euclidean','manhattan']}\ngs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n\n\n# fit the model on our train set\ng_res = gs.fit(X_train, y_train)\n\nFitting 3 folds for each of 60 candidates, totalling 180 fits\n\n\n\n# find the best score\ng_res.best_score_\n\n0.9696394686907022\n\n\n\n# get the hyperparameters with the best score\ng_res.best_params_\n\n{'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}\n\n\n\n# use the best hyperparameters\nknn = KNeighborsClassifier(n_neighbors = 5, weights = 'distance', \\\nalgorithm = 'brute',metric = 'manhattan')\nknn.fit(X_train, y_train)\n\nKNeighborsClassifier(algorithm='brute', metric='manhattan', weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(algorithm='brute', metric='manhattan', weights='distance') \n\n\n\n# get a prediction\ny_hat = knn.predict(X_train)\ny_knn = knn.predict(X_test)\n\n\nfrom sklearn import metrics\n\n\nprint('Training set accuracy: ', metrics.accuracy_score(y_train, y_hat))\nprint('Test set accuracy: ', metrics.accuracy_score(y_test, y_knn))\n\nTraining set accuracy:  1.0\nTest set accuracy:  0.9785714285714285\n\n\n\n\n11.6.9 Confusion Matrix\n     A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form. Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:\nTrue Positives (TP) – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\nTrue Negatives (TN) – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\nFalse Positives (FP) – False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called Type I error.\nFalse Negatives (FN) – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called Type II error.\nThese four outcomes are summarized in a confusion matrix given below.\n\n# Print the Confusion Matrix with k =3 and slice it into four pieces\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])\n\nConfusion matrix\n\n [[83  2]\n [ 2 53]]\n\nTrue Positives(TP) =  83\n\nTrue Negatives(TN) =  53\n\nFalse Positives(FP) =  2\n\nFalse Negatives(FN) =  2\n\n\n     The confusion matrix shows 83 + 53 = 136 correct predictions & 2 + 2 = 4 incorrect predictions. In this case, we have - True Positives (Actual Positive:1 and Predict Positive:1) - 83 True Negatives (Actual Negative:0 and Predict Negative:0) - 53 False Positives (Actual Negative:0 but Predict Positive:1) - 2 (Type I error) False Negatives (Actual Positive:1 but Predict Negative:0) - 2 (Type II error)\n\n# Print the Confusion Matrix with k =7 and slice it into four pieces\ncm_7 = confusion_matrix(y_test, y_pred_7)\nprint('Confusion matrix\\n\\n', cm_7)\nprint('\\nTrue Positives(TP) = ', cm_7[0,0])\nprint('\\nTrue Negatives(TN) = ', cm_7[1,1])\nprint('\\nFalse Positives(FP) = ', cm_7[0,1])\nprint('\\nFalse Negatives(FN) = ', cm_7[1,0])\n\nConfusion matrix\n\n [[83  2]\n [ 1 54]]\n\nTrue Positives(TP) =  83\n\nTrue Negatives(TN) =  54\n\nFalse Positives(FP) =  2\n\nFalse Negatives(FN) =  1\n\n\n\n# visualize confusion matrix with seaborn heatmap\nplt.figure(figsize=(6,4))\ncm_matrix = pd.DataFrame(data=cm_7, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='OrRd')\n\n\n\n\n\n\n\n\n\n\n11.6.10 Classification Matrices\n\n11.6.10.1 Classification Report\n     Classification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. I have described these terms in later. We can print a classification report as follows\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_7))\n\n              precision    recall  f1-score   support\n\n           2       0.99      0.98      0.98        85\n           4       0.96      0.98      0.97        55\n\n    accuracy                           0.98       140\n   macro avg       0.98      0.98      0.98       140\nweighted avg       0.98      0.98      0.98       140\n\n\n\n\n\n11.6.10.2 Classification Accuracy\n\nTP = cm_7[0,0]\nTN = cm_7[1,1]\nFP = cm_7[0,1]\nFN = cm_7[1,0]\n\n\n\n11.6.10.3 Classification Error\n\n# print classification error\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\nprint('Classification error : {0:0.4f}'.format(classification_error))\n\nClassification error : 0.0214\n\n\n\n\n11.6.10.4 Precision\n     Precision can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). Precision is a metric that tells us about the quality of positive predictions. So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class. Precision is a useful metric in cases where False Positive is a higher concern than False Negatives. Precision is important in music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business. Mathematically, precision can be defined as the ratio of TP to (TP + FP).\n\n# print precision score\nprecision = TP / float(TP + FP)\nprint('Precision : {0:0.4f}'.format(precision))\n\nPrecision : 0.9765\n\n\n\n\n11.6.10.5 Recall\n     Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). Recall tells us about how well the model identifies true positives. Recall is also called Sensitivity. Recall identifies the proportion of correctly predicted actual positives. Mathematically, recall can be given as the ratio of TP to (TP + FN). Recall is a useful metric in cases where False Negative triumphs over False Positive. Recall is important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n\nrecall = TP / float(TP + FN)\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))\n\nRecall or Sensitivity : 0.9881\n\n\n\n\n11.6.10.6 Precision vs Recall\n     Data scientists optimize their model to have higher precision or recall depending on the circumstances. A model with higher recall than precision often makes more positive predictions. A model like this comes with higher false positives and low false negatives. In scenarios like disease prediction, models should always be optimized for recall. False positives are better than false negatives in the healthcare industry.\n     On the other hand, a model with higher precision will have fewer false positives and more false negatives. If you were to build a bot detection machine learning model for an online store, you may want to optimize for higher precision, since banning legitimate users from the website will lead to a decline in sales.\n\n\n11.6.10.7 f1-Score\n     In practice, when we try to increase the precision of our model, the recall goes down, and vice-versa. The F1-score captures both the trends in a single value:\n    $f1-score = 2/((1/Recall) + (1/Precision))$\n     f1-score is the weighted harmonic mean of precision and recall, and so it gives a combined idea about these two metrics. It is maximum when Precision is equal to Recall. The best possible f1-score would be 1.0 and the worst would be 0.0. f1-score is the harmonic mean of precision and recall. So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of f1-score should be used to compare classifier models, not global accuracy.\n\n\n11.6.10.8 Support\n     Support is the actual number of occurrences of the class in our dataset.\n\n\n11.6.10.9 True Positive Rate\n     True Positive Rate is synonymous with Recall.\n\ntrue_positive_rate = TP / float(TP + FN)\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))\n\nTrue Positive Rate : 0.9881\n\n\n\n\n11.6.10.10 False Positive Rate\n\nfalse_positive_rate = FP / float(FP + TN)\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))\n\nFalse Positive Rate : 0.0357\n\n\n\n\n11.6.10.11 Specificity (True Negative Rate)\n\nspecificity = TN / (TN + FP)\nprint('Specificity : {0:0.4f}'.format(specificity))\n\nSpecificity : 0.9643\n\n\n\n\n11.6.10.12 Adjusting the Classification Threshold Level\n\n# print the first 10 predicted probabilities of two classes- 2 and 4\ny_pred_prob = knn.predict_proba(X_test)[0:10]\ny_pred_prob\n\narray([[1.        , 0.        ],\n       [1.        , 0.        ],\n       [0.21219604, 0.78780396],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [1.        , 0.        ],\n       [0.        , 1.        ],\n       [0.40998626, 0.59001374]])\n\n\nIn each row, the numbers sum to 1. There are 2 columns which correspond to 2 classes - 2 and 4.\nClass 2 - predicted probability that there is benign cancer.\nClass 4 - predicted probability that there is malignant cancer.\nImportance of predicted probabilities\nWe can rank the observations by probability of benign or malignant cancer.\n\npredict_proba process\n\nPredicts the probabilities\nChoose the class with the highest probability\n\nClassification threshold level\n\nThere is a classification threshold level of 0.5.\nClass 4 - probability of malignant cancer is predicted if probability &gt; 0.5.\nClass 2 - probability of benign cancer is predicted if probability &lt; 0.5.\n\n\n\ny_pred_prob_df = pd.DataFrame(data=y_pred_prob, \\\ncolumns=['Prob of - benign cancer (2)', 'Prob of - malignant cancer (4)'])\ny_pred_prob_df\n\n\n\n\n\n\n\n\n\nProb of - benign cancer (2)\nProb of - malignant cancer (4)\n\n\n\n\n0\n1.000000\n0.000000\n\n\n1\n1.000000\n0.000000\n\n\n2\n0.212196\n0.787804\n\n\n3\n1.000000\n0.000000\n\n\n4\n0.000000\n1.000000\n\n\n5\n1.000000\n0.000000\n\n\n6\n0.000000\n1.000000\n\n\n7\n1.000000\n0.000000\n\n\n8\n0.000000\n1.000000\n\n\n9\n0.409986\n0.590014\n\n\n\n\n\n\n\n\n\n# print the first 10 predicted probabilities for class 4 - Probability of malignant cancer\nknn.predict_proba(X_test)[0:10, 1]\n\narray([0.        , 0.        , 0.78780396, 0.        , 1.        ,\n       0.        , 1.        , 0.        , 1.        , 0.59001374])\n\n\n\n# store the predicted probabilities for class 4 - Probability of malignant cancer\ny_pred_1 = knn.predict_proba(X_test)[:, 1]\n\n\n# plot histogram of predicted probabilities\n# adjust figure size\nplt.figure(figsize=(6,4))\n# adjust the font size \nplt.rcParams['font.size'] = 12\n# plot histogram with 10 bins\n#plt.hist(y_pred_1, bins = 10)\nsns.histplot(y_pred_1, bins=10, kde=False, color='orangered')\n# set the title of predicted probabilities\nplt.title('Histogram of predicted probabilities of malignant cancer')\n# set the x-axis limit\nplt.xlim(0,1)\n# set the title\nplt.xlabel('Predicted probabilities of malignant cancer')\nplt.ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\n\n\n\n11.6.11 ROC (Receiver Operating Characteristics) - AUC (Area Under ROC Curve) Curve\n\n11.6.11.1 ROC Curve\n     Another tool to measure the classification model performance visually is ROC Curve. ROC Curve stands for Receiver Operating Characteristic Curve. An ROC Curve is a plot which shows the performance of a classification model at various classification threshold levels.\n     The ROC Curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold levels. True Positive Rate (TPR) is also called Recall. It is defined as the ratio of TP to (TP + FN). False Positive Rate (FPR) is defined as the ratio of FP to (FP + TN).\n     In the ROC Curve, we will focus on the TPR (True Positive Rate) and FPR (False Positive Rate) of a single point. This will give us the general performance of the ROC curve which consists of the TPR and FPR at various threshold levels. So, an ROC Curve plots TPR vs FPR at different classification threshold levels. If we lower the threshold levels, it may result in more items being classified as positive. It will increase both True Positives (TP) and False Positives (FP).\n\n# plot ROC Curve\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_1, pos_label=4)\nplt.figure(figsize=(6,4))\n# plt.plot(fpr, tpr, linewidth=2)\n# plt.plot([0,1], [0,1], 'k--' )\nsns.lineplot(x=fpr, y=tpr, linewidth=2, color='orangered')\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Breast Cancer kNN classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.show()\n\n\n\n\n\n\n\n\n     ROC curve help us to choose a threshold level that balances sensitivity and specificity for a particular context.\n\n\n11.6.11.2 ROC AUC\n     ROC AUC stands for Receiver Operating Characteristic - Area Under Curve. It is a technique to compare classifier performance. In this technique, we measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. So, ROC AUC is the percentage of the ROC plot that is underneath the curve.\n\n# compute ROC AUC\nfrom sklearn.metrics import roc_auc_score\nROC_AUC = roc_auc_score(y_test, y_pred_1)\nprint('ROC AUC : {:.4f}'.format(ROC_AUC))\n\nROC AUC : 0.9827\n\n\nInterpretation:\n* ROC AUC is a single number summary of classifier performance. The higher the value, the better the classifier.\n\n* ROC AUC of our model approaches towards 1. So, we can conclude that our classifier does a good job in predicting whether it is benign or malignant cancer.\n\nfrom sklearn.model_selection import cross_val_score\nCross_validated_ROC_AUC = \\\ncross_val_score(knn_7, X_train, y_train, cv=5,scoring='roc_auc').mean()\nprint('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))\n\nCross validated ROC AUC : 0.9910\n\n\nInterpretation:\n     Our Cross Validated ROC AUC is very close to 1. So, we can conclude that, the KNN classifier is indeed a very good model.\n\n\n\n11.6.12 Cross Validation\n     Cross-validation involves splitting the dataset into multiple subsets, training the model on some subsets, and validating it on the remaining subsets. This process is repeated several times, and the results are averaged to obtain a more reliable estimate of the model’s performance. There are three common cross validation approaches - Holdout cross valiation, K-fold cross validation, and Leave-one-out (LOO) cross validation.\n\n11.6.12.1 Holdout Cross Validation\n     Holdout CV is the simplest of all cross validation approaches. In holdout CV, we “hold out” a random proportion of our dataset as test data set and train the model on remaining dataset called training data set. Next, we pass the test data set through the model and calculate performance metrics. Figure 11.7 shows the process of holdout cross validation. A common rule of thumb for the training and testing split is 80-20.\n\n\n\n\n\n\nFigure 11.7: Holdout Cross Validation\n\n\n\n\n\n11.6.12.2 K-fold Cross Validation\n     In K-fold CV, the dataset is divided into \\(K\\) equally sized folds. The model is trained on K-1 folds and validated on the remaining fold. The process is repeated K times, with each fold used ad the validation set once. Figure 11.8 shows the process of K-fold cv. There is a variation of K-fold cross validation, which is called stratified K-fold CV, which is similar to K-fold CV, but ensures that each fold has a similar distribution of the target variable. The stratified K-fold CV is useful for imbalanced dataset.\n\n\n\n\n\n\nFigure 11.8: K-fold Cross Validation\n\n\n\n\n\n11.6.12.3 Leave-One-Out (LOO) Cross Validation\n     In LOO, each data point is used as a test and the model is trained on the remaining data points. The process is repeated until all data point is used as a test data. For a data set of 1000 observations, LOO cv involves training the model 1000 times, each time using a different data point as the validation set. Figure 11.9 shows the process of LOO cv.\n\n\n\n\n\n\nFigure 11.9: Leave-One-Out Cross Validation\n\n\n\n\n# Applying 10-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(knn_7, X_train, y_train, cv = 10, scoring='accuracy')\nprint('Cross-validation scores:{}'.format(scores))\n\nCross-validation scores:[0.875      0.96428571 0.94642857 0.98214286 0.96428571 0.96428571\n 0.98214286 0.98214286 1.         0.98181818]\n\n\n\n# compute Average cross-validation score\nprint('Average cross-validation score: {:.4f}'.format(scores.mean()))\n\nAverage cross-validation score: 0.9643\n\n\nInterpretation\n\nUsing the mean cross-validation, we can conclude that we expect the model to be around 96.46 % accurate on average.\nIf we look at all the 10 scores produced by the 10-fold cross-validation, we can also conclude that there is a relatively high variance in the accuracy between folds, ranging from 100% accuracy to 87.72% accuracy. So, we can conclude that the model is very dependent on the particular folds used for training, but it also be the consequence of the small size of the dataset.\nWe can see that 10-fold cross-validation accuracy does not result in performance improvement for this model.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#conclusion",
    "href": "machine_learning.html#conclusion",
    "title": "11  Machine Learning",
    "section": "11.7 Conclusion",
    "text": "11.7 Conclusion\n\n\n\n\n\n\n\nDeloitte. 2024. “Deloitte Invests $2 Billion to Accelerate IndustryAdvantage™ for Its Clients.” https://www2.deloitte.com/us/en/pages/about-deloitte/articles/press-releases/deloitte-invests-two-billion-to-accelerate-industryadvantage-for-its-clients.html.\n\n\nEstep, Cassandra, Emily E Griffith, and Nikki L MacKenzie. 2024. “How Do Financial Executives Respond to the Use of Artificial Intelligence in Financial Reporting and Auditing?” Review of Accounting Studies 29 (3): 2798–2831.\n\n\nEY. 2023. “EY Announces Launch of Artificial Intelligence Platform EY.ai Following US$1.4b Investment.” https://www.ey.com/en_gl/newsroom/2023/09/ey-announces-launch-of-artificial-intelligence-platform-ey-ai-following-us-1-4b-investment.\n\n\nFedyk, Anastassia, James Hodson, Natalya Khimich, and Tatiana Fedyk. 2022. “Is Artificial Intelligence Improving the Audit Process?” Review of Accounting Studies 27 (3): 938–85.\n\n\nLoten, Angus. 2023. “PricewaterhouseCoopers to Pour $1 Billion into Generative AI.” https://www.wsj.com/articles/pricewaterhousecoopers-to-pour-1-billion-into-generative-ai-cac2cedd.\n\n\nPersico, F, H Sidhu, et al. 2017. “How AI Will Turn Auditors into Analysts.” Accounting Today. https://www.accountingtoday.com/opinion/how-ai-will-turn-auditors-into-analysts.\n\n\nRaschka, Sebastian, Yuxi (Hayden) Liu, and Vahid Mirjalili. 2022. Machine Learning with PyTorch and Scikit-Learn: Develop Machine Learning and Deep Learning Models with Python. Packt Publishing. https://books.google.com/books/about/Machine_Learning_with_PyTorch_and_Scikit.html?id=SVxaEAAAQBAJ.\n\n\nReddi, Vijay. 2025. Machine Learning Systems. \"Creative Commons\". https://mlsysbook.ai/.\n\n\nRhys, Hefin. 2020. Machine Learning with r, the Tidyverse, and Mlr. Simon; Schuster.\n\n\nSchatsky, D, C Muraskin, and G Ragu. 2015. “Cognitive Technologies: The Real Opportunities for Business.” https://www2.deloitte.com/us/en/insights/deloitte-review/issue-16/cognitive-technologies-business-applications.html.\n\n\nVien, Courtney. 2018. “Using Drones to Enhance Audit.” Journal of Accountancy. https://www.journalofaccountancy.com/podcast/using-drones-to-enhance-audits.html.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "machine_learning.html#footnotes",
    "href": "machine_learning.html#footnotes",
    "title": "11  Machine Learning",
    "section": "",
    "text": "Though AI and ML are used interchangeably in the book, there are differences between them. Reddi (2025) differentiates them in the following way: Artificial Intelligence - the goal of creating machines that can match or exceed human intelligence—representing humanity’s quest to build systems that can think, reason, and adapt. Machine Learning - the scientific discipline of understanding how systems can learn and improve from experience—providing the theoretical foundation for building intelligent systems.↩︎\nSometimes, we are confused between parameters and hyperparameters. Parameters are learned from the training data, while hyperparameters are set (supplied) before training and are not learned from the data. For example, in linear regression, coefficients - betas (\\(\\beta\\)) are parameters because they are learned from the data whereas in ridge regression - lambda (\\(\\lambda\\)) is a hyperparameter, which controls the amount of regularization applied to the model and it is supplied from outside of the model.↩︎",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "fraud.html",
    "href": "fraud.html",
    "title": "12  Fraud Detection and Risk Management",
    "section": "",
    "text": "12.1 Techniques for Fraud Detection",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fraud Detection and Risk Management</span>"
    ]
  },
  {
    "objectID": "fraud.html#risk-assessment-models",
    "href": "fraud.html#risk-assessment-models",
    "title": "12  Fraud Detection and Risk Management",
    "section": "12.2 Risk Assessment Models",
    "text": "12.2 Risk Assessment Models",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fraud Detection and Risk Management</span>"
    ]
  },
  {
    "objectID": "fraud.html#case-studies-in-fraud-detection",
    "href": "fraud.html#case-studies-in-fraud-detection",
    "title": "12  Fraud Detection and Risk Management",
    "section": "12.3 Case Studies in Fraud Detection",
    "text": "12.3 Case Studies in Fraud Detection",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fraud Detection and Risk Management</span>"
    ]
  },
  {
    "objectID": "performance_measurement.html",
    "href": "performance_measurement.html",
    "title": "13  Performance Measurement and Management",
    "section": "",
    "text": "13.1 Key Performance Indicators (KPIs)",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Measurement and Management</span>"
    ]
  },
  {
    "objectID": "performance_measurement.html#balanced-scorecard",
    "href": "performance_measurement.html#balanced-scorecard",
    "title": "13  Performance Measurement and Management",
    "section": "13.2 Balanced Scorecard",
    "text": "13.2 Balanced Scorecard",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Measurement and Management</span>"
    ]
  },
  {
    "objectID": "performance_measurement.html#financial-ratio-analysis",
    "href": "performance_measurement.html#financial-ratio-analysis",
    "title": "13  Performance Measurement and Management",
    "section": "13.3 Financial Ratio Analysis",
    "text": "13.3 Financial Ratio Analysis",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Performance Measurement and Management</span>"
    ]
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "14  Regulatory and Ethical Considerations",
    "section": "",
    "text": "14.1 Compliance and Regulatory Requirements",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regulatory and Ethical Considerations</span>"
    ]
  },
  {
    "objectID": "ethics.html#ethichal-use-of-data",
    "href": "ethics.html#ethichal-use-of-data",
    "title": "14  Regulatory and Ethical Considerations",
    "section": "14.2 Ethichal Use of Data",
    "text": "14.2 Ethichal Use of Data",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regulatory and Ethical Considerations</span>"
    ]
  },
  {
    "objectID": "ethics.html#data-privacy-security",
    "href": "ethics.html#data-privacy-security",
    "title": "14  Regulatory and Ethical Considerations",
    "section": "14.3 Data Privacy & Security",
    "text": "14.3 Data Privacy & Security",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Regulatory and Ethical Considerations</span>"
    ]
  },
  {
    "objectID": "future.html",
    "href": "future.html",
    "title": "15  Future Directions in Accounting Analytics",
    "section": "",
    "text": "15.1 Emerging Technologies",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Future Directions in Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "future.html#trends-predictions",
    "href": "future.html#trends-predictions",
    "title": "15  Future Directions in Accounting Analytics",
    "section": "15.2 Trends & Predictions",
    "text": "15.2 Trends & Predictions",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Future Directions in Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "future.html#preparing-for-the-future-accounting-analytics",
    "href": "future.html#preparing-for-the-future-accounting-analytics",
    "title": "15  Future Directions in Accounting Analytics",
    "section": "15.3 Preparing for the Future Accounting Analytics",
    "text": "15.3 Preparing for the Future Accounting Analytics",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Future Directions in Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "python_basics.html",
    "href": "python_basics.html",
    "title": "Appendix A — A Primer on Python",
    "section": "",
    "text": "Learning Objectives of the Appendix\nAt the End of the Appendix, Students should be Able to -",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#learning-objectives-of-the-appendix",
    "href": "python_basics.html#learning-objectives-of-the-appendix",
    "title": "Appendix A — A Primer on Python",
    "section": "",
    "text": "Gain an Understanding about Python\nGain an Understanding about the Data Types and Data Structures in Python\nGain an Understanding about Arrays in Numpy, Indexing and Slicing of Arrays, and Operations of Arrays\nGain an Understanding about for Loop function, map function, and User Defined Function in Python",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#what-is-python",
    "href": "python_basics.html#what-is-python",
    "title": "Appendix A — A Primer on Python",
    "section": "A.1 What is Python?",
    "text": "A.1 What is Python?\n\n     According to www.python.org “Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development, as well as for use as a scripting or glue language to connect existing components together.” It further explains - “Python’s simple, easy to learn syntax emphasizes readability and therefore reduces the cost of program maintenance. Python supports modules and packages, which encourages program modularity and code reuse.”",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#data-types-in-python",
    "href": "python_basics.html#data-types-in-python",
    "title": "Appendix A — A Primer on Python",
    "section": "A.2 Data Types in Python",
    "text": "A.2 Data Types in Python\n\n     Data has different types. When dealing with data, we need to know the types of the data because different data types can do different things. There are six basic data types in python. They include - int, float, complex, bool, str, and bytes. We use type () function to know the types of the data. However, most commmonly used data types are int, float, str, and bool.\n\n\nx = \"hello world\"\ntype(x)\n\nstr\n\n\n\nx = 25\ntype(x)\n\nint\n\n\n\nx = 25.34\ntype(x)\n\nfloat\n\n\n\nx = True\ntype(x)\n\nbool\n\n\n\nx =7j\ntype(x)\n\ncomplex\n\n\n\nx = b\"Hello World\"\ntype(x)\n\nbytes",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#data-structures-in-python",
    "href": "python_basics.html#data-structures-in-python",
    "title": "Appendix A — A Primer on Python",
    "section": "A.3 Data Structures in Python",
    "text": "A.3 Data Structures in Python\n\n     Data structures are the collection of data on which different processes can be done efficiently. It enables quick and easier access, and efficient modifications. Data Structures allows to organize data in such a way that enables to store collections of data, relate them and perform operations on them. Data structures in python can broadly be classified into two groups - Built-in data structures and User-defined data structures. Figure A.1 Shows the data structure in python. Built-in data structure is important because they are widely used. Therefore, we will elaborate on built-in data structure.\n\n\n\n\n\n\n\nFigure A.1: Data Structure in Python",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#built-in-data-structure",
    "href": "python_basics.html#built-in-data-structure",
    "title": "Appendix A — A Primer on Python",
    "section": "A.4 Built-in Data Structure",
    "text": "A.4 Built-in Data Structure\n\nA.4.1 List\n\n     List is used to store collection of ordered1data items. Lists are created using square brackets ([]). We can also create a list using list () function. Lists can hold different types of data, including integers (int), floats (float), strings (str), and even other lists. We can use len () function to know the number to elements in the list. Moreover, lists are mutable, meaning that their contents can be changed after the list has been created.\n\n\ncolors = ['red', 'blue', 'green']\nprint(colors)\n\n['red', 'blue', 'green']\n\n\n\nlen(colors)\n\n3\n\n\n\na = [1, 'apple', 3.14, [5, 6]]\nprint(a)\n\n[1, 'apple', 3.14, [5, 6]]\n\n\n\nb = list((1, 'apple', 3.14, [5, 6]))\nprint(b)\n\n[1, 'apple', 3.14, [5, 6]]\n\n\n\nA.4.1.1 Creating a List with Repeated Elements\nA list with repeated elements can be created using the multiplication operator.\n\nx = [2] * 5\ny = [0] * 7\n\nprint(x)\nprint(y)\n\n[2, 2, 2, 2, 2]\n[0, 0, 0, 0, 0, 0, 0]\n\n\n\n\nA.4.1.2 Accessing List Elements\n\n     Indexing can be used to access the elements in the list. Python indexes start at 0. Therefore, a[0] will access the first element in the list a. Figure A.2 shows the index of the list - colors.\n\n\n\n\n\n\n\nFigure A.2: Index of List Elements\n\n\n\n\ncolors[0]\n\n'red'\n\n\n\ncolors[-1]\n\n'green'\n\n\n\n\nA.4.1.3 Adding Elements to the List\nWe can add elements to the list using three methods - append (), insert (), and extend ().\n\n# Initialize an empty list\nm = []\n\n# Adding 10 to end of list\nm.append(50)  \nprint(\"After append(150):\", m)  \n\n# Inserting 40 at index 0\nm.insert(0, 40)\nprint(\"After insert(0, 40):\", m) \n\n# Adding multiple elements  [60,70,80] at the end\nm.extend([60, 70, 80])  \nprint(\"After extend([60,70,80]):\", m) \n\nAfter append(150): [50]\nAfter insert(0, 40): [40, 50]\nAfter extend([60,70,80]): [40, 50, 60, 70, 80]\n\n\n\n\nA.4.1.4 Updating Elements to the List\nWe can change the value of an element by accessing it using its index.\n\np = [10, 20, 30, 40, 50]\n# Change the second element\np[1] = 25 \n\nprint(p)  \n\n[10, 25, 30, 40, 50]\n\n\n\n\nA.4.1.5 Removing Elements from the List\nWe can remove elements from the list using three methods - remove (), pop (), and del ().\n\na = [10, 20, 30, 40, 50]\n\n# Removes the first occurrence of 30\na.remove(30)  \nprint(\"After remove(30):\", a)\n\n# Removes the element at index 1 (20)\npopped_val = a.pop(1)  \nprint(\"Popped element:\", popped_val)\nprint(\"After pop(1):\", a) \n\n# Deletes the first element (10)\ndel a[0]  \nprint(\"After del a[0]:\", a)\n\nAfter remove(30): [10, 20, 40, 50]\nPopped element: 20\nAfter pop(1): [10, 40, 50]\nAfter del a[0]: [40, 50]\n\n\n\n\n\nA.4.2 Dictionary\n\n     Dictionary data structure in python is used to store data in key:value format. Unlike list - which uses square brackets ([]) - dictionary uses curly brackets ({}). Like lists, dictionary is mutable. Dictionary items can be referred by using key name. We can use len () function to know the total number of element of a dictionary and type () to know the type.\n\n\nmy_car = {\n  \"brand\": \"Ford\",\n  \"model\": \"Escape\",\n  \"year\": 2017\n}\nprint(my_car)\n\n{'brand': 'Ford', 'model': 'Escape', 'year': 2017}\n\n\n\nprint(my_car['model'])\n\nEscape\n\n\nThe values in dictionary items can be of any data type\n\ncar_features = {\n  \"brand\": \"Ford\", # string \n  \"electric\": False, # boolean \n  \"year\": 1964, # integer\n  \"colors\": [\"red\", \"white\", \"blue\"] # list of string\n}\n\nThe function dict () can also be used to construct dictionary.\n\nmy_friends = dict(\n    name = [\"John\", \"Smith\", \"Mark\"], \n    age = [36, 45, 49], \n    country = [\"Norway\", \"Sweden\", \"Finland\"]\n)\nprint(my_friends)\n\n{'name': ['John', 'Smith', 'Mark'], 'age': [36, 45, 49], 'country': ['Norway', 'Sweden', 'Finland']}\n\n\nSome built-in dictionary methods2 are -\n\ndict.clear() - removes all the elements from the dictionary\n\n\nemployee = {\n    'name': [\"John\", \"Jessica\", \"Zack\"], \n    'age': [18, 19, 20]\n}\nprint(employee)\n\n{'name': ['John', 'Jessica', 'Zack'], 'age': [18, 19, 20]}\n\n\n\nemployee.clear()\nprint(employee)\n\n{}\n\n\n\ndict.copy() - returns a copy of the dictionary\ndict.get(key, default = “None”) - returns the value of specified key\ndict.items() - returns the value of specified key\ndict.keys() - returns a list containing dictionary’s key\ndict.values() - returns a list of all the values of the dictionary.\n\n\n\nA.4.3 Tuple\n\n     In python, tuple is very similar to list, except one difference. List is mutable, but tuple is not. Once a tuple is created, its elements cannot be changed. Unlike lists, we cannot add, remove, or change elelment in tuple. Tuple is created by using parenthese (()). Also, the function tuple () can also be used to create tuple. We can access the elements of tuple by indexing as we did for lists.\n\n\nmy_tuple = ('10', '20', '30', 'hello', 'world')\nmy_tuple\n\n('10', '20', '30', 'hello', 'world')\n\n\n\nmy_tuple[3]\n\n'hello'\n\n\n\nThere are different operations that can be performed on the tuple. Some of them include -\n\nConcatenation - To concatenate, plus operator (+) is used.\nNesting - Nested tuple means a tuple is inside the another tuple\nRepetition - creating a tuple of several times\n\n\n\nsecond_tuple = ('10', '20', 'SIU', \"SOA\", \"Carbondale\")\nsecond_tuple*3\n\n('10',\n '20',\n 'SIU',\n 'SOA',\n 'Carbondale',\n '10',\n '20',\n 'SIU',\n 'SOA',\n 'Carbondale',\n '10',\n '20',\n 'SIU',\n 'SOA',\n 'Carbondale')\n\n\n\nSlicing - Dividing a given tuple into small tuples using indexing is slicing.\n\n\nsecond_tuple[1:]\nsecond_tuple[2:4]\nsecond_tuple[::-1]\n\n('Carbondale', 'SOA', 'SIU', '20', '10')\n\n\n\n\nFinding the Length - using len () function, we can figure out the total number of elements in the tuple.\nDifferent data types in tuples - Tuple can include heterogenous data.\nLists to tuples - Using tuple () functions, we can convert a list into tuple.\n\n\n\n\nA.4.4 Set\n\n     A set in python is a collection of unordered, unchangeable, and unindexed items. Set items are unchangeable, but new items can be added to the set and old items can be deleted from the set. Another important characteristics of set is that it has no duplicate elements. Curly bracket ({}) is used to create a set. The function difference () or minus operator (-) is used to calculate difference between two sets.\n\n\nnew_set = {'Hello', 'World', \"World\"}\nnew_set \n\n{'Hello', 'World'}\n\n\n\ntype(new_set)\n\nset\n\n\n\nnew_set[0] = \"Hi\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#what-is-numpy",
    "href": "python_basics.html#what-is-numpy",
    "title": "Appendix A — A Primer on Python",
    "section": "A.5 What is Numpy?",
    "text": "A.5 What is Numpy?\n\nA.5.1 Installing and Importing Numpy\n\n     Numpy is a library in python and it is one of the most important and essential libraries for data science becasue almost all of the libraries in python PyData ecosystem use numpy. Therefore, understanding numpy is important. Moreover, numpy arrays are very fast as they are implemented in C. In this section, we will learn some useful numpy methods.\n     Before we start using the numpy module (library), we need to install it. We can run the following code to install numpy.\n\npip install numpy \n# OR\nconda install numpy \n\n     Once the numpy is installed, we need to load (import) the library by running the following code -\n\nimport numpy as np\n\n\nA.5.2 Some Useful numpy Functions for Array\n\nA.5.2.1 Array Creation\n     Array is a multi dimensional data structure, which describes a collection of “items” of the same type (homogenous). Arrays are powerful for performing different mathematical and scientific computation. There are many functions in numpy to create arrays. Below some those methods (functions) are described.\n     * np.array() is used to create an array in numpy. The array object is also called ndarray. You can pass a list or tuple to the np.array () function. You can create zero, one, two, or three dimensional arrays in numpy. Using the ndim() you can check the dimension of an array.\n\nlist = [2025, 2024, 2023, 2022, 2021, 2020, 2019]\nnp.array(list)\narray = np.array(list)\narray.ndim # 1 dimensional array\n\nnp.array((20)).ndim # zero dimensional array. \n\narray2 = np.array([[1,3,5,7], [2,4,6,8]])\narray2\narray2.ndim # 2 dimensional array\n\n2\n\n\n     * np.arange(start, stop, step)3 function is used to create array with values starting from start up to, but not including, stop value, increasing by step.\n\nnp.arange(10,21,1)\nnp.arange(21)\nnp.arange(10,21)\nnp.arange (-1,1)\nnp.arange(-1,1,0.001)\nnp.arange(10,30,3)\n\narray([10, 13, 16, 19, 22, 25, 28])\n\n\n     * np.linspace(start, stop, n) creates an array of n evenly spaced number between start and stop.\n\nnp.linspace(10,21,10)\nnp.linspace(1,100, 10)\n\narray([  1.,  12.,  23.,  34.,  45.,  56.,  67.,  78.,  89., 100.])\n\n\n     * np.zeros() is used to create an array with elements 0.\n\nnp.zeros(5)\nnp.zeros((5,5))\nnp.zeros([5,5])\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0.]])\n\n\n     * np.ones() is used to create an array with elements 1.\n\nnp.ones(5)\nnp.ones((5,5))\nnp.ones([5,5])\n\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\n\n\n     * np.eye() is used to create an identity matrix. The same can be done by using np.identity() function.\n\nnp.eye(5)\n\narray([[1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 1.]])\n\n\n     * np.random.rand() function generates an array of random numbers between 0 and 1 from a uniform distribution.\n\nnp.random.rand(10) # one dimensional \nnp.random.rand(3,2) # two dimensional \n\narray([[0.02645752, 0.94033618],\n       [0.83656222, 0.59655895],\n       [0.32579328, 0.80937829]])\n\n\n     * np.random.randn() generates an array of random numbers between 0 and 1 from a standard normal distribution.\n\nnp.random.randn(10) # one dimensional \nnp.random.randn(3,4) # two dimensional \n\narray([[ 0.30210099,  0.51770939,  1.82834208,  0.68299169],\n       [-0.20429609,  0.48092033,  1.25141883,  1.63325492],\n       [ 0.3242677 ,  0.78609986, -0.59844598, -0.78931083]])\n\n\n     * np.random.randint() allows to generate random integer numbers given an interval of integers.\n\nnp.random.randint(low=0, high=10, size = 5)\n\narray([7, 5, 7, 3, 5], dtype=int32)\n\n\n     * np.reshape() allows to chgange the shape (rows and columns) without changing the data in the array.\n\narray = np.random.randn(3,4) # two dimensional \narray\narray.reshape(6,2)\n\narray([[ 0.84150961, -0.99492664],\n       [-2.06698101, -1.22421433],\n       [-1.60569132, -1.75631627],\n       [ 0.35674724, -0.47488292],\n       [ 0.17706789, -1.85904999],\n       [ 2.01366597, -0.65283419]])\n\n\n     Some other useful functions from numpy include - np.shape, np.dtype, np.transpose. Some useful functions related to linear algebra include - np.linalg.inv() - to compute inverse of a matrix, np.linalg.det() - to compute determinant of a matrix, np.linalg.eig() - to compute eigenvalues and eigenvectors of a matrix, np.linalg.solve() - to solve a system of linear equations.\n\n\nA.5.2.2 Array Indexing and Selection\n     Both indexing and slicing of arrays are important skill to learn. Indexing refers to obtaining individual elements from an array while slicing refers to obtaining a sequence of elements from the array. We use array[start:end] to index an array.\n\n# One Dimensional Array\narray = np.random.randn(10)\narray\narray[2]\narray[1:3]\narray[:5]\narray[5:]\narray[-3:]\narray[:-3]\narray[array&gt;0.50] # conditional indexing \n\narray([1.44293469, 1.16446499])\n\n\n\n# Two Dimensional Array \narray2D = np.random.randn(8,5)\narray2D\narray2D[1]\narray2D[1][2] # double brackets \narray2D[1,2] # single bracket (preferred method)\narray2D[2:,]\narray2D[2:,3:]\narray2D[2:3]\narray2D[2:4] # Only rows \narray2D[:,3:]\n\narray([[-2.2122436 ,  0.06178527],\n       [ 1.54939543, -0.25975965],\n       [-0.74136396, -1.60482342],\n       [-1.33514363, -0.7696434 ],\n       [-2.34376853,  0.1489806 ],\n       [ 0.62060885,  0.08980056],\n       [ 0.63882864,  1.51053534],\n       [-0.24009317, -1.36122267]])\n\n\n\n\nA.5.2.3 Array Operations\n     Array operations involve performing matematical operations on the array as a whole. They are not perfomed on the individual element of the array.\n\narray1 = np.arange(85,96)\narray2 = np.arange (35,46)\narray1 + array2\narray1 - array2\narray1*array2\narray1 / array2\n\narray([2.42857143, 2.38888889, 2.35135135, 2.31578947, 2.28205128,\n       2.25      , 2.2195122 , 2.19047619, 2.1627907 , 2.13636364,\n       2.11111111])\n\n\n\narray1.mean()\narray2.std()\nnp.mean(array1)\nnp.min(array2)\nnp.max(array2)\nnp.sqrt(array1)\nnp.sum(array1)\nnp.log(np.sum(array1))\n\nnp.float64(6.897704943128636)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#functions-in-python",
    "href": "python_basics.html#functions-in-python",
    "title": "Appendix A — A Primer on Python",
    "section": "A.6 Functions in Python",
    "text": "A.6 Functions in Python\n\n     In python, there are some functions that we use very frequently. In this section, we will discuss some of those functions.\n\nA.6.1 for Loop Function\n     for loop function in python allows to iterate over iterable sequences such as list, tuple, string, or range and execuate codes for each elements in the sequence. for loop function helps to handle repititve tasks more efficiently and effectively. The syntax of a for loop function is -\n\nfor element in sequence:\n  # Expected code to execute on each element of the sequence\n\n     A basic example of a for loop function is -\n\nanalytics_students = ['Ashley', 'Elijah', 'John', 'Jack', 'Adams']\nfor student in analytics_students:\n  print(student)\n\nAshley\nElijah\nJohn\nJack\nAdams\n\n\n     Other examples of a for loop function is -\n\neven_numbers = [2,4,6,8,10]\nfor numbers in even_numbers:\n  square = numbers**2\n  print (square)\n\n4\n16\n36\n64\n100\n\n\n\neven_numbers = [2,4,6,8,10]\nsquares = []\nfor numbers in even_numbers:\n  square = numbers**2\n  squares.append(numbers**2)\n\nprint (squares)\n\n[4, 16, 36, 64, 100]\n\n\n\n\nA.6.2 map() Function\n     map () function, like for loop function, allows to apply a function on each item in an iterable (list, tuple, or string) sequence. The syntax for map () is - map (function, iterable). Below is an example of map () function -\n\nvar = [16, 17, 18, 19, 20]\nvar_log = map(lambda x: np.log(x), var)\n\nfor x in var_log:\n  print(x)\n\n2.772588722239781\n2.833213344056216\n2.8903717578961645\n2.9444389791664403\n2.995732273553991\n\n\n     map () function is useful for simple calculations; however, for complex transformations, using for loop is efficient and effective.\n\n\nA.6.3 User Defined Function (Named Function)\n     In addition to predifend functions from different python modules, users can define their own functions, which are sometimes called named functions. The syntax for defining a user defined function in python -\n\ndef deduct_num (a, b):\n  \"\"\"\n  The function will deduct two numbers\n  \"\"\"\n  result = a - b\n  return result\n\n     In the above example, a user defined function is created. Then function name is deduct_num and it is created using def keyword. So, when we need to create a user defined function, we will start with def keyword followed by the name of the function. The a and b are the function’s arguments, which sometimes are also called parameters.\n     The tripple quote \"\"\" \"\"\" is used to create a docstring, which also explains the nature of the function or what it will do. The return statement in function will return a value.\n\ndeduct_num(15, 100)\n\n-85\n\n\n     Another example of user defined function -\n\ndef welcome (name):\n  \"\"\"\n  The function greets the person\n  \"\"\"\n  print(f\"Welcome {name}! How are you doing?\")\n\n\nwelcome(\"John\")\n\nWelcome John! How are you doing?\n\n\n\n\nA.6.4 Anonymous (Lambda) Function\n     Anonymous function is a function without a name. It is also called lambda function in python. The syntax for lambda function is - lambda arguments: expression. Below is an example of lambda function -\n\nsqr = lambda x: x**2\nsqr(5)\n\n25\n\n\n     lambda function can take many arguments (parameters), but accepts only one expression.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#conclusions",
    "href": "python_basics.html#conclusions",
    "title": "Appendix A — A Primer on Python",
    "section": "A.7 Conclusions",
    "text": "A.7 Conclusions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#exercises",
    "href": "python_basics.html#exercises",
    "title": "Appendix A — A Primer on Python",
    "section": "Exercises",
    "text": "Exercises\n\n\nCreate an array of integers from 10 to 50.\nCreate an array of all even integers from 10 to 50.\nCreate an array of 10 threes (use either np.full() or np.ones() or np.repeat()).\nCreate a 3 by 3 matrix with values ranging from 10 to 18.\nCreate an array of 5 by 5 identify matrix.\nUse numpy to generate a random number between 0 and 1.\nUse numpy to generate an array of 25 random numbers sampled from a standard normal distribution.\nCreate a matrix like below -\n\narray([[0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ],\n       [0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 ],\n       [0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 ],\n       [0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 ],\n       [0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 ],\n       [0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ],\n       [0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 ],\n       [0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 ],\n       [0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 ],\n       [0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  ]])\n\nCreate an array of 50 linearly spaced points between 0 and 1.\nCreate an array of 20 linearly spaced points between 0 and 1.\n\n\nmat = np.arange(1,26).reshape(5,5)\nmat\n\narray([[ 1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10],\n       [11, 12, 13, 14, 15],\n       [16, 17, 18, 19, 20],\n       [21, 22, 23, 24, 25]])\n\n\n\nProduce the following matrix from the mat matrix above.\n\narray([[12, 13, 14, 15],\n       [17, 18, 19, 20],\n       [22, 23, 24, 25]])\n\nProduce the following (value) 20 from mat matrix - np.int64(20)\nProduce the following matrix from mat.\n\narray([[ 2],\n       [ 7],\n       [12]])\n\nProduce the following matrix from mat - array([21, 22, 23, 24, 25])\nProduce the following matrix from mat\n\narray([[16, 17, 18, 19, 20],\n       [21, 22, 23, 24, 25]])\n\nGet the sum of all values in the mat matrix\nGet the standard deviation of the values in mat matrix\nGet the sum of all columns in the mat matrix\nGet the sum of all rows in the mat matrix\nGet the determinant and eigenvalues and eigenvectors of the matrix mat.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "python_basics.html#footnotes",
    "href": "python_basics.html#footnotes",
    "title": "Appendix A — A Primer on Python",
    "section": "",
    "text": "When we say that lists are ordered, it means that the items have a defined order, and that order will not change. If you add new items to a list, the new items will be placed at the end of the list.↩︎\nIn python, functions are called methods.↩︎\nrange(start, stop, step) function creates a sequence of numbers starting from start, and stopping at stop. Usually, the step in range () function is 1.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A Primer on Python</span>"
    ]
  },
  {
    "objectID": "r_basics.html",
    "href": "r_basics.html",
    "title": "Appendix B — A Primer on R",
    "section": "",
    "text": "Learning Objectives of the Appendix\nAt the End of the Appendix, Students should be Able to -",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#learning-objectives-of-the-appendix",
    "href": "r_basics.html#learning-objectives-of-the-appendix",
    "title": "Appendix B — A Primer on R",
    "section": "",
    "text": "Gain an Understanding about R\nGain an Understanding about the Data Types and Data Structures in R\nGain an Understanding about Indexing and Slicing of vectors and Data Frames, and Mathematical Operations on them\nGain an Understanding about for Loop function, map function, and User Defined Function, and Anonymous Function in R",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#what-is-r",
    "href": "r_basics.html#what-is-r",
    "title": "Appendix B — A Primer on R",
    "section": "B.1 What is R?",
    "text": "B.1 What is R?\n\n    R is an object-oriented programming language. R was first developed by two statisticans - Ross Ihaka and Robert Gentleman1 - in early 1990s2. Though it was first designed for statistial purposes, nowadays, it is widely used in statistical analysis, visualization, finance, healthcare industry, and many more. Many companies in the world use R. For example, The Newyork Times and Airbnb use R for their day to day operations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#packages-in-r",
    "href": "r_basics.html#packages-in-r",
    "title": "Appendix B — A Primer on R",
    "section": "B.2 Packages in R",
    "text": "B.2 Packages in R\n\n     Packages are collections of functions, data and so on. Packages enhance the functionalities of R. There are tons of packages for different purposes in R. The Comprehensive R Archive Network (CRAN) - https://cran.r-project.org/- is the primary repository of R packages. Some of the widely used R packages include - dplyr, ggplot, lubridate, and shiny. The function - install.package('packagename')- is used to install a package in R. Once the package is installed, one needs to load (import) package by running the code - library (packagename)3. Please note that you need to install the package only once, but you need to import (load) your package each time you start a new R session.\n\n\n# installing tidyverse package \ninstall.packages('tidyverse')\n\n\n# Loading tidyverse package \nlibrary(tidyverse)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#data-types",
    "href": "r_basics.html#data-types",
    "title": "Appendix B — A Primer on R",
    "section": "B.3 Data Types",
    "text": "B.3 Data Types\n\n      Data4 has different types. When dealing with data, we need to know the types of the data because different data types can do different things. There are five basic data types in R. They include - double, integer, character, logical, and complex. We use typeof () or class () functions to know the types of the vector and length() to know the size of the vector. However, most commmonly used data types are double, integer, character, and logical.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#data-structure",
    "href": "r_basics.html#data-structure",
    "title": "Appendix B — A Primer on R",
    "section": "B.4 Data Structure",
    "text": "B.4 Data Structure\n\n      There are several types of data structures in R. These structures include - vector, matrix, array, list, data frame, tibble, and facotrs. Of these data structures - vector, list, data frame and tibble are the most common.\n\nB.4.1 Vector\n      Vector is one of the basic data structures in R. In most cases, vectors are created by using c() function, where “c” means concatenate. Sometimes vectors are classified into two groups - atomic vectors and lists. Figure B.1 shows the classfication of the vectors. Atomic vectors include - double, integer, character, and logical vectors5. The double and integer vectors are collectively known as numeric vectors. Lists include dataframe or tibble. Lists are called recursive vectors because they can include other lists. NULL6 is often used to represent the absence of a vector. NULL indicates that the vector length is 0.\n\n\n\n\n\n\nFigure B.1: Types of Vector\n\n\n\n\n# An example of a vector\nstudents = c (\"John\", \"Adam\", \"Alex\", \"Emily\")\nprint(students)\n\n[1] \"John\"  \"Adam\"  \"Alex\"  \"Emily\"\n\ntypeof(students)\n\n[1] \"character\"\n\nclass(students)\n\n[1] \"character\"\n\nlength(students)\n\n[1] 4\n\n\n\n# Numeric vector \nnumeric_vector &lt;- c(1.1, 2.2, 3.3) \n# Integer vector \ninteger_vector &lt;- c(1L, 2L, 3L) \n# Character vector \ncharacter_vector &lt;- c(\"a\", \"b\", \"c\") \n# Logical vector \nlogical_vector &lt;- c(TRUE, FALSE, TRUE)\n\n\n# Some vector operations \nnumeric_vector + 5\n\n[1] 6.1 7.2 8.3\n\nnumeric_vector - 5\n\n[1] -3.9 -2.8 -1.7\n\nmax(numeric_vector)\n\n[1] 3.3\n\nmin(integer_vector)\n\n[1] 1\n\nsum(integer_vector)\n\n[1] 6\n\nsd(numeric_vector)\n\n[1] 1.1\n\n\n\n# Vectors as sequence of Numbers \nnum_seq = 1:10\nnum_seq\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nseq(15)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nseq(1,40, by = 2)\n\n [1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39\n\n\n\n\nB.4.2 Matrix\n      Matrix is a special type of vector with dimensions, meaning that matrix has rows and columns. Matrix is constructed columnwise.\n\nmatrix(seq(1:12), nrow = 4, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\nm = 1:12\ndim (m) = c(4,3) # dimension added to the matrix \nm\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\n\n\nB.4.3 Arrays\n      Arrays are very similar to matrix, but they have more than two dimensions.\n\narray(1:27, dim = c(3,3,3))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   10   13   16\n[2,]   11   14   17\n[3,]   12   15   18\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   19   22   25\n[2,]   20   23   26\n[3,]   21   24   27\n\n\n\n\nB.4.4 Data Frames (tibble)\n      Like matrix, data frames are two dimensional, having rows and columns. However, unlike matrix, they can contain different types of data. Data frames are widely used in data analytics. Tibble is an updated or modern type of data frame. It is recommended to use tibble.\n\ndata.frame(\n    id = 1:3,\n    name = c ('A', \"B\", \"C\"),\n    age = c (18, 21, 23)\n)\n\n  id name age\n1  1    A  18\n2  2    B  21\n3  3    C  23\n\n\n\ntibble(\n   id = 1:3,\n    name = c ('A', \"B\", \"C\"),\n    age = c (18, 21, 23) \n)\n\n# A tibble: 3 × 3\n     id name    age\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 A        18\n2     2 B        21\n3     3 C        23\n\n\n\n\nB.4.5 Lists\n      Lists are very versatile in that it can contain different types of data. Unlike atomic vectors, which contain same type of data, lists can contain many types of data. Lists can also contain another list; therefore, it is sometimes called recursive vector. List is created by using function list().\n\nlist_example = list (\n    name = \"Jessica\",\n    age = c(22),\n    cgpa = c(3.75),\n    student  = TRUE,\n    address = list(\n        street = c(\"5 W Main St\"),\n        city = c(\"Carbondale\"),\n        zip = 62901\n    ),\n    hobbies = c (\"Fishing\", \"Hiking\", \"Cooking\")\n)\nlist_example\n\n$name\n[1] \"Jessica\"\n\n$age\n[1] 22\n\n$cgpa\n[1] 3.75\n\n$student\n[1] TRUE\n\n$address\n$address$street\n[1] \"5 W Main St\"\n\n$address$city\n[1] \"Carbondale\"\n\n$address$zip\n[1] 62901\n\n\n$hobbies\n[1] \"Fishing\" \"Hiking\"  \"Cooking\"\n\nprint(list_example$name)\n\n[1] \"Jessica\"\n\nprint(list_example$address$city) # nested list element\n\n[1] \"Carbondale\"\n\nlist_example[1]\n\n$name\n[1] \"Jessica\"\n\nlist_example[[1]]\n\n[1] \"Jessica\"\n\nlist_example[c(1,2)]\n\n$name\n[1] \"Jessica\"\n\n$age\n[1] 22\n\n\n\n\nB.5 Indexing and Slicing Vectors\n      Using [], one can access elements in a vector. Unlike python7, R uses 1 based indexing, meaning that first element has an index 1. For slicing, one can specify the range of index using :.\n\nsample_vector = c (10:25)\nsample_vector[5]\n\n[1] 14\n\nsample_vector[5:6]\n\n[1] 14 15\n\nsample_vector[c(5,8)]\n\n[1] 14 17\n\n\n      Negative indexing, unlike python, excludes the element from the vector.\n\nsample_vector[-5]\n\n [1] 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25\n\nsample_vector[-c(5,8)]\n\n [1] 10 11 12 13 15 16 18 19 20 21 22 23 24 25\n\n\n      Conditional or logical indexing can be done on the vector as well.\n\nsample_vector &gt; 13\n\n [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE\n\nsample_vector[sample_vector&gt;13]\n\n [1] 14 15 16 17 18 19 20 21 22 23 24 25\n\n\n      Additionally, different kinds of mathematical operatins can be applied on the vector.\n\nmin(sample_vector)\n\n[1] 10\n\nmax(sample_vector)\n\n[1] 25\n\nsum(sample_vector)\n\n[1] 280\n\nsd(sample_vector)\n\n[1] 4.760952\n\nrange(sample_vector)\n\n[1] 10 25\n\nmean(sample_vector)\n\n[1] 17.5\n\n\n      For data frame or tibble, one can use [row,column] for indexing or slicing.\n\ndf = data.frame(\n    id = 1:3,\n    name = c ('A', \"B\", \"C\"),\n    age = c (18, 21, 23)\n)\ndf['age']\n\n  age\n1  18\n2  21\n3  23\n\ndf$age\n\n[1] 18 21 23\n\ndf$age[1]\n\n[1] 18\n\ndf['age'][[1]][1]\n\n[1] 18\n\ndf[,]\n\n  id name age\n1  1    A  18\n2  2    B  21\n3  3    C  23\n\ndf[1,'age']\n\n[1] 18\n\ndf[1:2,'age']\n\n[1] 18 21\n\ndf[1:2,c('age','id')]\n\n  age id\n1  18  1\n2  21  2\n\n\n\ndf2 = tibble(\n    id = 1:3,\n    name = c ('A', \"B\", \"C\"),\n    age = c (18, 21, 23)\n)\ndf2['age']\n\n# A tibble: 3 × 1\n    age\n  &lt;dbl&gt;\n1    18\n2    21\n3    23\n\ndf2$age\n\n[1] 18 21 23\n\ndf2$age[1]\n\n[1] 18\n\ndf2['age'][[1]][1]\n\n[1] 18\n\ndf2[,]\n\n# A tibble: 3 × 3\n     id name    age\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 A        18\n2     2 B        21\n3     3 C        23\n\ndf2[1,'age']\n\n# A tibble: 1 × 1\n    age\n  &lt;dbl&gt;\n1    18\n\ndf2[1:2,'age']\n\n# A tibble: 2 × 1\n    age\n  &lt;dbl&gt;\n1    18\n2    21\n\ndf2[1:2,c('age','id')]\n\n# A tibble: 2 × 2\n    age    id\n  &lt;dbl&gt; &lt;int&gt;\n1    18     1\n2    21     2\n\n\n\n\nB.6 Random Numbers\n      In R, two widely used functions to generate random numbers are - runif() and rnorm(). runif() is used to generate random numbers from uniform distribution. The range of numbers generated using runif is 0 to 1.\n\n# vector of 2 elements \nrunif(2)\n\n[1] 0.3094118 0.9153653\n\n# vector of 5 elements from 0 to 100\nrunif(5, min = 0, max = 100)\n\n[1]  9.831609 34.816336 30.055107 62.850976 14.218783\n\n\n      rnorm() function generates random numbers from normal distribution with mean 0 and standard deviation 1.\n\nrnorm(5)\n\n[1]  1.4839078  0.8830858  0.1174016 -2.2038416  1.0810854\n\nrnorm(5, mean = 5, sd= 2)\n\n[1] 6.843230 3.361812 8.062965 3.504972 4.308373\n\nmean(rnorm(500, mean = 5, sd= 2))\n\n[1] 5.048081\n\nsd(rnorm(500, mean = 5, sd= 2))\n\n[1] 1.906707\n\nhist(rnorm(500, mean = 5, sd= 2))\n\n\n\n\n\n\n\n\n      Using rnorm(), we can create matrix and convert it into data frame or tibble.\n\nmat = matrix(rnorm(50), ncol = 5)\ncolnames(mat) = c(\"a\", \"b\", \"c\", \"d\", \"e\")\nas_tibble(mat)\n\n# A tibble: 10 × 5\n        a      b       c      d      e\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.812 -0.675  0.866   0.906 -0.225\n 2 -0.154 -0.958 -0.571  -0.314  1.45 \n 3  0.352 -1.79   1.17   -2.34   0.120\n 4 -0.569 -0.308 -0.368   0.264 -1.17 \n 5 -1.23   0.385 -0.375  -1.38   0.254\n 6 -0.869 -0.247  0.0179 -1.23   0.133\n 7  1.14   1.40   0.544   1.24   0.587\n 8 -1.10   1.49  -1.35   -0.651  1.32 \n 9 -0.752  0.145 -0.569   1.11  -0.968\n10  1.56   1.03  -0.249  -2.47   0.188\n\n\n      In additon to runif() and rnorm(), sample() function can be used to generate random numbers.\n\nsample(0:100,5)\n\n[1] 80 77 64 34 21\n\nsample(0:100,5, replace = TRUE)\n\n[1] 56 11 86 62 68\n\n\n\nmatrix(sample(0:50,50), ncol = 5)\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]   11   12   35   50   33\n [2,]   17    1   43   22   15\n [3,]   40   29   18   39   44\n [4,]   41   21   14   13   23\n [5,]    2    3   31   20   42\n [6,]   25   19   28   24    7\n [7,]   37   32   46    5   47\n [8,]    4   34   30   38   36\n [9,]   48   10    9    6   49\n[10,]   27   16   26    8    0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#indexing-and-slicing-vectors",
    "href": "r_basics.html#indexing-and-slicing-vectors",
    "title": "Appendix B — A Primer on R",
    "section": "B.5 Indexing and Slicing Vectors",
    "text": "B.5 Indexing and Slicing Vectors\n      Using [], one can access elements in a vector. Unlike python7, R uses 1 based indexing, meaning that first element has an index 1. For slicing, one can specify the range of index using :.\n\nsample_vector = c (10:25)\nsample_vector[5]\n\n[1] 14\n\nsample_vector[5:6]\n\n[1] 14 15\n\nsample_vector[c(5,8)]\n\n[1] 14 17\n\n\n      Negative indexing, unlike python, excludes the element from the vector.\n\nsample_vector[-5]\n\n [1] 10 11 12 13 15 16 17 18 19 20 21 22 23 24 25\n\nsample_vector[-c(5,8)]\n\n [1] 10 11 12 13 15 16 18 19 20 21 22 23 24 25\n\n\n      Conditional or logical indexing can be done on the vector as well.\n\nsample_vector &gt; 13\n\n [1] FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE\n\nsample_vector[sample_vector&gt;13]\n\n [1] 14 15 16 17 18 19 20 21 22 23 24 25\n\n\n      Additionally, different kinds of mathematical operatins can be applied on the vector.\n\nmin(sample_vector)\n\n[1] 10\n\nmax(sample_vector)\n\n[1] 25\n\nsum(sample_vector)\n\n[1] 280\n\nsd(sample_vector)\n\n[1] 4.760952\n\nrange(sample_vector)\n\n[1] 10 25\n\nmean(sample_vector)\n\n[1] 17.5\n\n\n      For data frame or tibble, one can use [row,column] for indexing or slicing.\n\ndf = data.frame(\n    id = 1:3,\n    name = c ('A', \"B\", \"C\"),\n    age = c (18, 21, 23)\n)\ndf['age']\n\n  age\n1  18\n2  21\n3  23\n\ndf$age\n\n[1] 18 21 23\n\ndf$age[1]\n\n[1] 18\n\ndf['age'][[1]][1]\n\n[1] 18\n\ndf[,]\n\n  id name age\n1  1    A  18\n2  2    B  21\n3  3    C  23\n\ndf[1,'age']\n\n[1] 18\n\ndf[1:2,'age']\n\n[1] 18 21\n\ndf[1:2,c('age','id')]\n\n  age id\n1  18  1\n2  21  2\n\n\n\ndf2 = tibble(\n    id = 1:3,\n    name = c ('A', \"B\", \"C\"),\n    age = c (18, 21, 23)\n)\ndf2['age']\n\n# A tibble: 3 × 1\n    age\n  &lt;dbl&gt;\n1    18\n2    21\n3    23\n\ndf2$age\n\n[1] 18 21 23\n\ndf2$age[1]\n\n[1] 18\n\ndf2['age'][[1]][1]\n\n[1] 18\n\ndf2[,]\n\n# A tibble: 3 × 3\n     id name    age\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 A        18\n2     2 B        21\n3     3 C        23\n\ndf2[1,'age']\n\n# A tibble: 1 × 1\n    age\n  &lt;dbl&gt;\n1    18\n\ndf2[1:2,'age']\n\n# A tibble: 2 × 1\n    age\n  &lt;dbl&gt;\n1    18\n2    21\n\ndf2[1:2,c('age','id')]\n\n# A tibble: 2 × 2\n    age    id\n  &lt;dbl&gt; &lt;int&gt;\n1    18     1\n2    21     2",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#random-numbers",
    "href": "r_basics.html#random-numbers",
    "title": "Appendix B — A Primer on R",
    "section": "B.6 Random Numbers",
    "text": "B.6 Random Numbers\n      In R, two widely used functions to generate random numbers are - runif() and rnorm(). runif() is used to generate random numbers from uniform distribution. The range of numbers generated using runif is 0 to 1.\n\n# vector of 2 elements \nrunif(2)\n\n[1] 0.3094118 0.9153653\n\n# vector of 5 elements from 0 to 100\nrunif(5, min = 0, max = 100)\n\n[1]  9.831609 34.816336 30.055107 62.850976 14.218783\n\n\n      rnorm() function generates random numbers from normal distribution with mean 0 and standard deviation 1.\n\nrnorm(5)\n\n[1]  1.4839078  0.8830858  0.1174016 -2.2038416  1.0810854\n\nrnorm(5, mean = 5, sd= 2)\n\n[1] 6.843230 3.361812 8.062965 3.504972 4.308373\n\nmean(rnorm(500, mean = 5, sd= 2))\n\n[1] 5.048081\n\nsd(rnorm(500, mean = 5, sd= 2))\n\n[1] 1.906707\n\nhist(rnorm(500, mean = 5, sd= 2))\n\n\n\n\n\n\n\n\n      Using rnorm(), we can create matrix and convert it into data frame or tibble.\n\nmat = matrix(rnorm(50), ncol = 5)\ncolnames(mat) = c(\"a\", \"b\", \"c\", \"d\", \"e\")\nas_tibble(mat)\n\n# A tibble: 10 × 5\n        a      b       c      d      e\n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1  0.812 -0.675  0.866   0.906 -0.225\n 2 -0.154 -0.958 -0.571  -0.314  1.45 \n 3  0.352 -1.79   1.17   -2.34   0.120\n 4 -0.569 -0.308 -0.368   0.264 -1.17 \n 5 -1.23   0.385 -0.375  -1.38   0.254\n 6 -0.869 -0.247  0.0179 -1.23   0.133\n 7  1.14   1.40   0.544   1.24   0.587\n 8 -1.10   1.49  -1.35   -0.651  1.32 \n 9 -0.752  0.145 -0.569   1.11  -0.968\n10  1.56   1.03  -0.249  -2.47   0.188\n\n\n      In additon to runif() and rnorm(), sample() function can be used to generate random numbers.\n\nsample(0:100,5)\n\n[1] 80 77 64 34 21\n\nsample(0:100,5, replace = TRUE)\n\n[1] 56 11 86 62 68\n\n\n\nmatrix(sample(0:50,50), ncol = 5)\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]   11   12   35   50   33\n [2,]   17    1   43   22   15\n [3,]   40   29   18   39   44\n [4,]   41   21   14   13   23\n [5,]    2    3   31   20   42\n [6,]   25   19   28   24    7\n [7,]   37   32   46    5   47\n [8,]    4   34   30   38   36\n [9,]   48   10    9    6   49\n[10,]   27   16   26    8    0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#functions",
    "href": "r_basics.html#functions",
    "title": "Appendix B — A Primer on R",
    "section": "B.7 Functions",
    "text": "B.7 Functions\n\n     In R, there are some functions that we use very frequently. In this section, we will discuss some of those functions.\n\nB.7.1 for Loop Function\n     for loop function in R is used to iterate over8 the elements of a vector, or a list, or a matrix, or a data frame, or a tibble and apply a set of instructions (operations) on each item of the object. for loop function is used to avoid unnecessary repeition of code, thus enhancing efficiency and effectiveness of code. The basic syntax of for loop function in R -\n\nfor (item in object) {\n    Statement\n}\n\n     Some examples of for loop function are -\n\nfor (x in seq(1:5)) {\n    print(x^2)\n}\n\n[1] 1\n[1] 4\n[1] 9\n[1] 16\n[1] 25\n\n\n\nfor (x in c('Adam', \"Joseph\", \"John\")) {\n    print(x)\n}\n\n[1] \"Adam\"\n[1] \"Joseph\"\n[1] \"John\"\n\n\n\n# Vectors for numbers and names\nnumbers &lt;- 1:3\nnames &lt;- c('Adam', 'Joseph', 'John')\n\n# Loop through the vectors and print the desired output\nfor (i in 1:length(numbers)) {\n    print(paste(numbers[i], names[i], sep = \", \"))\n}\n\n[1] \"1, Adam\"\n[1] \"2, Joseph\"\n[1] \"3, John\"\n\n\n\n\nB.7.2 map() Function\n      In map() function, the purpose is to apply a function on a vector or list, meaning that map() functions take a list/vector and a function as arguments. Figure B.2 sows the nature of map() function. The map() function then applies the function to each element of the list/vector and the result of this process is then combined into a list. For example, a list called sample_list is created. The length() function can be used to know the elements of the list. However, if you want to the element of each list, then map() function can be used. The other variation of map() functions that the package purrr include - map_int(), map_dbl(), map_chr(), and map_lgl()9.\n\n\n\n\n\n\nFigure B.2: map() Function Argument\n\n\n\n\nsample_list &lt;- \n    list(\n        x = 1737.1,\n        y = c(11.3, 6.2),\n        z = c(60.4, 81.4, 156, 174.8, 194, 34.8, 420, 2705.2, 340, 62)\n )\nsample_list\n\n$x\n[1] 1737.1\n\n$y\n[1] 11.3  6.2\n\n$z\n [1]   60.4   81.4  156.0  174.8  194.0   34.8  420.0 2705.2  340.0   62.0\n\nlength(sample_list)\n\n[1] 3\n\n\n\nmap(sample_list, length)\n\n$x\n[1] 1\n\n$y\n[1] 2\n\n$z\n[1] 10\n\n\n\nB.7.2.1 map() Functions with Multiple Inputs\n      The map2() functions are very similar to the map() functions, but they take two input vectors instead of one. Since the map2() functions iterate along the two vectors in parallel, they need to be of the same length. Figure B.3 shows a map2() function.\n\n\n\n\n\n\nFigure B.3: map2() Function\n\n\n\n\nx &lt;- c(1, 2, 4)\ny &lt;- c(6, 5, 3)\nmap2_dbl(x, y, min)\n\n[1] 1 2 3\n\n\n\ndf &lt;-\n    tibble(\n        a = c(1, 2, 4),\n        b = c(6, 5, 3)\n        )\n\ndf %&gt;%\n    mutate(min = map2_dbl(a, b, min))\n\n# A tibble: 3 × 3\n      a     b   min\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     6     1\n2     2     5     2\n3     4     3     3\n\n\n\ndf %&gt;%\n    mutate(\n        min = min(a,b)\n        )\n\n# A tibble: 3 × 3\n      a     b   min\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     6     1\n2     2     5     1\n3     4     3     1\n\n\n      By default mutate(), uses column-wise operations. map2_dbl() produces a column the same length at a and b. We can accomplish the same calculation using row-wise operations.\n\n df %&gt;%\n    rowwise() %&gt;%\n    mutate(min = min(a, b)) %&gt;%\n    ungroup()\n\n# A tibble: 3 × 3\n      a     b   min\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     6     1\n2     2     5     2\n3     4     3     3\n\n\n\n\nB.7.2.2 pmap() Functions\n      There are no map3() or map4() functions. Instead, you can use a pmap() (p for parallel) function to map over more than two vectors. The pmap() functions work slightly differently than the map() and map2() functions. In map() and map2() functions, you specify the vector(s) to supply to the function. In pmap() functions, you specify a single list that contains all the vectors (or lists) that you want to supply to your function. Figure B.4 shows a pmap() function.\n\n\n\n\n\n\nFigure B.4: pmap() Function (List)\n\n\n\n      Flipping the list diagram makes it easier to see that pmap() is basically just a generalized version of map2() (See Figure B.5).\n\n\n\n\n\n\nFigure B.5: pmap() Function (Flipped)\n\n\n\n      The only difference is that map2() lets you specify each vector as a separate argument. In pmap(), you have to store all your input vectors in a single list. This functionality allows pmap() to handle any number of input vectors.\n\nx &lt;- c(1, 2, 4)\ny &lt;- c(6, 5, 3)\nmap2_dbl(x, y, min)\n\n[1] 1 2 3\n\n\n\npmap_dbl(list(x, y), min)\n\n[1] 1 2 3\n\nz &lt;- c(100, 15, 1)\npmap_dbl(list(x, y, z), max)\n\n[1] 100  15   4\n\n\n\n\n\nB.7.3 User Defined Functions\n      User defined functions are also called named functions because they are assigned a name and expected to be called frequently. The goal of user defined functions is to optimize program, avoid or minimize the reptition of the code, prevent errors, and make the code more readable. In R, to declare a user defined function, we use the keywoard function(). The syntax of user defined function is given below. It is clear that there are three components of an R function - function name, function parameters (arguments), and function body (expression). We can drop curly braces ({}) from the function if the function body contains a single statement. Moreover, the function must be called with the correct number of arguments.\n\nfunctionname = function (arguments) {\n    function body \n}\n\n      Below are examples of some user defined functions.\n\nadd_numbers = function (x,y){\n    sum = x + y \n    return(sum)\n}\nadd_numbers(5,-25)\n\n[1] -20\n\n\n\ngreetings = function (name, greeting = \"Welcome\", sign = \"!\"){\n    greetingtogether = paste(name, sign, sep = \"\")\n    print(paste(greeting, greetingtogether))\n}\ngreetings(\"Nicole\")\n\n[1] \"Welcome Nicole!\"\n\n\n\n\nB.7.4 Anonymous Functions\n      Anonymous functions are functions that are not named, meaning that unlike normal functions, they do not have any name. Anonymous functions are useful when we do not plan to use them again and again. In most cases, we use anonymous function as an argument to other functions. In R, the synax of anonymous function is very much similar to named function. The syntax of anonymous function is - function (x) expression.\n\nvec = c (1,5,6)\nsapply(vec, function(x) x^2)\n\n[1]  1 25 36\n\n\n      Sometimes, using purrr, we can create anonymous function by one sided formula. In the below example, ~.x**2 works like function (x) x**2.\n\nmap_int(1:5, ~.x**2)\n\n[1]  1  4  9 16 25",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#conclusions",
    "href": "r_basics.html#conclusions",
    "title": "Appendix B — A Primer on R",
    "section": "B.8 Conclusions",
    "text": "B.8 Conclusions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#exercises",
    "href": "r_basics.html#exercises",
    "title": "Appendix B — A Primer on R",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "r_basics.html#footnotes",
    "href": "r_basics.html#footnotes",
    "title": "Appendix B — A Primer on R",
    "section": "",
    "text": "R got its name from the first letters of its creators. Both of their names start with letter R.↩︎\n1996 is the year when R was introduced.↩︎\nPlease note that in install.package(), you need to use quote (either single or double), but in library(), you do not need to use quote.↩︎\nThe word “data” sometimes is considered plural because there is a singular word for “data”, which is “datum”. In this book, both words are used interchangeably. Therefore, the word “data” is used as both singular and plural.↩︎\nThe other two atomic vectors include - complex and raw.↩︎\nAs opposed to NULL,NA refers to the absence of value.↩︎\nPython uses 0 based indexing.↩︎\nIteration (also called looping) is the process of repeating the same operation on different columns, or on different datasets. There are two important iteration paradigms: imperative programming and functional programming. On the imperative side, one has tools like for () loops and while () loops, which are a great place to start because they make iteration very explicit, so it is obvious what is happening. However, for () loops are quite verbose, and require quite a bit of bookkeeping code that is duplicated for every for loop. One the other hand Functional programming (FP) offers tools to extract out this dupli cated code, so each common for loop pattern gets its own function. Once one masters the vocabulary of FP, one can solve many common iteration problems with less code, more ease, and fewer errors. The apply () family functions in base R are equivalent to for () loop. The apply () family functions include the following functions for looping- apply (); lapply (); sapply (), tapply (), vapply (), and mapply (). The purrr package in tidyverse offers map () family functions for functional programming, which we will be using for the loop operations.↩︎\nThe output of map() function is always list. These variations help to get the desired output. To learn more about map functions, please visit - https://adv-r.hadley.nz/functionals.html.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>A Primer on R</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alles, Michael. 2015. “Drivers of the Use and Facilitators and\nObstacles of the Evolution of Big Data by the Audit Profession.”\nAccounting Horizons 29 (2): 439–49. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/439/2188.\n\n\nAlles, Michael, and Glen Gray. 2016. “Incorporating Big Data in\nAudits: Identifying Inhibitors and a Research Agenda to\nAddress Those Inhibitors.” International Journal of\nAccounting Information Systems 22: 44–59. https://www.sciencedirect.com/science/article/pii/S1467089516300811.\n\n\nAmerican Institute of Certified Public Accountants (AICPA). 2015.\n“Audit Analytics and Continuous\nAudit: Looking Toward the\nFuture.”\n\n\n———. 2017. “Description Criteria for\nManagement’s Description of the\nEntity’s Cybersecurity Risk\nManagement Program.”\n\n\nAppelbaum, Deniz. 2016. “Securing Big Data Provenance for\nAuditors: The Big Data Provenance Black Box as Reliable\nEvidence.” Journal of Emerging Technologies in\nAccounting 13 (1): 17–36. https://publications.aaahq.org/jeta/article-abstract/13/1/17/9219.\n\n\nAppelbaum, Deniz, Alexander Kogan, and Miklos A. Vasarhelyi. 2017.\n“Big Data and Analytics in the Modern Audit\nEngagement: Research Needs.” Auditing: A Journal\nof Practice & Theory 36 (4): 1–27. https://publications.aaahq.org/ajpt/article-abstract/36/4/1/6016.\n\n\nBarr-Pulliam, Dereck, Helen L. Brown-Liburd, and Amanda G. Carlson.\n2023. “Do Audit Data\nAnalytics Influence Juror\nPerceptions of Audit Quality and\nAuditor Negligence?” Current Issues\nin Auditing 17 (2): P1–10. https://publications.aaahq.org/cia/article/17/2/P1/10096.\n\n\nBarton, Dominic, and David Court. 2012. “Making Advanced Analytics\nWork for You.” Harvard Business Review 90 (10): 78–83.\nhttp://www.buyukverienstitusu.com/s/1870/i/Making_Advanced_Analytics_Work_For_You.pdf.\n\n\nBollen, Johan, Huina Mao, and Xiaojun Zeng. 2011. “Twitter Mood\nPredicts the Stock Market.” Journal of Computational\nScience 2 (1): 1–8. https://www.sciencedirect.com/science/article/pii/S187775031100007X.\n\n\nCao, Min, Roman Chychyla, and Trevor Stewart. 2015. “Big Data\nAnalytics in Financial Statement Audits.” Accounting\nHorizons 29 (2): 423–29. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/423/2177.\n\n\nColumbus. 2017. “53% Of Companies\nAre Adopting Big\nData Analytics.” Forbes. https://www.forbes.com/sites/louiscolumbus/2017/12/24/53-of-companies-are-adopting-big-data-analytics/?sh=6c98f39939a1.\n\n\nCrawley, Michael, and James Wahlen. 2014. “Analytics in\nEmpirical/Archival Financial Accounting Research.” Business\nHorizons 57 (5): 583–93. https://www.sciencedirect.com/science/article/pii/S0007681314000792.\n\n\nDai, Jun, and Miklos A. Vasarhelyi. 2016. “Imagineering\nAudit 4.0.” Journal of Emerging Technologies in\nAccounting 13 (1): 1–15. https://publications.aaahq.org/jeta/article-abstract/13/1/1/9242.\n\n\nDavis, Angela K., Jeremy M. Piger, and Lisa M. Sedor. 2012.\n“Beyond the Numbers: Measuring the\nInformation Content of Earnings\nPress Release Language.”\nContemporary Accounting Research 29 (3): 845–68. https://doi.org/10.1111/j.1911-3846.2011.01130.x.\n\n\nDeloitte. 2016. “Tax Data Analytics\nA New Era for Tax\nPlanning and Compliance.” https://www2.deloitte.com/content/dam/Deloitte/us/Documents/Tax/us-tax-data-analytics-a-new-era-for-tax-planning-and-compliance.pdf.\n\n\nDeloitte. 2024. “Deloitte Invests $2 Billion to Accelerate\nIndustryAdvantage™ for Its Clients.” https://www2.deloitte.com/us/en/pages/about-deloitte/articles/press-releases/deloitte-invests-two-billion-to-accelerate-industryadvantage-for-its-clients.html.\n\n\nEstep, Cassandra, Emily E Griffith, and Nikki L MacKenzie. 2024.\n“How Do Financial Executives Respond to the Use of Artificial\nIntelligence in Financial Reporting and Auditing?” Review of\nAccounting Studies 29 (3): 2798–2831.\n\n\nEY. 2023. “EY Announces Launch of Artificial Intelligence Platform\nEY.ai Following US$1.4b Investment.” https://www.ey.com/en_gl/newsroom/2023/09/ey-announces-launch-of-artificial-intelligence-platform-ey-ai-following-us-1-4b-investment.\n\n\nFedyk, Anastassia, James Hodson, Natalya Khimich, and Tatiana Fedyk.\n2022. “Is Artificial Intelligence Improving the Audit\nProcess?” Review of Accounting Studies 27 (3): 938–85.\n\n\nFeldman, Ronen, Suresh Govindaraj, Joshua Livnat, and Benjamin Segal.\n2010. “Management’s Tone Change, Post Earnings Announcement Drift\nand Accruals.” Review of Accounting Studies 15 (4):\n915–53. https://doi.org/10.1007/s11142-009-9111-x.\n\n\nForbes. 2024. “The Dawn of a New Era: AI’s Revolutionary Role in\nAccounting.” https://www.forbes.com/sites/neilsahota/2024/04/22/the-dawn-of-a-new-era-ais-revolutionary-role-in-accounting/?sh=1ff4bc4858af.\n\n\nKrahel, John Peter, and William R. Titera. 2015. “Consequences of\nBig Data and Formalization on Accounting and Auditing Standards.”\nAccounting Horizons 29 (2): 409–22. https://publications.aaahq.org/accounting-horizons/article/29/2/409/2149.\n\n\nLehavy, Reuven, Feng Li, and Kenneth Merkley. 2011. “The Effect of\nAnnual Report Readability on Analyst Following and the Properties of\nTheir Earnings Forecasts.” The Accounting Review 86 (3):\n1087–1115. https://publications.aaahq.org/accounting-review/article-abstract/86/3/1087/3300.\n\n\nLi, Feng. 2008. “Annual Report Readability, Current Earnings, and\nEarnings Persistence.” Journal of Accounting and\nEconomics 45 (2-3): 221–47. https://www.sciencedirect.com/science/article/pii/S0165410108000141.\n\n\n———. 2010. “The Information Content of\nForward‐Looking Statements in\nCorporate Filings—A\nNaïve Bayesian Machine\nLearning Approach.” Journal of\nAccounting Research 48 (5): 1049–1102. https://doi.org/10.1111/j.1475-679X.2010.00382.x.\n\n\nLi, Feng, Russell Lundholm, and Michael Minnis. 2013. “A\nMeasure of Competition Based on\n10‐K Filings.” Journal of\nAccounting Research 51 (2): 399–436. https://doi.org/10.1111/j.1475-679X.2012.00472.x.\n\n\nLoten, Angus. 2023. “PricewaterhouseCoopers to Pour $1 Billion\ninto Generative AI.” https://www.wsj.com/articles/pricewaterhousecoopers-to-pour-1-billion-into-generative-ai-cac2cedd.\n\n\nPersico, F, H Sidhu, et al. 2017. “How AI Will Turn Auditors into\nAnalysts.” Accounting Today. https://www.accountingtoday.com/opinion/how-ai-will-turn-auditors-into-analysts.\n\n\nProtiviti. 2017. “Embracing Analytics in\nAuditing.” https://www.protiviti.com/sites/default/files/2022-06/2017-internal-audit-capabilities-and-needs-survey-protiviti.pdf.\n\n\nProvost, Foster, and Tom Fawcett. 2013. “Data Science\nand Its Relationship to Big Data\nand Data-Driven Decision\nMaking.” Big Data 1 (1): 51–59. https://doi.org/10.1089/big.2013.1508.\n\n\nRaschka, Sebastian, Yuxi (Hayden) Liu, and Vahid Mirjalili. 2022.\nMachine Learning with PyTorch and Scikit-Learn: Develop Machine\nLearning and Deep Learning Models with Python. Packt Publishing. https://books.google.com/books/about/Machine_Learning_with_PyTorch_and_Scikit.html?id=SVxaEAAAQBAJ.\n\n\nReddi, Vijay. 2025. Machine Learning Systems. \"Creative\nCommons\". https://mlsysbook.ai/.\n\n\nRhys, Hefin. 2020. Machine Learning with r, the Tidyverse, and\nMlr. Simon; Schuster.\n\n\nRichins, Greg, Andrea Stapleton, Theophanis C. Stratopoulos, and\nChristopher Wong. 2017. “Big Data Analytics: Opportunity or Threat\nfor the Accounting Profession?” Journal of Information\nSystems 31 (3): 63–79. https://publications.aaahq.org/jis/article-abstract/31/3/63/1114.\n\n\nRose, Anna M., Jacob M. Rose, Kerri-Ann Sanderson, and Jay C. Thibodeau.\n2017. “When Should Audit Firms Introduce Analyses of Big Data into\nthe Audit Process?” Journal of Information Systems 31\n(3): 81–99. https://publications.aaahq.org/jis/article-abstract/31/3/81/1123.\n\n\nSchatsky, D, C Muraskin, and G Ragu. 2015. “Cognitive\nTechnologies: The Real Opportunities for Business.” https://www2.deloitte.com/us/en/insights/deloitte-review/issue-16/cognitive-technologies-business-applications.html.\n\n\nSchneider, Gary P., Jun Dai, Diane J. Janvrin, Kemi Ajayi, and Robyn L.\nRaschke. 2015. “Infer, Predict, and Assure:\nAccounting Opportunities in Data Analytics.”\nAccounting Horizons 29 (3): 719–42. https://publications.aaahq.org/accounting-horizons/article-abstract/29/3/719/2262.\n\n\nSivarajah, Uthayasankar, Muhammad Mustafa Kamal, Zahir Irani, and\nVishanth Weerakkody. 2017. “Critical Analysis of Big\nData Challenges and Analytical Methods.” Journal\nof Business Research 70: 263–86. https://www.sciencedirect.com/science/article/pii/S014829631630488X.\n\n\nThe Economist. 2017. “The World’s Most Valuable Resource Is No\nLonger Oil, but Data.” https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data.\n\n\nVasarhelyi, Miklos A., Alexander Kogan, and Brad M. Tuttle. 2015.\n“Big Data in Accounting: An Overview.”\nAccounting Horizons 29 (2): 381–96. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/381/2184.\n\n\nVerver, John. 2015. “Six Audit Analytics\nSuccess Factors.” Internal\nAuditor 72 (3).\n\n\nVien, Courtney. 2018. “Using Drones to Enhance Audit.”\nJournal of Accountancy. https://www.journalofaccountancy.com/podcast/using-drones-to-enhance-audits.html.\n\n\nWarren, J. Donald, Kevin C. Moffitt, and Paul Byrnes. 2015. “How\nBig Data Will Change Accounting.” Accounting Horizons 29\n(2): 397–407. https://publications.aaahq.org/accounting-horizons/article/29/2/397/2168.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.\n\n\nYoon, Kyunghee, Lucas Hoogduin, and Li Zhang. 2015. “Big Data as\nComplementary Audit Evidence.” Accounting Horizons 29\n(2): 431–38. https://publications.aaahq.org/accounting-horizons/article/29/2/431/2215.\n\n\nZhang, Juan, Xiongsheng Yang, and Deniz Appelbaum. 2015. “Toward\nEffective Big Data Analysis in Continuous Auditing.”\nAccounting Horizons 29 (2): 469–76. https://publications.aaahq.org/accounting-horizons/article/29/2/469/2160.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytics for Accounting Data",
    "section": "",
    "text": "Welcome\nWelcome to the book - Analytics for Accounting Data - which is slated to be published future. It will be available for purchase in both paperback and hardback, with pre-ordering available on both Amazon and online.\nThis is the online version of the book, which is free to use. The book is being developed. So it is recommended to use the book with caution. More chapters and materials wil be added to the book gradually.\nIf you want me to add some materials that are necessary for the students, please do not hesitate to ask. All kinds of comments, recommendations, suggestions, criticisms are welcome.\n\n\n\n\n\n\nWarning\n\n\n\nThis book is a work in progress.\n\n\n\n\nPreface\n\n\nAbout the Book\nThe book is written for the students in undergraduate and graduate programs.\n\n\nAbout the Author\n\n\n\n\nSharif Islam, DBA, CPA, CMA is an Assistant professor in School of Accountancy in Southern Illinois University Carbondale (SIUC). He is a licensed CPA in Illinois and a Certified Management Accountant (CMA). He teaches Auditing, Accounting Information Systems, Machine Learning, and Analytics for Accounting Data. He did his doctorate from Louisiana Tech University in Computer Information Systems and Accounting. His mansucripts are selected for “Best Research Paper Award” in several conferences of American Accounting Association (AAA). His research also got 2024 “Notable Contribution to the Literature Award” by AIS section of AAA. He published research in Accounting Horizons, Journal of Accounting and Public Policy, Journal of Emerging Technologies in Accounting, Issues in Accounting Education, Advances in Accounting and Managerial Auditing Journal. His research interests lie at the intersection of Accounting and Data Science.\n\n\n\nHow to Read the Book\n\n\nAcknowledgment\n\nTo prepare the book, I took help from many sources on the internet and published materials. Many of them are cited in the book. I acknowledge the contribution of all of those resources that help me to prepare the book for the students.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "1  Overview of Accounting Analytics",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#data-analytics-big-data",
    "href": "overview.html#data-analytics-big-data",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.1 Data Analytics & Big Data",
    "text": "1.1 Data Analytics & Big Data\n\n     Given the availability of vast amount of data, companies in numerous industries exploit such data for competitive advantage, aiming to either increase revenues or decrease costs. Data Driven Decisions (DDD) are making significant differences in productivity, on Return on Assets (ROA), Return on Equity (ROE), asset utilization, and on market value (Provost and Fawcett 2013). Firms using data analytics in their operations can outperform their competitors by 5% in productivity and 6% in profitability (Barton and Court 2012). In 2017, 53% companies have adopted big data, as compared to only 17% in 2015 (Columbus 2017). Additionally, regulators are increasingly calling for organizations to use analytics (Protiviti 2017). This evolving landascape in industry emphasizes the significance of data analytics in organizations.\n     Analytics is a means of extracting value from data. Analytics is the assessment of data with technology tools. Today, there are more powerful analytics tools to more efficiently and effectively analyze a broader range of data and types of data than in the past. Thus, there is an increased opportunity for enhanced insights about what stories data can tell to address business issues and transform the way decisions are made.\n     The meaning of (big) data analytics varies across different disciplines and there is substantive confusion between the slightly differing characterizations of “big data,” “business intelligence,” and “data analytics” (Vasarhelyi, Kogan, and Tuttle 2015). Though many people consider big data in terms of quantities, it is also related to large-scale analysis of large amounts of data to generate insights and knowledge (Verver 2015). Big data is characterized by four Vs: Volume; Velocity; Variety; and Veracity. Volume refers to the size of the dataset, velocity to the speed of data generation, variety to the multiplicity of data sources, and veracity to the elimination of noise and obtaining truthful information from big data. Sometimes big data are characterized by six Vs: Volume, Velocity, Variety, Veracity, Variability, and Value; or, even seven Vs: Volume, Velocity, Variety, Veracity, Variability, Value, and Visualization (Sivarajah et al. 2017). Some people also identify sometimes eight Vs for big data. Figure 1.1 and Figure 1.2 depict the six and eight Vs of big data respectively.\n\n\n\n\n\n\nFigure 1.1: Six Vs of Big Data\n\n\n\n     Data analytics is defined as “the art and science of discovering and analyzing patterns, identifying anomalies, and extracting other useful information in data underlying or related to the subject matter of an audit through analysis, modeling, and visualization for the purpose of planning or performing the audit” (American Institute of Certified Public Accountants (AICPA) 2015, 105). Cao, Chychyla, and Stewart (2015) define big data analytics as the process of inspecting, cleaning, transforming, and modeling big data to discover and communicate useful information and patterns, suggest conclusions, and to provide support for decision-making.\n\n\n\n\n\n\nFigure 1.2: Eight Vs of Big Data\n\n\n\n      Data analytics promises significant potential in auditing. Therefore, in accounting, sometimes data analytics becomes synonymous with audit analytics. Audit analytics involves the application of data analytics in the audit. Specifically, American Institute of Certified Public Accountants (AICPA) (2017) defines audit data analytics as “the science and art of discovering and analyzing patterns, identifying anomalies and extracting other useful information in data underlying or related to the subject matter of an audit through analysis, modeling and visualization for the purpose of planning or performing the audit.” In other words, audit data analytics are techniques that can be used to perform a number of audit procedures such as risk assessment, tests of details, and substantive analytical procedure to gather audit evidence. The benefits of using audit data analytics include improved understanding of an entity’s operations and associated risk including the risk of fraud, increased potential for detecting material misstatements, and improved communications with those charged with governance of audited entities.\n\n\n1.1.1 Big Data Spectrum\n\n     The data dynamics and the way businesses are using data are relatively new to the world of business and something that accounting and business students must become more familiar with. Figure 1.3 depicts the big data spectrum. You’ll notice that the data in yellow at the left bottom portion of the spectrum has less of the four V’s. This typically is data that is sourced from enterprise resource planning (ERP) systems. This is also the type of data that a business analyst, especially an accountant, most often works with.\n     Continuing along the trajectory, you’ll see data generated from operating systems, such as call center records, email, voicemail, etc. From there, you’ll see data that comes from the web, including shopping cart information, web logs, browser history, promotion information, etc. This then moves beyond to the “biggest” category of data, which includes videos, radio-frequency identification (RFID), global positioning system (GPS) coordinates, social media feeds and more.\n\n\n\n\n\n\nFigure 1.3: Big Data Spectrum",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#importance-of-data-analytics-in-accounting",
    "href": "overview.html#importance-of-data-analytics-in-accounting",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.2 Importance of Data Analytics in Accounting",
    "text": "1.2 Importance of Data Analytics in Accounting\n\n     Data analytics is important for accounting profession because data gathering and analytics technologies have the potential to fundamentally change accounting and auditing task processes (Schneider et al. 2015). Scholars note that the emergence of data analytics will significantly change the infer/predict/assure (e.g., insight/foresight/oversight) tasks performed by accountants and auditors. Big data and analytics have increasingly important implications for accounting and will provide the means to improve managerial accounting, financial accounting, and financial reporting practices (Warren, Moffitt, and Byrnes 2015). It is further suggested that big data offers an unprecedented potential for diverse, voluminous datasets and sophisticated analyses. Research indicates that big data has great potential to produce better forecast estimates, going concern calculations, fraud, and other variables that are of concern to both internal and external auditors (Alles 2015). Moreover, auditors might reduce audit costs and enhance profitability and effectiveness by means of big data or data analytics. Sixty-six percent of internal audit departments currently utilize some form of data analytics as part of the audit process (Protiviti 2017).\n\n\n1.2.1 Data Analytics in Financial Accounting\n\n     Warren, Moffitt, and Byrnes (2015) note that “in financial accounting, big data will improve the quality and relevance of accounting information, thereby enhancing transparency and stakeholder decision-making. In reporting, big data can assist with the creation and refinement of accounting standards, helping to ensure that the accounting profession will continue to provide useful information as the dynamic, real-time, global economy evolves.” In particular, they suggest that big data could significantly impact the future of financial accounting and Generally Accepted Accounting Principles (GAAP). Big data can also help to supplement financial statement disclosures by accumulating, processing, and analyzing information about a given intangible of interest. Furthermore, big data or data analytics can help in narrowing the differences between accounting standards such US GAAP and International Financial Reporting Standards (IFRS) and facilitate different measurement processes such as Fair Value Accounting (FVA) by analyzing different kinds of unstructured data (Warren, Moffitt, and Byrnes 2015).\n     Crawley and Wahlen (2014) noted that data analytics allows researchers to explore a large amount of qualitative information disclosed by organizations, and examines the consequences of such disclosures. Moreover, data analytics now provides the opportunity to judge the informational content of qualitative financial information. For example, Davis, Piger, and Sedor (2012) found that the extent of optimism expressed in firms’ earnings announcements is positively associated with Return on Assets (ROA) and stock reactions. By the same token, Li (2010) suggested that the tone of forward-looking statements is positively associated with future earnings performance. In addition, Feldman et al. (2010) found that changes in disclosure tone is indicative of future changes in earnings. Interestingly, research shows that even information on social media such as Twitter can predict stock market responses (Bollen, Mao, and Zeng 2011).\n     Data analytics helps to relate textual data to earnings quality. For example, firms having more complicated and less transparent financial statement disclosures are more likely to have poor quality earnings, less persistent positive earnings and more persistent negative earnings (Li 2008). Li, Lundholm, and Minnis (2013) confirmed that firms discussing their competition frequently have ROAs that mean returns more severely than the firms discussing the competition infrequently.\n     With the help of textual data analytics, researchers recently documented the role that qualitative disclosures have in forming the information environment of organizations; such information environments include factors such as the number of analyst following a firm, characteristics of its investors, its trading activities, and the litigation it is involved with. Less readable 10-Ks are associated with greater number of analysts following the firm and a greater amount of effort needed to generate report about it (Lehavy, Li, and Merkley 2011). They also find that less readable 10-Ks are associated with greater dispersion, lower accuracy, and greater uncertainty in analyst’s earnings forecasts about a given firm.\n\n\n\n\n\n\n\nSummary Point\n\n\n\n\n\nData analytics in Financial Accounnting - A. has potential to enhance quality and relevance of accounting information, B. can supplemental financial statement disclosures, C. can facilitate different measurement processes, and D. allows to explore a large amount of qualitative information\n\n\n\n\n\n1.2.2 Data Analytics in Management Accounting\n\n     Warren et al. (2015, 397) noted that “in managerial accounting, big data will contribute to the development and evolution of effective management control systems and budgeting processes. In particular, they elaborate on how big data or data analytics can play a role in management control systems by discovering behaviors that have correlation with specific goal outcomes. Essentially, big data analytics can locate new kinds of behaviors that might impact goal outcomes by simplifying the identification of important motivational measurement tools linked to organizational goals. Moreover, by analyzing non-structured data, big data analytics can help discern employee morale, productivity, and customer satisfaction. Data analytics can also be used to improve “beyond budgeting practices” since traditional budgeting sometimes creates barriers to creativity and flexibility (Warren, Moffitt, and Byrnes 2015).\n     Richins et al. (2017) suggest that big data analytics could improve customer service quality. They suggest that most of the time organizations use structured data that are in their records to evaluate customer service quality; however, this approach does not take into account the customer perspective. Big data analytics allow organizations to evaluate this customer perspective by using unstructured data from social media or e-commerce sites, thus permitting organizations to have a holistic view of customer service quality.\n     Managers recognize that financial measures, alone, are insufficient to forecast future financial success or to use for performance management. Big data analytics provides opportunities to incorporate non-financial measures by incorporating unstructured data (Richins et al. 2017). Using big data analytics (particularly the analysis of unstructured data) accountants can identify the causes of underlying problems, understand ramifications, and develop plans to mitigate adverse impacts (Richins et al. 2017). Data analytics can also provide accountants with additional tools to monitor operations and product quality, discover opportunities to reduce costs, and contribute to decision-making (Dai and Vasarhelyi 2016).\n\n\n\n\n\n\n\nSummary Point\n\n\n\n\n\nData analytics in Management Accounnting - A. will contribute to the development of effective management control systems and budgeting processes, B. can enhance employee morale, productivity, and customer satisfaction, C. can enhance customer service quality by evaluating customer perspectives using unstructured data from social media or e-commerce, D. creates opportunities to incorporate non-financial measures by incorporating unstructured data, and E. provide accountants with additional tools to monitor operations and product quality, and discover opportunities to reduce costs.\n\n\n\n\n\n1.2.3 Data Analytics in Auditing\n\n     Data analytics has the potential to improve the effectiveness of auditing by providing new forms of audit evidence. Data analytics can be used in both auditing planning and in audit procedures, helping auditors to identify and assess risk by analyzing large volumes of data. Even organizations that have very immature capabilities indicate that a strong level of value is derived from including analytics in the audit process (Protiviti 2017).\n     Big data is being seen by practitioners as an essential part of assurance services (Alles and Gray 2016), but its application in auditing is not as straightforward as it is in marketing and medical research. Appelbaum (2016) and Cao, Chychyla, and Stewart (2015) identified several areas that are likely to benefit from the use of big data analytics. Some of the areas are:\n\nAt the engagement phase – supplementing auditors’ industry and client knowledge\nAt the planning phase – supplementing auditors’ risk assessment process\nAt the substantive test phase – verifying the management assertions\nAt the review phase – advanced data analytical tools as analytical procedures\nAt the continuous auditing phase – enhancing knowledge about the clients\n\n     Yoon, Hoogduin, and Zhang (2015) suggest that big data create great opportunities through providing audit evidence. They focused on the “sufficiency” and “appropriate” criteria and noted that though there are some issues about the propriety of big data due to different kinds of “noise,” big data can be used as complementary audit evidence. Additionally, they discussed how big data can be integrated with traditional audit evidence in order to add value in the process. Big data or data analytics can also help auditors to test the existence of assertions (e.g. fixed assets) using non-conventional data such as video recording (Warren, Moffitt, and Byrnes 2015). In the world of big data, potential types and sources of audit evidence have changed (Appelbaum 2016). For this reason, Krahel and Titera (2015) suggest that big data might change the focus of auditors, shifting emphasis from management to the verification of data.\n     Data quality and reliability or verifiability have become important issues in auditors’ evaluations of audit evidence. In this way, big data can be used as part of analytical procedures, which are required at the planning and review phase, but which are optional at the substantive procedure phase. However, many issues remain unresolved about how to use big data since analytical procedures and auditing standards are not very specific about the selection of analytical audit procedures; the choice depends on the professional judgment of auditors (Appelbaum, Kogan, and Vasarhelyi 2017). For this reason, auditors need to exercise increased professional skepticism in the big data era because in many cases sources of big data lack provenance and, subsequently, veracity, and sometimes auditors (particularly internal auditors) have little or no involvement in data quality evaluation of such sources (Appelbaum 2016). Considering the prediction that analytics will spell the demise of auditing, Richins et al. (2017) suggest that auditors in the big data era are still essential because they know “the language of business.” Particularly, they suggest that big data analytics cannot replace the professional judgment used by auditors, suggesting that analytics will instead complement auditors’ professional judgment.\n     Alles and Gray (2016) identify four potential advantages of incorporating big data into audit practices: strong predictive power to set expectations for financial statement audits, great opportunities to identify potential fraudulent activities, increased probabilities of discovering red flags, and the possibility of developing more predictive models for going concern assumptions. To that end, internal audit groups with dedicated analytics functions and organizations that have attained a managed or optimized to the state of analytics maturity are far more likely to conduct continuous auditing (Protiviti 2017). Though big data creates many opportunities for improving auditing, it also suffers from different shortcomings that hinder its application in Continuous Auditing (CA). For example, Zhang, Yang, and Appelbaum (2015) suggest big data characteristics such as volume, velocity, variety, and veracity creates problems in its application in CA through different gaps such as data consistency, data integrity, data identification, data aggregation, and data confidentiality.\n     Rose et al. (2017) found that the timing of the introduction of data analytics tools into the audit process affects the evaluation of evidence and professional judgment. Barr-Pulliam, Brown-Liburd, and Carlson (2023) found that jurors consider auditors more negligent when they use traditional auditing technique rather than audit data analytics techniques. Additionally, they confirmed that audit data analytics tools increase the perceptions of audit quality. Schneider et al. (2015) suggest that data analytics can be used by auditors to evaluate the internal control effectiveness and policy compliance. They further suggest that by analyzing unusual data flows, unexpected large volumes of data, high frequency transactions, or duplicate vendor payments, auditors can better detect fraud.\n\n\n\n1.2.4 Data Analytics in Tax Accounting\n\n     Traditionally, tax analytics has focused on hindsight, particularly dealing with data from transactions that have already occurred. However, recently tax organizations are looking to use data more for gaining insight and sometimes for foresight. Analytics can help to move tax toward insight and foresight, thus changing the mindset from “what do I need to do?” to “what do I need to know?” (Deloitte 2016).\n     Data analytics can help an organization and its tax function drive toward becoming an insight-driven organization, or IDO (Deloitte 2016). Various types of analytics can be applied to tax issues. Organizations have used tax analytics mostly in creating descriptive scorecards and visualizations (hindsight). These kinds of tax analytics help to determine where to allocate resources, focus on anomalies in results, and identify potential areas of risk.",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#types-of-data-analytics",
    "href": "overview.html#types-of-data-analytics",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.3 Types of Data Analytics",
    "text": "1.3 Types of Data Analytics\n\n     Data analytics can be classified in many ways, but usually there are four types of data analytics. Figure 1.4 depicts the types -\n\nDescriptive Analytics (Business Intelligence & Data Mining)\nDiagnostic Analytics\nPredictive Analytics (Forecasting)\nPrescriptive Analytics (Optimization & Simulation)\n\n\n1.3.1 Descriptive Analytics\n     Descriptive analytics looks at data and analyze past event for insight as to how to approach future events. It looks at past performance and understands the performance by mining historical data to understand the cause of success or failure in the past. Almost all management reporting such as sales, marketing, operations, and finance uses this type of analysis. Common types of descriptive analytis are -\n\nData Queries\nReports\nDescriptive Statistics\nData Dashboard\n\n\n\n\n\n\n\nFigure 1.4: Types of Data Analytics\n\n\n\n\n\n1.3.2 Diagnostic Analytics\n     In this analysis, we generally use historical data over other data to answer any question or for the solution of any problem. We try to find any dependency and pattern in the historical data of the particular problem. For example, companies go for this analysis because it gives a great insight into a problem, and they also keep detailed information about their disposal otherwise data collection may turn out individual for every problem and it will be very time-consuming. Common techniques used for Diagnostic Analytics are:\n\nData Discovery\nData Mining\nCorrelations\n\n\n\n1.3.3 Predictive Analytics\n     Predictive analytics turn the data into valuable, actionable information. predictive analytics uses data to determine the probable outcome of an event or a likelihood of a situation occurring. Predictive analytics holds a variety of statistical techniques from modeling, machine learning, data mining, and game theory that analyze current and historical facts to make predictions about a future event. Techniques that are used for predictive analytics are:\n\nLinear Regression\nTime Series Analysis and Forecasting\nData Mining\n\n\n\n1.3.4 Prescriptive Analytics\n     Prescriptive Analytics automatically synthesize big data, mathematical science, business rule, and machine learning to make a prediction and then suggests a decision option to take advantage of the prediction. Prescriptive analytics goes beyond predicting future outcomes by also suggesting action benefits from the predictions and showing the decision maker the implication of each decision option. Prescriptive Analytics not only anticipates what will happen and when to happen but also why it will happen. Further, Prescriptive Analytics can suggest decision options on how to take advantage of a future opportunity or mitigate a future risk and illustrate the implication of each decision option.",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#data-analytics-processes",
    "href": "overview.html#data-analytics-processes",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.4 Data Analytics Processes",
    "text": "1.4 Data Analytics Processes\n\n     As in in any scientific discipline, data analytics involves a rigorous step-by-step process. Each step demands different skills and know-how. However, to realize the full potential of data analytics, understanding the whole process is important. Figure 1.5 delineates the whole process.\n\n1.4.1 Defining the Questions\n     The first step in any data analysis process is to define your objective. In data analytics jargon, this is sometimes called the ‘problem statement’. Defining your objective means coming up with a hypothesis and figuring how to test it. For instance, your organization’s senior management might pose an issue, such as: “Why are we losing customers?” It’s possible, though, that this doesn’t get to the core of the problem. A data analyst’s job is to understand the business and its goals in enough depth that they can frame the problem the right way.\n\n\n1.4.2 Collecting the Data\n     \n\n\n1.4.3 Cleaning the Data\n     \n\n\n\n\n\n\nFigure 1.5: Data Analytics Process\n\n\n\n\n\n1.4.4 Analyzing the Data\n     \n\n\n1.4.5 Sharing Your Results",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#analytics-mindset",
    "href": "overview.html#analytics-mindset",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.5 Analytics Mindset",
    "text": "1.5 Analytics Mindset\n\n     Having an analytics mindset is important while performing data analytics processes. An analytics mindset is the ability to:\n\nAsk the right questions\nExtract, transform and load relevant data (i.e., the ETL process)\nApply appropriate data analytics techniques\nInterpret and share the results with stakeholders\n\n\n1.5.1 Ask the Right Questions\n     To drive better decisions, one must ask the right questions first and then seek answers in the data. Then one looks to find relevant data and the appropriate data sources to perform the analytics. These analytics will provide the insights and answer the business questions being asked, which then drive the decisions. Throughtout the process, there is a continuous feedback loop that makes the process iterative. An analytics mindset keeps asking questions until the right answers emerge. The right questions are those that lead to the right answers. Figure 1.6 depicts how analytics mindset works.\n\n\n\n\n\n\nFigure 1.6: Analytics Mindset Decision Making\n\n\n\n     Being able to ask the right question is not easy; rather it heavily relies on several factors. First, the analyst needs to understand who the relevant stakeholders are and their objectives. Knowing your audience and what they want to accomplish is critical to understanding value and how to identify a right question. Second, the analyst needs to have an understanding of the business and the underlying business processes — the overall business context. As an example, if you were asked to perform a competitive analysis across the high-tech industry and if you didn’t have a strong understanding of the industry and key performance indicators, you might not ask the right questions (e.g., select the right indicators to analyze).\n\n\n1.5.2 Extract, Transform, and Load Relevant Data (The ETL Process)\n     The first focus with this competency is understanding data characteristics and their relevance. In terms of data characteristics, we already discussed the four/six V’s and the big data spectrum. In determining relevance, this is a focus on what data aligns with the analysis you need to perform to answer your question.It is also important to understand the flow of data in an accounting information system to understand where your data is coming from and how it is generated. This understanding includes - type of accounting information systems, what modules are in the context, capabilities and the limitations of the data and so on. Once this understanding is established, the ETL process can begin. This starts with the extraction of data. For extraction, key things need to be known, including: what data to ask for, how to ask for data, how to manage data security, what format the data needs to be in.\n     The next step is transformation, which also is referred to as data cleansing. This involves converting data from one format to another to load it into an analytics tool. This includes making certain that only the data needed is extracted and that this data is complete and accurate. Data cleansing needs to be performed both before and after the data loading process. Loading data includes knowing which tool the data should be loaded into for the most efficient and effective analysis. For example, this might be driven by the amount of data and the capacity of a given analytics tool. Throughout the ETL process, it is important to maintain the integrity of your data. This is often done through data validation, for example, a control total of your data matching an account balance total in the general ledger.\n\n\n1.5.3 Apply Appropriate Data Analytics Techniques\n     In determining how to apply appropriate data analytics techniques, it is important to understand: the purpose of different types of data analytics techniques, how to determine which techniques are most appropriate for the objectives of your analysis. objectives might include a need to prove or disprove your expectation if one was developed. For example, during the planning phase of an audit, the auditor is required to assess risk. One way of doing this is by exploring the data and looking for anomalies. As the audit progresses into the execution phase, the auditor considers the risk of error or intentional misstatement, the effectiveness of controls and the amounts actually recorded. The objective is to confirm or disconfirm an expectation regarding recorded amounts.\n     There are many ways to analyze data. Some of the more fundamental analyses that you should be able to understand and apply include:\n– Ratios (e.g., gross margin or a day’s sales in accounts receivable)\n– Sorting (e.g., by industry or month)\n– Aggregation (e.g., total of an account balance)\n– Trends (e.g., the movement in inventory associated with both purchases and sales)\n– Comparisons (e.g., sales month to month)\n– Forecasting (e.g., budgeted expenses)\n     It is also important to gain familiarity with analytics tools. There are many tools capable of performing analytics and it isn’t necessary for you to know how to use each one, but you should have some hands-on experience with a few of the more fundamental tools that are most readily used by an analyst. Some of the fundamental tools include: Excel, Basic databases (Access), Visualization (Tableau, Power BI). It is also good to have a working knowledge or awareness of other tools, including those that might be specific to the career path you are choosing. Note that beyond these fundamental tools, there are other tools students should be familiar with, to a lesser extent (a working knowledge or awareness level).\n\n\n1.5.4 Interpret and Share the Results\n     As discussed previously, the end goal is to provide insights to your stakeholders based on the objectives that were identified. Your insights are derived from your interpretation of the analytics results. Therefore, it is important that you interpret the outcomes of your analysis appropriately, based on your question and expectation, if you had one. When you look at the results of your analysis, use your critical thinking and ask yourself:\n– What do you see?\n– Do you see what you expected to see?\n– Do your results make sense to you?\n– Is any further analysis required to meet your objective?\n     Once you have interpreted your results, you need to summarize them in a manner conducive to and compelling for your stakeholder. Visualization can be used as a technique and a way to present findings as well. You can make choices about displaying your analysis in a variety of ways, which might include tables, area charts, map charts, heat maps, Gantt charts, horizontal or vertical bar charts, pie charts, line charts, scatter plots, bubble charts and more. When making choices about which visualization is appropriate, there are many design principles to consider. These might include color, sizing, labeling, visual simplicity (e.g., elimination of visual clutter), etc",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#data-analytics-skillset",
    "href": "overview.html#data-analytics-skillset",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.6 Data Analytics Skillset",
    "text": "1.6 Data Analytics Skillset\n\n     Proficiency in data analytics involves a combination of skills. For example, a solid understanding in mathematics and statistics helps one to gain the foundational knowledge in data science. Figure 1.7 depicts the skills necessary to be proficient in data science.\n\n\n\n\n\n\nFigure 1.7: Data Science Skillset\n\n\n\n\n\n\n\n\nAlles, Michael. 2015. “Drivers of the Use and Facilitators and Obstacles of the Evolution of Big Data by the Audit Profession.” Accounting Horizons 29 (2): 439–49. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/439/2188.\n\n\nAlles, Michael, and Glen Gray. 2016. “Incorporating Big Data in Audits: Identifying Inhibitors and a Research Agenda to Address Those Inhibitors.” International Journal of Accounting Information Systems 22: 44–59. https://www.sciencedirect.com/science/article/pii/S1467089516300811.\n\n\nAmerican Institute of Certified Public Accountants (AICPA). 2015. “Audit Analytics and Continuous Audit: Looking Toward the Future.”\n\n\n———. 2017. “Description Criteria for Management’s Description of the Entity’s Cybersecurity Risk Management Program.”\n\n\nAppelbaum, Deniz. 2016. “Securing Big Data Provenance for Auditors: The Big Data Provenance Black Box as Reliable Evidence.” Journal of Emerging Technologies in Accounting 13 (1): 17–36. https://publications.aaahq.org/jeta/article-abstract/13/1/17/9219.\n\n\nAppelbaum, Deniz, Alexander Kogan, and Miklos A. Vasarhelyi. 2017. “Big Data and Analytics in the Modern Audit Engagement: Research Needs.” Auditing: A Journal of Practice & Theory 36 (4): 1–27. https://publications.aaahq.org/ajpt/article-abstract/36/4/1/6016.\n\n\nBarr-Pulliam, Dereck, Helen L. Brown-Liburd, and Amanda G. Carlson. 2023. “Do Audit Data Analytics Influence Juror Perceptions of Audit Quality and Auditor Negligence?” Current Issues in Auditing 17 (2): P1–10. https://publications.aaahq.org/cia/article/17/2/P1/10096.\n\n\nBarton, Dominic, and David Court. 2012. “Making Advanced Analytics Work for You.” Harvard Business Review 90 (10): 78–83. http://www.buyukverienstitusu.com/s/1870/i/Making_Advanced_Analytics_Work_For_You.pdf.\n\n\nBollen, Johan, Huina Mao, and Xiaojun Zeng. 2011. “Twitter Mood Predicts the Stock Market.” Journal of Computational Science 2 (1): 1–8. https://www.sciencedirect.com/science/article/pii/S187775031100007X.\n\n\nCao, Min, Roman Chychyla, and Trevor Stewart. 2015. “Big Data Analytics in Financial Statement Audits.” Accounting Horizons 29 (2): 423–29. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/423/2177.\n\n\nColumbus. 2017. “53% Of Companies Are Adopting Big Data Analytics.” Forbes. https://www.forbes.com/sites/louiscolumbus/2017/12/24/53-of-companies-are-adopting-big-data-analytics/?sh=6c98f39939a1.\n\n\nCrawley, Michael, and James Wahlen. 2014. “Analytics in Empirical/Archival Financial Accounting Research.” Business Horizons 57 (5): 583–93. https://www.sciencedirect.com/science/article/pii/S0007681314000792.\n\n\nDai, Jun, and Miklos A. Vasarhelyi. 2016. “Imagineering Audit 4.0.” Journal of Emerging Technologies in Accounting 13 (1): 1–15. https://publications.aaahq.org/jeta/article-abstract/13/1/1/9242.\n\n\nDavis, Angela K., Jeremy M. Piger, and Lisa M. Sedor. 2012. “Beyond the Numbers: Measuring the Information Content of Earnings Press Release Language.” Contemporary Accounting Research 29 (3): 845–68. https://doi.org/10.1111/j.1911-3846.2011.01130.x.\n\n\nDeloitte. 2016. “Tax Data Analytics A New Era for Tax Planning and Compliance.” https://www2.deloitte.com/content/dam/Deloitte/us/Documents/Tax/us-tax-data-analytics-a-new-era-for-tax-planning-and-compliance.pdf.\n\n\nFeldman, Ronen, Suresh Govindaraj, Joshua Livnat, and Benjamin Segal. 2010. “Management’s Tone Change, Post Earnings Announcement Drift and Accruals.” Review of Accounting Studies 15 (4): 915–53. https://doi.org/10.1007/s11142-009-9111-x.\n\n\nForbes. 2024. “The Dawn of a New Era: AI’s Revolutionary Role in Accounting.” https://www.forbes.com/sites/neilsahota/2024/04/22/the-dawn-of-a-new-era-ais-revolutionary-role-in-accounting/?sh=1ff4bc4858af.\n\n\nKrahel, John Peter, and William R. Titera. 2015. “Consequences of Big Data and Formalization on Accounting and Auditing Standards.” Accounting Horizons 29 (2): 409–22. https://publications.aaahq.org/accounting-horizons/article/29/2/409/2149.\n\n\nLehavy, Reuven, Feng Li, and Kenneth Merkley. 2011. “The Effect of Annual Report Readability on Analyst Following and the Properties of Their Earnings Forecasts.” The Accounting Review 86 (3): 1087–1115. https://publications.aaahq.org/accounting-review/article-abstract/86/3/1087/3300.\n\n\nLi, Feng. 2008. “Annual Report Readability, Current Earnings, and Earnings Persistence.” Journal of Accounting and Economics 45 (2-3): 221–47. https://www.sciencedirect.com/science/article/pii/S0165410108000141.\n\n\n———. 2010. “The Information Content of Forward‐Looking Statements in Corporate Filings—A Naïve Bayesian Machine Learning Approach.” Journal of Accounting Research 48 (5): 1049–1102. https://doi.org/10.1111/j.1475-679X.2010.00382.x.\n\n\nLi, Feng, Russell Lundholm, and Michael Minnis. 2013. “A Measure of Competition Based on 10‐K Filings.” Journal of Accounting Research 51 (2): 399–436. https://doi.org/10.1111/j.1475-679X.2012.00472.x.\n\n\nProtiviti. 2017. “Embracing Analytics in Auditing.” https://www.protiviti.com/sites/default/files/2022-06/2017-internal-audit-capabilities-and-needs-survey-protiviti.pdf.\n\n\nProvost, Foster, and Tom Fawcett. 2013. “Data Science and Its Relationship to Big Data and Data-Driven Decision Making.” Big Data 1 (1): 51–59. https://doi.org/10.1089/big.2013.1508.\n\n\nRichins, Greg, Andrea Stapleton, Theophanis C. Stratopoulos, and Christopher Wong. 2017. “Big Data Analytics: Opportunity or Threat for the Accounting Profession?” Journal of Information Systems 31 (3): 63–79. https://publications.aaahq.org/jis/article-abstract/31/3/63/1114.\n\n\nRose, Anna M., Jacob M. Rose, Kerri-Ann Sanderson, and Jay C. Thibodeau. 2017. “When Should Audit Firms Introduce Analyses of Big Data into the Audit Process?” Journal of Information Systems 31 (3): 81–99. https://publications.aaahq.org/jis/article-abstract/31/3/81/1123.\n\n\nSchneider, Gary P., Jun Dai, Diane J. Janvrin, Kemi Ajayi, and Robyn L. Raschke. 2015. “Infer, Predict, and Assure: Accounting Opportunities in Data Analytics.” Accounting Horizons 29 (3): 719–42. https://publications.aaahq.org/accounting-horizons/article-abstract/29/3/719/2262.\n\n\nSivarajah, Uthayasankar, Muhammad Mustafa Kamal, Zahir Irani, and Vishanth Weerakkody. 2017. “Critical Analysis of Big Data Challenges and Analytical Methods.” Journal of Business Research 70: 263–86. https://www.sciencedirect.com/science/article/pii/S014829631630488X.\n\n\nThe Economist. 2017. “The World’s Most Valuable Resource Is No Longer Oil, but Data.” https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data.\n\n\nVasarhelyi, Miklos A., Alexander Kogan, and Brad M. Tuttle. 2015. “Big Data in Accounting: An Overview.” Accounting Horizons 29 (2): 381–96. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/381/2184.\n\n\nVerver, John. 2015. “Six Audit Analytics Success Factors.” Internal Auditor 72 (3).\n\n\nWarren, J. Donald, Kevin C. Moffitt, and Paul Byrnes. 2015. “How Big Data Will Change Accounting.” Accounting Horizons 29 (2): 397–407. https://publications.aaahq.org/accounting-horizons/article/29/2/397/2168.\n\n\nYoon, Kyunghee, Lucas Hoogduin, and Li Zhang. 2015. “Big Data as Complementary Audit Evidence.” Accounting Horizons 29 (2): 431–38. https://publications.aaahq.org/accounting-horizons/article/29/2/431/2215.\n\n\nZhang, Juan, Xiongsheng Yang, and Deniz Appelbaum. 2015. “Toward Effective Big Data Analysis in Continuous Auditing.” Accounting Horizons 29 (2): 469–76. https://publications.aaahq.org/accounting-horizons/article/29/2/469/2160.",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "2  Foundations of Accounting Data",
    "section": "",
    "text": "2.1 Types of Accounting Data",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundations of Accounting Data</span>"
    ]
  },
  {
    "objectID": "foundations.html#data-sources-and-collection-methods",
    "href": "foundations.html#data-sources-and-collection-methods",
    "title": "2  Foundations of Accounting Data",
    "section": "2.2 Data Sources and Collection Methods",
    "text": "2.2 Data Sources and Collection Methods",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundations of Accounting Data</span>"
    ]
  },
  {
    "objectID": "foundations.html#data-quality-integrity",
    "href": "foundations.html#data-quality-integrity",
    "title": "2  Foundations of Accounting Data",
    "section": "2.3 Data Quality & Integrity",
    "text": "2.3 Data Quality & Integrity\nsummary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2\n\n\n\nplot(mtcars[1:3])\n\n\n\n\n\n\n\n\nSee Figure 2.1 for first ggplot graph\n\nlibrary(tidyverse)\nmtcars %&gt;% \n  as_tibble() %&gt;% \n  ggplot(mapping = aes(x = mpg, y = disp))+\n  geom_point()+\n  geom_smooth()+\n  labs(x = 'Miles Per Gallon (mpg)',\n       y = 'Displacement in Cubic Inches (disp)')\n\n\n\n\n\n\n\nFigure 2.1: Scatter plot and line plot of the relation between mpg and disp\n\n\n\n\n\n\nlibrary(lubridate)\n\n\nimport os\nimport sys\nsys.version\n\n'3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]'\n\n\n\nfor x in os.listdir():\n  print (x)\n\n.git\n.gitattributes\n.gitignore\n.quarto\n.README.md.swp\n.README_BASE_1260.md.swp\n.README_LOCAL_1260.md.swp\n.README_REMOTE_1260.md.swp\naccounting_analytics_book\nadvanced_analytics.qmd\nchapter1_solution.qmd\nchapter2_solution.qmd\ncover.png\ndashboard.qmd\nDATA\ndata_management.qmd\neda.qmd\nethics.qmd\nfoundations.qmd\nfoundations.rmarkdown\nfoundations_files\nfraud.qmd\nfuture.qmd\nimages\nindex.html\nindex.qmd\nintro.qmd\nmachine_learning.qmd\nnlp.qmd\noverview.html\noverview.qmd\nperformance_measurement.qmd\npredictive.qmd\nprescriptive.qmd\npython_basics.qmd\npython_version_used.txt\nREADME.html\nREADME.md\nREADME_files\nreferences.bib\nreferences.qmd\nrequirements.txt\nr_basics.qmd\nsite_libs\nvisualization.html\nvisualization.qmd\nvisualization_files\n_book\n_quarto.yml",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundations of Accounting Data</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\n\n     In descriptive statistics, we summarize the data using different metrics such as mean, median, standard deviation, minimum value, maximum value, and percentile. Descriptive statisics is also called summary statistics.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#data-collection-importing",
    "href": "eda.html#data-collection-importing",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.2 Data Collection & Importing",
    "text": "3.2 Data Collection & Importing",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#data-cleaning",
    "href": "eda.html#data-cleaning",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.3 Data Cleaning",
    "text": "3.3 Data Cleaning",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#packages-for-exploratory-data-analysis-eda",
    "href": "eda.html#packages-for-exploratory-data-analysis-eda",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.4 Packages for Exploratory Data Analysis (EDA)",
    "text": "3.4 Packages for Exploratory Data Analysis (EDA)\n\n     In order to use pyjanitor, the data frame must be pandas because pyjanitor extends pandas data frame functionality.\n\n\ndplyrpandas\n\n\n\n# loading packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n\n\n\n\n# loading the package\nimport numpy as np\nimport pandas as pd\n# from pyjanitor package \n# pip install pyjanitor\nimport janitor \nfrom janitor import clean_names, remove_empty",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#importing-the-dataset",
    "href": "eda.html#importing-the-dataset",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.5 Importing the Dataset",
    "text": "3.5 Importing the Dataset\n\ndplyrpandas\n\n\n\n# importing data frame \ndf = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n\n\n\n\n# importing data frame \ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#meta-data",
    "href": "eda.html#meta-data",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.6 Meta Data",
    "text": "3.6 Meta Data\n\n     Meta data is data about the data. Before we put the data into analysis, we need to learn about our dataset. This learning invovles knowing about the number of rows, number of columns, the types of the fields, the appropriateness of those types, the missing values in the dataset and so on.\n\n\ndplyrPandas\n\n\n\nglimpse(df)\n\nRows: 14,967\nColumns: 14\n$ InvoiceNo       &lt;dbl&gt; 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396…\n$ Date            &lt;chr&gt; \"1/1/2014\", \"1/1/2014\", \"1/1/2014\", \"1/1/2014\", \"1/1/2…\n$ Country         &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United S…\n$ ProductID       &lt;dbl&gt; 2152, 2230, 2160, 2234, 2222, 2173, 2200, 2238, 2191, …\n$ Shop            &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"…\n$ Gender          &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"F…\n$ `Size (US)`     &lt;dbl&gt; 11.0, 11.5, 9.5, 9.5, 9.0, 10.5, 9.0, 10.0, 10.5, 9.0,…\n$ `Size (Europe)` &lt;chr&gt; \"44\", \"44-45\", \"42-43\", \"40\", \"39-40\", \"43-44\", \"39-40…\n$ `Size (UK)`     &lt;dbl&gt; 10.5, 11.0, 9.0, 7.5, 7.0, 10.0, 7.0, 9.5, 10.0, 7.0, …\n$ UnitPrice       &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129,…\n$ Discount        &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,…\n$ Year            &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, …\n$ Month           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ SalePrice       &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0…\n\n\n\nmap_df(df, ~sum(is.na(.))) |&gt;\n     glimpse()\n\nRows: 1\nColumns: 14\n$ InvoiceNo       &lt;int&gt; 0\n$ Date            &lt;int&gt; 0\n$ Country         &lt;int&gt; 0\n$ ProductID       &lt;int&gt; 0\n$ Shop            &lt;int&gt; 0\n$ Gender          &lt;int&gt; 0\n$ `Size (US)`     &lt;int&gt; 0\n$ `Size (Europe)` &lt;int&gt; 0\n$ `Size (UK)`     &lt;int&gt; 0\n$ UnitPrice       &lt;int&gt; 0\n$ Discount        &lt;int&gt; 0\n$ Year            &lt;int&gt; 0\n$ Month           &lt;int&gt; 0\n$ SalePrice       &lt;int&gt; 0\n\n\n\nncol(df)\n\n[1] 14\n\nnrow(df)\n\n[1] 14967\n\n\n\nhead(df)\n\n# A tibble: 6 × 14\n  InvoiceNo Date     Country  ProductID Shop  Gender `Size (US)` `Size (Europe)`\n      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;          \n1     52389 1/1/2014 United …      2152 UK2   Male          11   44             \n2     52390 1/1/2014 United …      2230 US15  Male          11.5 44-45          \n3     52391 1/1/2014 Canada        2160 CAN7  Male           9.5 42-43          \n4     52392 1/1/2014 United …      2234 US6   Female         9.5 40             \n5     52393 1/1/2014 United …      2222 UK4   Female         9   39-40          \n6     52394 1/1/2014 United …      2173 US15  Male          10.5 43-44          \n# ℹ 6 more variables: `Size (UK)` &lt;dbl&gt;, UnitPrice &lt;dbl&gt;, Discount &lt;dbl&gt;,\n#   Year &lt;dbl&gt;, Month &lt;dbl&gt;, SalePrice &lt;dbl&gt;\n\n\n\ntail(df)\n\n# A tibble: 6 × 14\n  InvoiceNo Date      Country ProductID Shop  Gender `Size (US)` `Size (Europe)`\n      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;          \n1     65772 12/31/20… United…      2168 US13  Male           8   41             \n2     65773 12/31/20… United…      2154 UK2   Male           9.5 42-43          \n3     65774 12/31/20… United…      2181 US12  Female        12   42-43          \n4     65775 12/31/20… Canada       2203 CAN6  Male          10.5 43-44          \n5     65776 12/31/20… Germany      2231 GER1  Female         9.5 40             \n6     65777 12/31/20… Germany      2156 GER1  Female         6.5 37             \n# ℹ 6 more variables: `Size (UK)` &lt;dbl&gt;, UnitPrice &lt;dbl&gt;, Discount &lt;dbl&gt;,\n#   Year &lt;dbl&gt;, Month &lt;dbl&gt;, SalePrice &lt;dbl&gt;\n\n\n\ndplyr::sample_n(df, 10)\n\n# A tibble: 10 × 14\n   InvoiceNo Date     Country ProductID Shop  Gender `Size (US)` `Size (Europe)`\n       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;          \n 1     59296 1/8/2016 United…      2180 UK4   Male           9   42             \n 2     65308 11/28/2… United…      2232 US15  Male          10.5 43-44          \n 3     60960 4/18/20… Germany      2233 GER2  Female         8   38-39          \n 4     58489 11/10/2… Canada       2171 CAN3  Female         7   37-38          \n 5     58454 11/8/20… United…      2234 UK5   Female         8.5 39             \n 6     59241 1/4/2016 United…      2184 US5   Female         8   38-39          \n 7     57752 9/18/20… Germany      2172 GER3  Female         9   39-40          \n 8     58603 11/18/2… Canada       2195 CAN2  Female         9.5 40             \n 9     59531 1/22/20… United…      2207 UK5   Male           8   41             \n10     59699 2/2/2016 Germany      2179 GER1  Male           9.5 42-43          \n# ℹ 6 more variables: `Size (UK)` &lt;dbl&gt;, UnitPrice &lt;dbl&gt;, Discount &lt;dbl&gt;,\n#   Year &lt;dbl&gt;, Month &lt;dbl&gt;, SalePrice &lt;dbl&gt;\n\n\n\n\n\ndf_pd.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   InvoiceNo      14967 non-null  int64  \n 1   Date           14967 non-null  object \n 2   Country        14967 non-null  object \n 3   ProductID      14967 non-null  int64  \n 4   Shop           14967 non-null  object \n 5   Gender         14967 non-null  object \n 6   Size (US)      14967 non-null  float64\n 7   Size (Europe)  14967 non-null  object \n 8   Size (UK)      14967 non-null  float64\n 9   UnitPrice      14967 non-null  int64  \n 10  Discount       14967 non-null  float64\n 11  Year           14967 non-null  int64  \n 12  Month          14967 non-null  int64  \n 13  SalePrice      14967 non-null  float64\ndtypes: float64(4), int64(5), object(5)\nmemory usage: 1.6+ MB\n\n\n\ndf_pd.shape\n\n(14967, 14)\n\n\n\nprint('The total number of rows and columns of the product data is \\\n {} and {} respectively.'.format(df_pd.shape[0], df_pd.shape[1]))\n\nThe total number of rows and columns of the product data is  14967 and 14 respectively.\n\n\n\nprint(f'The total number of rows and columns of the product data is \\\n {df_pd.shape[0]} and {df_pd.shape[1]} respectively.')\n\nThe total number of rows and columns of the product data is  14967 and 14 respectively.\n\n\n\ndf_pd.columns\n\nIndex(['InvoiceNo', 'Date', 'Country', 'ProductID', 'Shop', 'Gender',\n       'Size (US)', 'Size (Europe)', 'Size (UK)', 'UnitPrice', 'Discount',\n       'Year', 'Month', 'SalePrice'],\n      dtype='object')\n\n\n\ndf_pd.head()\n\n   InvoiceNo      Date         Country  ...  Year Month SalePrice\n0      52389  1/1/2014  United Kingdom  ...  2014     1     159.0\n1      52390  1/1/2014   United States  ...  2014     1     159.2\n2      52391  1/1/2014          Canada  ...  2014     1     119.2\n3      52392  1/1/2014   United States  ...  2014     1     159.0\n4      52393  1/1/2014  United Kingdom  ...  2014     1     159.0\n\n[5 rows x 14 columns]\n\n\n\ndf_pd.tail()\n\n       InvoiceNo        Date         Country  ...  Year Month SalePrice\n14962      65773  12/31/2016  United Kingdom  ...  2016    12     139.0\n14963      65774  12/31/2016   United States  ...  2016    12     149.0\n14964      65775  12/31/2016          Canada  ...  2016    12     125.3\n14965      65776  12/31/2016         Germany  ...  2016    12     199.0\n14966      65777  12/31/2016         Germany  ...  2016    12     125.1\n\n[5 rows x 14 columns]\n\n\n\ndf_pd.isna().sum()\n\nInvoiceNo        0\nDate             0\nCountry          0\nProductID        0\nShop             0\nGender           0\nSize (US)        0\nSize (Europe)    0\nSize (UK)        0\nUnitPrice        0\nDiscount         0\nYear             0\nMonth            0\nSalePrice        0\ndtype: int64\n\n\n\ndf_pd.dtypes\n\nInvoiceNo          int64\nDate              object\nCountry           object\nProductID          int64\nShop              object\nGender            object\nSize (US)        float64\nSize (Europe)     object\nSize (UK)        float64\nUnitPrice          int64\nDiscount         float64\nYear               int64\nMonth              int64\nSalePrice        float64\ndtype: object\n\n\n\ndf_pd.sample(n=10)\n\n       InvoiceNo        Date         Country  ...  Year Month SalePrice\n9651       61029   4/21/2016         Germany  ...  2016     4     189.0\n12214      63301   8/15/2016  United Kingdom  ...  2016     8     111.3\n3941       55832   4/21/2015   United States  ...  2015     4     111.3\n878        53122   4/28/2014         Germany  ...  2014     4     129.0\n2638       54629  12/17/2014   United States  ...  2014    12     118.3\n9941       61292    5/6/2016   United States  ...  2016     5     149.0\n11824      62952   7/29/2016          Canada  ...  2016     7     169.0\n8911       60375   3/13/2016         Germany  ...  2016     3     134.1\n2634       54625  12/16/2014   United States  ...  2014    12     111.2\n2896       54864   1/16/2015         Germany  ...  2015     1      64.5\n\n[10 rows x 14 columns]",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#cleaning-the-dataset",
    "href": "eda.html#cleaning-the-dataset",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.7 Cleaning the Dataset",
    "text": "3.7 Cleaning the Dataset\n\ndplyrpanads\n\n\n\n df |&gt;\n     rename_all(toupper) |&gt;\n     janitor::clean_names() |&gt;\n     rename_all(toupper) |&gt;\n     glimpse()\n\nRows: 14,967\nColumns: 14\n$ INVOICENO   &lt;dbl&gt; 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396, 52…\n$ DATE        &lt;chr&gt; \"1/1/2014\", \"1/1/2014\", \"1/1/2014\", \"1/1/2014\", \"1/1/2014\"…\n$ COUNTRY     &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United State…\n$ PRODUCTID   &lt;dbl&gt; 2152, 2230, 2160, 2234, 2222, 2173, 2200, 2238, 2191, 2237…\n$ SHOP        &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5…\n$ GENDER      &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Femal…\n$ SIZE_US     &lt;dbl&gt; 11.0, 11.5, 9.5, 9.5, 9.0, 10.5, 9.0, 10.0, 10.5, 9.0, 10.…\n$ SIZE_EUROPE &lt;chr&gt; \"44\", \"44-45\", \"42-43\", \"40\", \"39-40\", \"43-44\", \"39-40\", \"…\n$ SIZE_UK     &lt;dbl&gt; 10.5, 11.0, 9.0, 7.5, 7.0, 10.0, 7.0, 9.5, 10.0, 7.0, 9.5,…\n$ UNITPRICE   &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ SALEPRICE   &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 13…\n\n\n\ndf = df |&gt;\n     rename_all(toupper) |&gt;\n     janitor::clean_names() |&gt;\n     rename_all(toupper)\nglimpse(df)\n\nRows: 14,967\nColumns: 14\n$ INVOICENO   &lt;dbl&gt; 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396, 52…\n$ DATE        &lt;chr&gt; \"1/1/2014\", \"1/1/2014\", \"1/1/2014\", \"1/1/2014\", \"1/1/2014\"…\n$ COUNTRY     &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United State…\n$ PRODUCTID   &lt;dbl&gt; 2152, 2230, 2160, 2234, 2222, 2173, 2200, 2238, 2191, 2237…\n$ SHOP        &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5…\n$ GENDER      &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Femal…\n$ SIZE_US     &lt;dbl&gt; 11.0, 11.5, 9.5, 9.5, 9.0, 10.5, 9.0, 10.0, 10.5, 9.0, 10.…\n$ SIZE_EUROPE &lt;chr&gt; \"44\", \"44-45\", \"42-43\", \"40\", \"39-40\", \"43-44\", \"39-40\", \"…\n$ SIZE_UK     &lt;dbl&gt; 10.5, 11.0, 9.0, 7.5, 7.0, 10.0, 7.0, 9.5, 10.0, 7.0, 9.5,…\n$ UNITPRICE   &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ SALEPRICE   &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 13…\n\n\n\n\n\ndf_pd.columns.str.upper().to_list()\n\n['INVOICENO', 'DATE', 'COUNTRY', 'PRODUCTID', 'SHOP', 'GENDER', 'SIZE (US)', 'SIZE (EUROPE)', 'SIZE (UK)', 'UNITPRICE', 'DISCOUNT', 'YEAR', 'MONTH', 'SALEPRICE']\n\n\n\n(df_pd\n     .pipe(remove_empty)\n     .pipe(lambda x: x.clean_names(case_type = \"upper\"))\n     .pipe(lambda x: x.rename(columns = {'SIZE_US_': 'SIZE_US', 'SIZE_EUROPE_':\"SIZE_EUROPE\", \"SIZE_UK_\":\"SIZE_UK\"}))\n     .pipe(lambda x: x.info())\n     )\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   INVOICENO    14967 non-null  int64  \n 1   DATE         14967 non-null  object \n 2   COUNTRY      14967 non-null  object \n 3   PRODUCTID    14967 non-null  int64  \n 4   SHOP         14967 non-null  object \n 5   GENDER       14967 non-null  object \n 6   SIZE_US      14967 non-null  float64\n 7   SIZE_EUROPE  14967 non-null  object \n 8   SIZE_UK      14967 non-null  float64\n 9   UNITPRICE    14967 non-null  int64  \n 10  DISCOUNT     14967 non-null  float64\n 11  YEAR         14967 non-null  int64  \n 12  MONTH        14967 non-null  int64  \n 13  SALEPRICE    14967 non-null  float64\ndtypes: float64(4), int64(5), object(5)\nmemory usage: 1.6+ MB\n\n\n\n# Changing the names of the columns to uppercase\ndf_pd.rename(columns = str.upper, inplace = True)\ndf_pd.columns\n\nIndex(['INVOICENO', 'DATE', 'COUNTRY', 'PRODUCTID', 'SHOP', 'GENDER',\n       'SIZE (US)', 'SIZE (EUROPE)', 'SIZE (UK)', 'UNITPRICE', 'DISCOUNT',\n       'YEAR', 'MONTH', 'SALEPRICE'],\n      dtype='object')\n\n\n\nnew_column = df_pd.columns \\\n .str.replace(\"(\", '').str.replace(\")\", \"\") \\\n .str.replace(' ','_') # Cleaning the names of the variables\nnew_column\n\nIndex(['INVOICENO', 'DATE', 'COUNTRY', 'PRODUCTID', 'SHOP', 'GENDER',\n       'SIZE_US', 'SIZE_EUROPE', 'SIZE_UK', 'UNITPRICE', 'DISCOUNT', 'YEAR',\n       'MONTH', 'SALEPRICE'],\n      dtype='object')\n\n\n\ndf_pd.columns = new_column\ndf_pd.columns\n\nIndex(['INVOICENO', 'DATE', 'COUNTRY', 'PRODUCTID', 'SHOP', 'GENDER',\n       'SIZE_US', 'SIZE_EUROPE', 'SIZE_UK', 'UNITPRICE', 'DISCOUNT', 'YEAR',\n       'MONTH', 'SALEPRICE'],\n      dtype='object')\n\ndf_pd.rename(columns=str.upper, inplace = True)\ndf_pd.columns \n\nIndex(['INVOICENO', 'DATE', 'COUNTRY', 'PRODUCTID', 'SHOP', 'GENDER',\n       'SIZE_US', 'SIZE_EUROPE', 'SIZE_UK', 'UNITPRICE', 'DISCOUNT', 'YEAR',\n       'MONTH', 'SALEPRICE'],\n      dtype='object')\n\n\n\n\n\n\n3.7.1 Changing the Types of Variables\n\ndplyrpandas\n\n\n\ndf |&gt;\n    mutate (DATE = lubridate::mdy(DATE)) |&gt;\n    glimpse()\n\nRows: 14,967\nColumns: 14\n$ INVOICENO   &lt;dbl&gt; 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396, 52…\n$ DATE        &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United State…\n$ PRODUCTID   &lt;dbl&gt; 2152, 2230, 2160, 2234, 2222, 2173, 2200, 2238, 2191, 2237…\n$ SHOP        &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5…\n$ GENDER      &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Femal…\n$ SIZE_US     &lt;dbl&gt; 11.0, 11.5, 9.5, 9.5, 9.0, 10.5, 9.0, 10.0, 10.5, 9.0, 10.…\n$ SIZE_EUROPE &lt;chr&gt; \"44\", \"44-45\", \"42-43\", \"40\", \"39-40\", \"43-44\", \"39-40\", \"…\n$ SIZE_UK     &lt;dbl&gt; 10.5, 11.0, 9.0, 7.5, 7.0, 10.0, 7.0, 9.5, 10.0, 7.0, 9.5,…\n$ UNITPRICE   &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ SALEPRICE   &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 13…\n\n\n     From the above, it is now evident the the type of the DATE variable now is date.\n\ndf |&gt;\n    mutate (DATE = lubridate::mdy(DATE)) |&gt;\n    mutate (PRODUCTID = as.character(PRODUCTID)) |&gt;\n    glimpse()\n\nRows: 14,967\nColumns: 14\n$ INVOICENO   &lt;dbl&gt; 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396, 52…\n$ DATE        &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United State…\n$ PRODUCTID   &lt;chr&gt; \"2152\", \"2230\", \"2160\", \"2234\", \"2222\", \"2173\", \"2200\", \"2…\n$ SHOP        &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5…\n$ GENDER      &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Femal…\n$ SIZE_US     &lt;dbl&gt; 11.0, 11.5, 9.5, 9.5, 9.0, 10.5, 9.0, 10.0, 10.5, 9.0, 10.…\n$ SIZE_EUROPE &lt;chr&gt; \"44\", \"44-45\", \"42-43\", \"40\", \"39-40\", \"43-44\", \"39-40\", \"…\n$ SIZE_UK     &lt;dbl&gt; 10.5, 11.0, 9.0, 7.5, 7.0, 10.0, 7.0, 9.5, 10.0, 7.0, 9.5,…\n$ UNITPRICE   &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ SALEPRICE   &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 13…\n\n\n     From the above, it is now evident the the type of the DATE and PRODUCTID variable now is date (date) and character (chr) respectively. We can now incorparte the changes into the data frame.\n\ndf = df |&gt;\n    mutate (DATE = lubridate::mdy(DATE)) |&gt;\n    mutate (PRODUCTID = as.character(PRODUCTID)) \nglimpse(df)\n\nRows: 14,967\nColumns: 14\n$ INVOICENO   &lt;dbl&gt; 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396, 52…\n$ DATE        &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United State…\n$ PRODUCTID   &lt;chr&gt; \"2152\", \"2230\", \"2160\", \"2234\", \"2222\", \"2173\", \"2200\", \"2…\n$ SHOP        &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5…\n$ GENDER      &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Femal…\n$ SIZE_US     &lt;dbl&gt; 11.0, 11.5, 9.5, 9.5, 9.0, 10.5, 9.0, 10.0, 10.5, 9.0, 10.…\n$ SIZE_EUROPE &lt;chr&gt; \"44\", \"44-45\", \"42-43\", \"40\", \"39-40\", \"43-44\", \"39-40\", \"…\n$ SIZE_UK     &lt;dbl&gt; 10.5, 11.0, 9.0, 7.5, 7.0, 10.0, 7.0, 9.5, 10.0, 7.0, 9.5,…\n$ UNITPRICE   &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ SALEPRICE   &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 13…\n\n\n\n\n\n(\n    df_pd\n    .pipe(lambda x: x.assign(DATE = pd.to_datetime(x['DATE'])))\n    .pipe(lambda x: x.info())\n)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   INVOICENO    14967 non-null  int64         \n 1   DATE         14967 non-null  datetime64[ns]\n 2   COUNTRY      14967 non-null  object        \n 3   PRODUCTID    14967 non-null  int64         \n 4   SHOP         14967 non-null  object        \n 5   GENDER       14967 non-null  object        \n 6   SIZE_US      14967 non-null  float64       \n 7   SIZE_EUROPE  14967 non-null  object        \n 8   SIZE_UK      14967 non-null  float64       \n 9   UNITPRICE    14967 non-null  int64         \n 10  DISCOUNT     14967 non-null  float64       \n 11  YEAR         14967 non-null  int64         \n 12  MONTH        14967 non-null  int64         \n 13  SALEPRICE    14967 non-null  float64       \ndtypes: datetime64[ns](1), float64(4), int64(5), object(4)\nmemory usage: 1.6+ MB\n\n\n\n# converting integer to object\ndf_pd.INVOICENO = df_pd.INVOICENO.astype(str)\ndf_pd[['MONTH', 'PRODUCTID']] = df_pd[['MONTH', 'PRODUCTID']].astype(str)\ndf_pd.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   INVOICENO    14967 non-null  object \n 1   DATE         14967 non-null  object \n 2   COUNTRY      14967 non-null  object \n 3   PRODUCTID    14967 non-null  object \n 4   SHOP         14967 non-null  object \n 5   GENDER       14967 non-null  object \n 6   SIZE_US      14967 non-null  float64\n 7   SIZE_EUROPE  14967 non-null  object \n 8   SIZE_UK      14967 non-null  float64\n 9   UNITPRICE    14967 non-null  int64  \n 10  DISCOUNT     14967 non-null  float64\n 11  YEAR         14967 non-null  int64  \n 12  MONTH        14967 non-null  object \n 13  SALEPRICE    14967 non-null  float64\ndtypes: float64(4), int64(2), object(8)\nmemory usage: 1.6+ MB",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#some-other-useful-functions",
    "href": "eda.html#some-other-useful-functions",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.8 Some Other Useful Functions",
    "text": "3.8 Some Other Useful Functions\n     There are some other useful functions that can be used to explore the dataset for analysis. Some of those useful functions are discussed below.\n\ndplyrpandas\n\n\n\ndf|&gt; count(YEAR)\n\n# A tibble: 3 × 2\n   YEAR     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2014  2753\n2  2015  4848\n3  2016  7366\n\n\n\ndf|&gt; count(COUNTRY)\n\n# A tibble: 4 × 2\n  COUNTRY            n\n  &lt;chr&gt;          &lt;int&gt;\n1 Canada          2952\n2 Germany         4392\n3 United Kingdom  1737\n4 United States   5886\n\n\n\ndf|&gt; distinct(COUNTRY)\n\n# A tibble: 4 × 1\n  COUNTRY       \n  &lt;chr&gt;         \n1 United Kingdom\n2 United States \n3 Canada        \n4 Germany       \n\n\n\n\n\ndf_pd['YEAR'].value_counts()\n\nYEAR\n2016    7366\n2015    4848\n2014    2753\nName: count, dtype: int64\n\n\n\ndf_pd['YEAR'].unique()\n\narray([2014, 2015, 2016], dtype=int64)",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#six-verbs-for-eda",
    "href": "eda.html#six-verbs-for-eda",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.9 Six Verbs for EDA",
    "text": "3.9 Six Verbs for EDA\n     Table 3.1 shows the comparable functions in both dplyr and pandas packages. These functions are very much important to perform exploratory data analysis in both R and Python. group_by (groupby in pandas) and summarize ()1 (agg () in pandas) are often used together; therefore, they are in the same group in Table 3.1.\n\n\n\n\nTable 3.1: Tidyverse and Pandas Equivalent Functions\n\n\n\n\n\n\n\nVerb Number\ntidyverse\npandas\n\n\n\n\n1\nfilter ()\nquery () or loc () or iloc ()\n\n\n2\narrange ()\nsort_values ()\n\n\n3\nselect ()\nfilter () or loc ()\n\n\n4\nrename ()\nrename ()\n\n\n5\nmutate ()\nassign ()\n\n\n6\ngroup_by ()\ngroupby ()\n\n\n6\nsummarize ()\nagg ()\n\n\n\n\n\n\n\n\n\n\n\n\n3.9.1 1st Verb - filter () Function\n     Filter functions are used to subset a data frame based on rows, meaning that retaining rows that satisfy given conditions. Filtering rows is also called slicing2 becasue we obtain a set of elements by filtering.\n\ndplyrpandas\n\n\n\ndf |&gt; filter (YEAR == \"2015\")\n\n# A tibble: 4,848 × 14\n   INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     54725 2015-01-01 United States 2187      US7   Male       9.5 42-43      \n 2     54726 2015-01-01 United States 2174      US3   Male       8.5 41-42      \n 3     54727 2015-01-01 United States 2240      US11  Male       9   42         \n 4     54728 2015-01-01 Germany       2220      GER2  Male      10   43         \n 5     54729 2015-01-01 United Kingd… 2199      UK5   Male       9.5 42-43      \n 6     54730 2015-01-01 Canada        2169      CAN7  Female     7   37-38      \n 7     54731 2015-01-01 Germany       2188      GER1  Female     9.5 40         \n 8     54732 2015-01-02 United Kingd… 2155      UK5   Female    10   40-41      \n 9     54733 2015-01-02 United States 2173      US5   Female     9   39-40      \n10     54734 2015-01-02 Germany       2222      GER3  Female     7.5 38         \n# ℹ 4,838 more rows\n# ℹ 6 more variables: SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;,\n#   YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\ndf |&gt; filter (COUNTRY %in% c(\"United States\", \"Canada\"))\n\n# A tibble: 8,838 × 14\n   INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     52390 2014-01-01 United States 2230      US15  Male      11.5 44-45      \n 2     52391 2014-01-01 Canada        2160      CAN7  Male       9.5 42-43      \n 3     52392 2014-01-01 United States 2234      US6   Female     9.5 40         \n 4     52394 2014-01-01 United States 2173      US15  Male      10.5 43-44      \n 5     52396 2014-01-02 Canada        2238      CAN5  Male      10   43         \n 6     52397 2014-01-02 United States 2191      US13  Male      10.5 43-44      \n 7     52399 2014-01-02 United States 2197      US1   Male      10   43         \n 8     52399 2014-01-02 United States 2213      US11  Female     9.5 40         \n 9     52399 2014-01-02 United States 2206      US2   Female     9.5 40         \n10     52400 2014-01-02 United States 2152      US15  Male       8   41         \n# ℹ 8,828 more rows\n# ℹ 6 more variables: SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;,\n#   YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\ndf |&gt; filter (COUNTRY == \"United States\", YEAR == \"2016\")\n\n# A tibble: 2,935 × 14\n   INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     59206 2016-01-02 United States 2186      US13  Female     8   38-39      \n 2     59209 2016-01-02 United States 2193      US14  Female     9   39-40      \n 3     59213 2016-01-02 United States 2228      US13  Male       9.5 42-43      \n 4     59214 2016-01-02 United States 2177      US12  Female    10.5 41         \n 5     59214 2016-01-02 United States 2236      US6   Male       8.5 41-42      \n 6     59219 2016-01-03 United States 2188      US14  Female     9.5 40         \n 7     59221 2016-01-03 United States 2178      US13  Female     8   38-39      \n 8     59223 2016-01-03 United States 2158      US3   Male       8   41         \n 9     59225 2016-01-03 United States 2236      US13  Male       8   41         \n10     59226 2016-01-03 United States 2207      US14  Male      14   47         \n# ℹ 2,925 more rows\n# ℹ 6 more variables: SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;,\n#   YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\ndf |&gt; filter (COUNTRY == \"United States\", YEAR %in% c(\"2015\",\"2016\"))\n\n# A tibble: 4,859 × 14\n   INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     54725 2015-01-01 United States 2187      US7   Male       9.5 42-43      \n 2     54726 2015-01-01 United States 2174      US3   Male       8.5 41-42      \n 3     54727 2015-01-01 United States 2240      US11  Male       9   42         \n 4     54733 2015-01-02 United States 2173      US5   Female     9   39-40      \n 5     54738 2015-01-02 United States 2226      US3   Male      10   43         \n 6     54739 2015-01-02 United States 2199      US11  Male       9.5 42-43      \n 7     54742 2015-01-03 United States 2209      US6   Male      10   43         \n 8     54743 2015-01-03 United States 2238      US15  Female     7.5 38         \n 9     54745 2015-01-04 United States 2214      US12  Male      10.5 43-44      \n10     54749 2015-01-04 United States 2162      US15  Male       9.5 42-43      \n# ℹ 4,849 more rows\n# ℹ 6 more variables: SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;,\n#   YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\ndf |&gt; filter (COUNTRY %in% c(\"United States\", \"Canada\"), YEAR == \"2014\")\n\n# A tibble: 1,649 × 14\n   INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     52390 2014-01-01 United States 2230      US15  Male      11.5 44-45      \n 2     52391 2014-01-01 Canada        2160      CAN7  Male       9.5 42-43      \n 3     52392 2014-01-01 United States 2234      US6   Female     9.5 40         \n 4     52394 2014-01-01 United States 2173      US15  Male      10.5 43-44      \n 5     52396 2014-01-02 Canada        2238      CAN5  Male      10   43         \n 6     52397 2014-01-02 United States 2191      US13  Male      10.5 43-44      \n 7     52399 2014-01-02 United States 2197      US1   Male      10   43         \n 8     52399 2014-01-02 United States 2213      US11  Female     9.5 40         \n 9     52399 2014-01-02 United States 2206      US2   Female     9.5 40         \n10     52400 2014-01-02 United States 2152      US15  Male       8   41         \n# ℹ 1,639 more rows\n# ℹ 6 more variables: SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;,\n#   YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\n\n\ndf_pd.query(\"YEAR == 2015\")\n\n     INVOICENO        DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n2753     54725    1/1/2015   United States  ...  2015     1     179.0\n2754     54726    1/1/2015   United States  ...  2015     1     169.0\n2755     54727    1/1/2015   United States  ...  2015     1     116.1\n2756     54728    1/1/2015         Germany  ...  2015     1     129.0\n2757     54729    1/1/2015  United Kingdom  ...  2015     1     139.0\n...        ...         ...             ...  ...   ...   ...       ...\n7596     59192  12/31/2015   United States  ...  2015    12      79.5\n7597     59193  12/31/2015   United States  ...  2015    12     139.0\n7598     59194  12/31/2015         Germany  ...  2015    12     159.0\n7599     59195  12/31/2015         Germany  ...  2015    12     149.0\n7600     59196  12/31/2015   United States  ...  2015    12     179.0\n\n[4848 rows x 14 columns]\n\n\n\ndf_pd.query('COUNTRY== \"United States\" | COUNTRY == \"Canada\"')\n\n      INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390    1/1/2014  United States  ...  2014     1     159.2\n2         52391    1/1/2014         Canada  ...  2014     1     119.2\n3         52392    1/1/2014  United States  ...  2014     1     159.0\n5         52394    1/1/2014  United States  ...  2014     1     159.0\n7         52396    1/2/2014         Canada  ...  2014     1     169.0\n...         ...         ...            ...  ...   ...   ...       ...\n14959     65770  12/31/2016  United States  ...  2016    12     119.2\n14960     65771  12/31/2016  United States  ...  2016    12     189.0\n14961     65772  12/31/2016  United States  ...  2016    12     129.0\n14963     65774  12/31/2016  United States  ...  2016    12     149.0\n14964     65775  12/31/2016         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\n\n\ndf_pd.query(\"COUNTRY in ['United States', 'Canada']\")\n\n      INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390    1/1/2014  United States  ...  2014     1     159.2\n2         52391    1/1/2014         Canada  ...  2014     1     119.2\n3         52392    1/1/2014  United States  ...  2014     1     159.0\n5         52394    1/1/2014  United States  ...  2014     1     159.0\n7         52396    1/2/2014         Canada  ...  2014     1     169.0\n...         ...         ...            ...  ...   ...   ...       ...\n14959     65770  12/31/2016  United States  ...  2016    12     119.2\n14960     65771  12/31/2016  United States  ...  2016    12     189.0\n14961     65772  12/31/2016  United States  ...  2016    12     129.0\n14963     65774  12/31/2016  United States  ...  2016    12     149.0\n14964     65775  12/31/2016         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\n\n\ndf_pd.query(\"COUNTRY== 'United States' & YEAR== 2016\")\n\n      INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n7610      59206    1/2/2016  United States  ...  2016     1     132.3\n7613      59209    1/2/2016  United States  ...  2016     1     127.2\n7617      59213    1/2/2016  United States  ...  2016     1     125.3\n7618      59214    1/2/2016  United States  ...  2016     1     151.2\n7619      59214    1/2/2016  United States  ...  2016     1     151.2\n...         ...         ...            ...  ...   ...   ...       ...\n14956     65767  12/31/2016  United States  ...  2016    12     139.0\n14959     65770  12/31/2016  United States  ...  2016    12     119.2\n14960     65771  12/31/2016  United States  ...  2016    12     189.0\n14961     65772  12/31/2016  United States  ...  2016    12     129.0\n14963     65774  12/31/2016  United States  ...  2016    12     149.0\n\n[2935 rows x 14 columns]\n\n\n\ndf_pd.query(\"COUNTRY== 'United States' & YEAR in [2015,2016]\")\n\n      INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n2753      54725    1/1/2015  United States  ...  2015     1     179.0\n2754      54726    1/1/2015  United States  ...  2015     1     169.0\n2755      54727    1/1/2015  United States  ...  2015     1     116.1\n2761      54733    1/2/2015  United States  ...  2015     1     179.0\n2766      54738    1/2/2015  United States  ...  2015     1     199.0\n...         ...         ...            ...  ...   ...   ...       ...\n14956     65767  12/31/2016  United States  ...  2016    12     139.0\n14959     65770  12/31/2016  United States  ...  2016    12     119.2\n14960     65771  12/31/2016  United States  ...  2016    12     189.0\n14961     65772  12/31/2016  United States  ...  2016    12     129.0\n14963     65774  12/31/2016  United States  ...  2016    12     149.0\n\n[4859 rows x 14 columns]\n\n\n\ndf_pd[df_pd['COUNTRY'] == \"United States\"]\n\n      INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390    1/1/2014  United States  ...  2014     1     159.2\n3         52392    1/1/2014  United States  ...  2014     1     159.0\n5         52394    1/1/2014  United States  ...  2014     1     159.0\n8         52397    1/2/2014  United States  ...  2014     1     139.0\n10        52399    1/2/2014  United States  ...  2014     1     129.0\n...         ...         ...            ...  ...   ...   ...       ...\n14956     65767  12/31/2016  United States  ...  2016    12     139.0\n14959     65770  12/31/2016  United States  ...  2016    12     119.2\n14960     65771  12/31/2016  United States  ...  2016    12     189.0\n14961     65772  12/31/2016  United States  ...  2016    12     129.0\n14963     65774  12/31/2016  United States  ...  2016    12     149.0\n\n[5886 rows x 14 columns]\n\n\n\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\")]\n\n      INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390    1/1/2014  United States  ...  2014     1     159.2\n3         52392    1/1/2014  United States  ...  2014     1     159.0\n5         52394    1/1/2014  United States  ...  2014     1     159.0\n8         52397    1/2/2014  United States  ...  2014     1     139.0\n10        52399    1/2/2014  United States  ...  2014     1     129.0\n...         ...         ...            ...  ...   ...   ...       ...\n14956     65767  12/31/2016  United States  ...  2016    12     139.0\n14959     65770  12/31/2016  United States  ...  2016    12     119.2\n14960     65771  12/31/2016  United States  ...  2016    12     189.0\n14961     65772  12/31/2016  United States  ...  2016    12     129.0\n14963     65774  12/31/2016  United States  ...  2016    12     149.0\n\n[5886 rows x 14 columns]\n\n\n\ndf_pd.loc[df_pd['COUNTRY'].isin([\"United States\", \"Canada\"])]\n\n      INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390    1/1/2014  United States  ...  2014     1     159.2\n2         52391    1/1/2014         Canada  ...  2014     1     119.2\n3         52392    1/1/2014  United States  ...  2014     1     159.0\n5         52394    1/1/2014  United States  ...  2014     1     159.0\n7         52396    1/2/2014         Canada  ...  2014     1     169.0\n...         ...         ...            ...  ...   ...   ...       ...\n14959     65770  12/31/2016  United States  ...  2016    12     119.2\n14960     65771  12/31/2016  United States  ...  2016    12     189.0\n14961     65772  12/31/2016  United States  ...  2016    12     129.0\n14963     65774  12/31/2016  United States  ...  2016    12     149.0\n14964     65775  12/31/2016         Canada  ...  2016    12     125.3\n\n[8838 rows x 14 columns]\n\n\n\ndf_pd.loc[df_pd['COUNTRY']\\\n .isin([\"United States\", \"Canada\"]) &(df_pd['YEAR']==2014)]\n\n     INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1        52390    1/1/2014  United States  ...  2014     1     159.2\n2        52391    1/1/2014         Canada  ...  2014     1     119.2\n3        52392    1/1/2014  United States  ...  2014     1     159.0\n5        52394    1/1/2014  United States  ...  2014     1     159.0\n7        52396    1/2/2014         Canada  ...  2014     1     169.0\n...        ...         ...            ...  ...   ...   ...       ...\n2739     54713  12/30/2014  United States  ...  2014    12     189.0\n2745     54718  12/31/2014         Canada  ...  2014    12     151.2\n2746     54719  12/31/2014  United States  ...  2014    12     199.0\n2748     54721  12/31/2014         Canada  ...  2014    12      74.5\n2749     54722  12/31/2014  United States  ...  2014    12     118.3\n\n[1649 rows x 14 columns]\n\n\n\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\") &(df_pd [\"YEAR\"] ==2014)]\n\n     INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1        52390    1/1/2014  United States  ...  2014     1     159.2\n3        52392    1/1/2014  United States  ...  2014     1     159.0\n5        52394    1/1/2014  United States  ...  2014     1     159.0\n8        52397    1/2/2014  United States  ...  2014     1     139.0\n10       52399    1/2/2014  United States  ...  2014     1     129.0\n...        ...         ...            ...  ...   ...   ...       ...\n2731     54705  12/29/2014  United States  ...  2014    12     179.0\n2734     54708  12/30/2014  United States  ...  2014    12     159.0\n2739     54713  12/30/2014  United States  ...  2014    12     189.0\n2746     54719  12/31/2014  United States  ...  2014    12     199.0\n2749     54722  12/31/2014  United States  ...  2014    12     118.3\n\n[1027 rows x 14 columns]\n\n\n\ndf_pd.loc[df_pd['COUNTRY'] == \"United States\", :]\n\n      INVOICENO        DATE        COUNTRY  ...  YEAR MONTH SALEPRICE\n1         52390    1/1/2014  United States  ...  2014     1     159.2\n3         52392    1/1/2014  United States  ...  2014     1     159.0\n5         52394    1/1/2014  United States  ...  2014     1     159.0\n8         52397    1/2/2014  United States  ...  2014     1     139.0\n10        52399    1/2/2014  United States  ...  2014     1     129.0\n...         ...         ...            ...  ...   ...   ...       ...\n14956     65767  12/31/2016  United States  ...  2016    12     139.0\n14959     65770  12/31/2016  United States  ...  2016    12     119.2\n14960     65771  12/31/2016  United States  ...  2016    12     189.0\n14961     65772  12/31/2016  United States  ...  2016    12     129.0\n14963     65774  12/31/2016  United States  ...  2016    12     149.0\n\n[5886 rows x 14 columns]\n\n\n\ndf_pd.loc[\n    df_pd['COUNTRY']=='United States',\n    ['COUNTRY', \"UNITPRICE\", \"SALEPRICE\"]]\n\n             COUNTRY  UNITPRICE  SALEPRICE\n1      United States        199      159.2\n3      United States        159      159.0\n5      United States        159      159.0\n8      United States        139      139.0\n10     United States        129      129.0\n...              ...        ...        ...\n14956  United States        139      139.0\n14959  United States        149      119.2\n14960  United States        189      189.0\n14961  United States        129      129.0\n14963  United States        149      149.0\n\n[5886 rows x 3 columns]\n\n\n\n\n\n\n\n3.9.2 2nd Verb - arrange () Function\n     In arrange functions, we order the rows of a data frame by the values of given columns. It is like sorting or odering the data.\n\ndplyrpandas\n\n\n\ndf |&gt;\n    arrange(DATE)     \n\n# A tibble: 14,967 × 14\n   INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     52389 2014-01-01 United Kingd… 2152      UK2   Male      11   44         \n 2     52390 2014-01-01 United States 2230      US15  Male      11.5 44-45      \n 3     52391 2014-01-01 Canada        2160      CAN7  Male       9.5 42-43      \n 4     52392 2014-01-01 United States 2234      US6   Female     9.5 40         \n 5     52393 2014-01-01 United Kingd… 2222      UK4   Female     9   39-40      \n 6     52394 2014-01-01 United States 2173      US15  Male      10.5 43-44      \n 7     52395 2014-01-02 Germany       2200      GER2  Female     9   39-40      \n 8     52396 2014-01-02 Canada        2238      CAN5  Male      10   43         \n 9     52397 2014-01-02 United States 2191      US13  Male      10.5 43-44      \n10     52398 2014-01-02 United Kingd… 2237      UK1   Female     9   39-40      \n# ℹ 14,957 more rows\n# ℹ 6 more variables: SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;,\n#   YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\ndf |&gt;\n    arrange(desc(DATE))     \n\n# A tibble: 14,967 × 14\n   INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     65765 2016-12-31 Canada        2199      CAN5  Male      11   44         \n 2     65766 2016-12-31 United Kingd… 2202      UK1   Male       9.5 42-43      \n 3     65767 2016-12-31 United States 2147      US15  Male       9.5 42-43      \n 4     65768 2016-12-31 Germany       2205      GER1  Female     7.5 38         \n 5     65769 2016-12-31 Germany       2210      GER2  Male      10.5 43-44      \n 6     65770 2016-12-31 United States 2178      US13  Female     8   38-39      \n 7     65771 2016-12-31 United States 2209      US15  Male       9   42         \n 8     65772 2016-12-31 United States 2168      US13  Male       8   41         \n 9     65773 2016-12-31 United Kingd… 2154      UK2   Male       9.5 42-43      \n10     65774 2016-12-31 United States 2181      US12  Female    12   42-43      \n# ℹ 14,957 more rows\n# ℹ 6 more variables: SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;,\n#   YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\ndf |&gt;\n    arrange(MONTH, SALEPRICE)     \n\n# A tibble: 14,967 × 14\n   INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     52414 2014-01-05 Germany       2239      GER2  Female     8.5 39         \n 2     52533 2014-01-25 Canada        2151      CAN5  Female     9   39-40      \n 3     52539 2014-01-26 United Kingd… 2227      UK1   Female     7.5 38         \n 4     52548 2014-01-27 United Kingd… 2224      UK3   Male       8.5 41-42      \n 5     54734 2015-01-02 Germany       2222      GER3  Female     7.5 38         \n 6     54772 2015-01-06 Germany       2159      GER1  Male      12   45         \n 7     54864 2015-01-16 Germany       2239      GER2  Female     7.5 38         \n 8     54989 2015-01-28 Canada        2152      CAN6  Female     8   38-39      \n 9     59220 2016-01-03 Canada        2202      CAN7  Male      10.5 43-44      \n10     59242 2016-01-04 United States 2231      US9   Female     9   39-40      \n# ℹ 14,957 more rows\n# ℹ 6 more variables: SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;,\n#   YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\n\n\ndf_pd.sort_values(by =['DATE'])   \n\n      INVOICENO      DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n0         52389  1/1/2014  United Kingdom  ...  2014     1     159.0\n1         52390  1/1/2014   United States  ...  2014     1     159.2\n2         52391  1/1/2014          Canada  ...  2014     1     119.2\n3         52392  1/1/2014   United States  ...  2014     1     159.0\n4         52393  1/1/2014  United Kingdom  ...  2014     1     159.0\n...         ...       ...             ...  ...   ...   ...       ...\n12785     63807  9/9/2016         Germany  ...  2016     9     189.0\n12786     63808  9/9/2016         Germany  ...  2016     9     129.0\n12787     63809  9/9/2016   United States  ...  2016     9     179.0\n12789     63811  9/9/2016   United States  ...  2016     9     125.3\n12779     63802  9/9/2016   United States  ...  2016     9     125.1\n\n[14967 rows x 14 columns]\n\n\n\ndf_pd.sort_values(by =['DATE'], ascending = False)   \n\n      INVOICENO      DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n12778     63802  9/9/2016   United States  ...  2016     9     139.0\n12784     63806  9/9/2016         Germany  ...  2016     9     139.0\n12773     63797  9/9/2016         Germany  ...  2016     9      90.3\n12772     63796  9/9/2016         Germany  ...  2016     9     111.3\n12771     63795  9/9/2016         Germany  ...  2016     9     152.1\n...         ...       ...             ...  ...   ...   ...       ...\n5         52394  1/1/2014   United States  ...  2014     1     159.0\n4         52393  1/1/2014  United Kingdom  ...  2014     1     159.0\n3         52392  1/1/2014   United States  ...  2014     1     159.0\n2         52391  1/1/2014          Canada  ...  2014     1     119.2\n0         52389  1/1/2014  United Kingdom  ...  2014     1     159.0\n\n[14967 rows x 14 columns]\n\n\n\ndf_pd.sort_values(by =['MONTH', 'SALEPRICE'])\n\n      INVOICENO       DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n33        52414   1/5/2014         Germany  ...  2014     1      64.5\n177       52533  1/25/2014          Canada  ...  2014     1      64.5\n185       52539  1/26/2014  United Kingdom  ...  2014     1      64.5\n194       52548  1/27/2014  United Kingdom  ...  2014     1      64.5\n2762      54734   1/2/2015         Germany  ...  2015     1      64.5\n...         ...        ...             ...  ...   ...   ...       ...\n13245     64219  9/29/2016  United Kingdom  ...  2016     9     199.0\n13246     64220  9/29/2016   United States  ...  2016     9     199.0\n13248     64222  9/29/2016   United States  ...  2016     9     199.0\n13251     64224  9/29/2016         Germany  ...  2016     9     199.0\n13272     64244  9/30/2016   United States  ...  2016     9     199.0\n\n[14967 rows x 14 columns]\n\n\n\n\n\n\n\n3.9.3 3rd Verb - select () Function\n     Select functions help to select or obtain columns from the data frame. When there are a lot of columns in our dataset, select functions become very useful.\n\ndplyrpandas\n\n\n\ndf |&gt; select(DATE, UNITPRICE, DISCOUNT)   \n\n# A tibble: 14,967 × 3\n   DATE       UNITPRICE DISCOUNT\n   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 2014-01-01       159      0  \n 2 2014-01-01       199      0.2\n 3 2014-01-01       149      0.2\n 4 2014-01-01       159      0  \n 5 2014-01-01       159      0  \n 6 2014-01-01       159      0  \n 7 2014-01-02       179      0  \n 8 2014-01-02       169      0  \n 9 2014-01-02       139      0  \n10 2014-01-02       149      0  \n# ℹ 14,957 more rows\n\n\n\ndf |&gt; select(1:2, 5:8)  \n\n# A tibble: 14,967 × 6\n   INVOICENO DATE       SHOP  GENDER SIZE_US SIZE_EUROPE\n       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;      \n 1     52389 2014-01-01 UK2   Male      11   44         \n 2     52390 2014-01-01 US15  Male      11.5 44-45      \n 3     52391 2014-01-01 CAN7  Male       9.5 42-43      \n 4     52392 2014-01-01 US6   Female     9.5 40         \n 5     52393 2014-01-01 UK4   Female     9   39-40      \n 6     52394 2014-01-01 US15  Male      10.5 43-44      \n 7     52395 2014-01-02 GER2  Female     9   39-40      \n 8     52396 2014-01-02 CAN5  Male      10   43         \n 9     52397 2014-01-02 US13  Male      10.5 43-44      \n10     52398 2014-01-02 UK1   Female     9   39-40      \n# ℹ 14,957 more rows\n\n\n\ndf |&gt;\n    select(starts_with('SIZE'))\n\n# A tibble: 14,967 × 3\n   SIZE_US SIZE_EUROPE SIZE_UK\n     &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1    11   44             10.5\n 2    11.5 44-45          11  \n 3     9.5 42-43           9  \n 4     9.5 40              7.5\n 5     9   39-40           7  \n 6    10.5 43-44          10  \n 7     9   39-40           7  \n 8    10   43              9.5\n 9    10.5 43-44          10  \n10     9   39-40           7  \n# ℹ 14,957 more rows\n\n\n\ndf |&gt;\n    select(ends_with('PRICE'))\n\n# A tibble: 14,967 × 2\n   UNITPRICE SALEPRICE\n       &lt;dbl&gt;     &lt;dbl&gt;\n 1       159      159 \n 2       199      159.\n 3       149      119.\n 4       159      159 \n 5       159      159 \n 6       159      159 \n 7       179      179 \n 8       169      169 \n 9       139      139 \n10       149      149 \n# ℹ 14,957 more rows\n\n\n\ndf |&gt;\n    select(contains(\"_\"))\n\n# A tibble: 14,967 × 3\n   SIZE_US SIZE_EUROPE SIZE_UK\n     &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1    11   44             10.5\n 2    11.5 44-45          11  \n 3     9.5 42-43           9  \n 4     9.5 40              7.5\n 5     9   39-40           7  \n 6    10.5 43-44          10  \n 7     9   39-40           7  \n 8    10   43              9.5\n 9    10.5 43-44          10  \n10     9   39-40           7  \n# ℹ 14,957 more rows\n\n\n\ndf |&gt;\n    select(matches(\"SIZE\"))\n\n# A tibble: 14,967 × 3\n   SIZE_US SIZE_EUROPE SIZE_UK\n     &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1    11   44             10.5\n 2    11.5 44-45          11  \n 3     9.5 42-43           9  \n 4     9.5 40              7.5\n 5     9   39-40           7  \n 6    10.5 43-44          10  \n 7     9   39-40           7  \n 8    10   43              9.5\n 9    10.5 43-44          10  \n10     9   39-40           7  \n# ℹ 14,957 more rows\n\n\n\ndf |&gt;\n    select(matches(\"PRICE$\"))\n\n# A tibble: 14,967 × 2\n   UNITPRICE SALEPRICE\n       &lt;dbl&gt;     &lt;dbl&gt;\n 1       159      159 \n 2       199      159.\n 3       149      119.\n 4       159      159 \n 5       159      159 \n 6       159      159 \n 7       179      179 \n 8       169      169 \n 9       139      139 \n10       149      149 \n# ℹ 14,957 more rows\n\n\n\n# starts with letter S\ndf |&gt;\n    select(matches(\"^S\"))\n\n# A tibble: 14,967 × 5\n   SHOP  SIZE_US SIZE_EUROPE SIZE_UK SALEPRICE\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1 UK2      11   44             10.5      159 \n 2 US15     11.5 44-45          11        159.\n 3 CAN7      9.5 42-43           9        119.\n 4 US6       9.5 40              7.5      159 \n 5 UK4       9   39-40           7        159 \n 6 US15     10.5 43-44          10        159 \n 7 GER2      9   39-40           7        179 \n 8 CAN5     10   43              9.5      169 \n 9 US13     10.5 43-44          10        139 \n10 UK1       9   39-40           7        149 \n# ℹ 14,957 more rows\n\n\n\ndf |&gt;\n    select(where(is.character))\n\n# A tibble: 14,967 × 5\n   COUNTRY        PRODUCTID SHOP  GENDER SIZE_EUROPE\n   &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      \n 1 United Kingdom 2152      UK2   Male   44         \n 2 United States  2230      US15  Male   44-45      \n 3 Canada         2160      CAN7  Male   42-43      \n 4 United States  2234      US6   Female 40         \n 5 United Kingdom 2222      UK4   Female 39-40      \n 6 United States  2173      US15  Male   43-44      \n 7 Germany        2200      GER2  Female 39-40      \n 8 Canada         2238      CAN5  Male   43         \n 9 United States  2191      US13  Male   43-44      \n10 United Kingdom 2237      UK1   Female 39-40      \n# ℹ 14,957 more rows\n\n\n\ndf |&gt;\n    select(where(is.numeric))\n\n# A tibble: 14,967 × 8\n   INVOICENO SIZE_US SIZE_UK UNITPRICE DISCOUNT  YEAR MONTH SALEPRICE\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1     52389    11      10.5       159      0    2014     1      159 \n 2     52390    11.5    11         199      0.2  2014     1      159.\n 3     52391     9.5     9         149      0.2  2014     1      119.\n 4     52392     9.5     7.5       159      0    2014     1      159 \n 5     52393     9       7         159      0    2014     1      159 \n 6     52394    10.5    10         159      0    2014     1      159 \n 7     52395     9       7         179      0    2014     1      179 \n 8     52396    10       9.5       169      0    2014     1      169 \n 9     52397    10.5    10         139      0    2014     1      139 \n10     52398     9       7         149      0    2014     1      149 \n# ℹ 14,957 more rows\n\n\n\ndf |&gt;\n    select(MONTH, YEAR, everything())\n\n# A tibble: 14,967 × 14\n   MONTH  YEAR INVOICENO DATE       COUNTRY       PRODUCTID SHOP  GENDER SIZE_US\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1  2014     52389 2014-01-01 United Kingd… 2152      UK2   Male      11  \n 2     1  2014     52390 2014-01-01 United States 2230      US15  Male      11.5\n 3     1  2014     52391 2014-01-01 Canada        2160      CAN7  Male       9.5\n 4     1  2014     52392 2014-01-01 United States 2234      US6   Female     9.5\n 5     1  2014     52393 2014-01-01 United Kingd… 2222      UK4   Female     9  \n 6     1  2014     52394 2014-01-01 United States 2173      US15  Male      10.5\n 7     1  2014     52395 2014-01-02 Germany       2200      GER2  Female     9  \n 8     1  2014     52396 2014-01-02 Canada        2238      CAN5  Male      10  \n 9     1  2014     52397 2014-01-02 United States 2191      US13  Male      10.5\n10     1  2014     52398 2014-01-02 United Kingd… 2237      UK1   Female     9  \n# ℹ 14,957 more rows\n# ℹ 5 more variables: SIZE_EUROPE &lt;chr&gt;, SIZE_UK &lt;dbl&gt;, UNITPRICE &lt;dbl&gt;,\n#   DISCOUNT &lt;dbl&gt;, SALEPRICE &lt;dbl&gt;\n\n\n\n# any_of () vs all_of ()\ndf |&gt;\n    select(any_of(c(\"PRICE\", \"SIZE\")))\n\n# A tibble: 14,967 × 0\n\n\n\n# Dropping columns \ndf |&gt;\n    select(-DATE)\n\n# A tibble: 14,967 × 13\n   INVOICENO COUNTRY        PRODUCTID SHOP  GENDER SIZE_US SIZE_EUROPE SIZE_UK\n       &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1     52389 United Kingdom 2152      UK2   Male      11   44             10.5\n 2     52390 United States  2230      US15  Male      11.5 44-45          11  \n 3     52391 Canada         2160      CAN7  Male       9.5 42-43           9  \n 4     52392 United States  2234      US6   Female     9.5 40              7.5\n 5     52393 United Kingdom 2222      UK4   Female     9   39-40           7  \n 6     52394 United States  2173      US15  Male      10.5 43-44          10  \n 7     52395 Germany        2200      GER2  Female     9   39-40           7  \n 8     52396 Canada         2238      CAN5  Male      10   43              9.5\n 9     52397 United States  2191      US13  Male      10.5 43-44          10  \n10     52398 United Kingdom 2237      UK1   Female     9   39-40           7  \n# ℹ 14,957 more rows\n# ℹ 5 more variables: UNITPRICE &lt;dbl&gt;, DISCOUNT &lt;dbl&gt;, YEAR &lt;dbl&gt;, MONTH &lt;dbl&gt;,\n#   SALEPRICE &lt;dbl&gt;\n\n\n\n\n\ndf_pd['DATE']   \n\n0          1/1/2014\n1          1/1/2014\n2          1/1/2014\n3          1/1/2014\n4          1/1/2014\n            ...    \n14962    12/31/2016\n14963    12/31/2016\n14964    12/31/2016\n14965    12/31/2016\n14966    12/31/2016\nName: DATE, Length: 14967, dtype: object\n\n\n\ndf_pd[['DATE', 'UNITPRICE']]   \n\n             DATE  UNITPRICE\n0        1/1/2014        159\n1        1/1/2014        199\n2        1/1/2014        149\n3        1/1/2014        159\n4        1/1/2014        159\n...           ...        ...\n14962  12/31/2016        139\n14963  12/31/2016        149\n14964  12/31/2016        179\n14965  12/31/2016        199\n14966  12/31/2016        139\n\n[14967 rows x 2 columns]\n\n\n\ndf_pd.loc[:,['DATE', 'UNITPRICE']]   \n\n             DATE  UNITPRICE\n0        1/1/2014        159\n1        1/1/2014        199\n2        1/1/2014        149\n3        1/1/2014        159\n4        1/1/2014        159\n...           ...        ...\n14962  12/31/2016        139\n14963  12/31/2016        149\n14964  12/31/2016        179\n14965  12/31/2016        199\n14966  12/31/2016        139\n\n[14967 rows x 2 columns]\n\n\n\ndf_pd.iloc[:,5:8]\n\n       GENDER  SIZE_US SIZE_EUROPE\n0        Male     11.0          44\n1        Male     11.5       44-45\n2        Male      9.5       42-43\n3      Female      9.5          40\n4      Female      9.0       39-40\n...       ...      ...         ...\n14962    Male      9.5       42-43\n14963  Female     12.0       42-43\n14964    Male     10.5       43-44\n14965  Female      9.5          40\n14966  Female      6.5          37\n\n[14967 rows x 3 columns]\n\n\n\ndf_pd.iloc[:,[3,5,8]]\n\n      PRODUCTID  GENDER  SIZE_UK\n0          2152    Male     10.5\n1          2230    Male     11.0\n2          2160    Male      9.0\n3          2234  Female      7.5\n4          2222  Female      7.0\n...         ...     ...      ...\n14962      2154    Male      9.0\n14963      2181  Female     10.0\n14964      2203    Male     10.0\n14965      2231  Female      7.5\n14966      2156  Female      4.5\n\n[14967 rows x 3 columns]\n\n\n\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n\n       YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0      2014      159.0       0.0        159\n1      2014      159.2       0.2        199\n2      2014      119.2       0.2        149\n3      2014      159.0       0.0        159\n4      2014      159.0       0.0        159\n...     ...        ...       ...        ...\n14962  2016      139.0       0.0        139\n14963  2016      149.0       0.0        149\n14964  2016      125.3       0.3        179\n14965  2016      199.0       0.0        199\n14966  2016      125.1       0.1        139\n\n[14967 rows x 4 columns]\n\n\n\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n\n       YEAR  SALEPRICE  DISCOUNT  UNITPRICE\n0      2014      159.0       0.0        159\n1      2014      159.2       0.2        199\n2      2014      119.2       0.2        149\n3      2014      159.0       0.0        159\n4      2014      159.0       0.0        159\n...     ...        ...       ...        ...\n14962  2016      139.0       0.0        139\n14963  2016      149.0       0.0        149\n14964  2016      125.3       0.3        179\n14965  2016      199.0       0.0        199\n14966  2016      125.1       0.1        139\n\n[14967 rows x 4 columns]\n\n\n\n #RegularExpression(Regex)\ndf_pd.filter(regex =\"PRICE$\") #Ends with Price\n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\n\n\ndf_pd.filter(regex =\"ˆSIZE\") #Starts with SIZE\n\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n\n[14967 rows x 0 columns]\n\n\n\ndf_pd.filter(regex =\"PRICE\") #Contains the word Price\n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\n\n\ndf_pd.select_dtypes('object')\n\n      INVOICENO        DATE         COUNTRY  ...  GENDER SIZE_EUROPE MONTH\n0         52389    1/1/2014  United Kingdom  ...    Male          44     1\n1         52390    1/1/2014   United States  ...    Male       44-45     1\n2         52391    1/1/2014          Canada  ...    Male       42-43     1\n3         52392    1/1/2014   United States  ...  Female          40     1\n4         52393    1/1/2014  United Kingdom  ...  Female       39-40     1\n...         ...         ...             ...  ...     ...         ...   ...\n14962     65773  12/31/2016  United Kingdom  ...    Male       42-43    12\n14963     65774  12/31/2016   United States  ...  Female       42-43    12\n14964     65775  12/31/2016          Canada  ...    Male       43-44    12\n14965     65776  12/31/2016         Germany  ...  Female          40    12\n14966     65777  12/31/2016         Germany  ...  Female          37    12\n\n[14967 rows x 8 columns]\n\n\n\ndf_pd.select_dtypes('int')\n\n       UNITPRICE  YEAR\n0            159  2014\n1            199  2014\n2            149  2014\n3            159  2014\n4            159  2014\n...          ...   ...\n14962        139  2016\n14963        149  2016\n14964        179  2016\n14965        199  2016\n14966        139  2016\n\n[14967 rows x 2 columns]\n\n\n\ndf_pd.loc[:,df_pd.columns.str.startswith('SIZE')]\n\n       SIZE_US SIZE_EUROPE  SIZE_UK\n0         11.0          44     10.5\n1         11.5       44-45     11.0\n2          9.5       42-43      9.0\n3          9.5          40      7.5\n4          9.0       39-40      7.0\n...        ...         ...      ...\n14962      9.5       42-43      9.0\n14963     12.0       42-43     10.0\n14964     10.5       43-44     10.0\n14965      9.5          40      7.5\n14966      6.5          37      4.5\n\n[14967 rows x 3 columns]\n\n\n\ndf_pd.loc[:,df_pd.columns.str.contains('PRICE')]\n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\n\n\ndf_pd.loc[:,df_pd.columns.str.endswith('PRICE')]\n\n       UNITPRICE  SALEPRICE\n0            159      159.0\n1            199      159.2\n2            149      119.2\n3            159      159.0\n4            159      159.0\n...          ...        ...\n14962        139      139.0\n14963        149      149.0\n14964        179      125.3\n14965        199      199.0\n14966        139      125.1\n\n[14967 rows x 2 columns]\n\n\n\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\n\n      INVOICENO        DATE         COUNTRY  ...  YEAR MONTH SALEPRICE\n0         52389    1/1/2014  United Kingdom  ...  2014     1     159.0\n1         52390    1/1/2014   United States  ...  2014     1     159.2\n2         52391    1/1/2014          Canada  ...  2014     1     119.2\n3         52392    1/1/2014   United States  ...  2014     1     159.0\n4         52393    1/1/2014  United Kingdom  ...  2014     1     159.0\n...         ...         ...             ...  ...   ...   ...       ...\n14962     65773  12/31/2016  United Kingdom  ...  2016    12     139.0\n14963     65774  12/31/2016   United States  ...  2016    12     149.0\n14964     65775  12/31/2016          Canada  ...  2016    12     125.3\n14965     65776  12/31/2016         Germany  ...  2016    12     199.0\n14966     65777  12/31/2016         Germany  ...  2016    12     125.1\n\n[14967 rows x 12 columns]\n\n\n\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\\\n    .pipe(lambda x: x.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 12 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   INVOICENO  14967 non-null  object \n 1   DATE       14967 non-null  object \n 2   COUNTRY    14967 non-null  object \n 3   PRODUCTID  14967 non-null  object \n 4   SHOP       14967 non-null  object \n 5   GENDER     14967 non-null  object \n 6   SIZE_US    14967 non-null  float64\n 7   UNITPRICE  14967 non-null  int64  \n 8   DISCOUNT   14967 non-null  float64\n 9   YEAR       14967 non-null  int64  \n 10  MONTH      14967 non-null  object \n 11  SALEPRICE  14967 non-null  float64\ndtypes: float64(3), int64(2), object(7)\nmemory usage: 1.4+ MB\n\n\n\n# Rearranging columns \n# Sorting Alphabetically\ndf_pd.reindex(sorted(df_pd.columns), axis =1)\n\n              COUNTRY        DATE  DISCOUNT  ... SIZE_US UNITPRICE  YEAR\n0      United Kingdom    1/1/2014       0.0  ...    11.0       159  2014\n1       United States    1/1/2014       0.2  ...    11.5       199  2014\n2              Canada    1/1/2014       0.2  ...     9.5       149  2014\n3       United States    1/1/2014       0.0  ...     9.5       159  2014\n4      United Kingdom    1/1/2014       0.0  ...     9.0       159  2014\n...               ...         ...       ...  ...     ...       ...   ...\n14962  United Kingdom  12/31/2016       0.0  ...     9.5       139  2016\n14963   United States  12/31/2016       0.0  ...    12.0       149  2016\n14964          Canada  12/31/2016       0.3  ...    10.5       179  2016\n14965         Germany  12/31/2016       0.0  ...     9.5       199  2016\n14966         Germany  12/31/2016       0.1  ...     6.5       139  2016\n\n[14967 rows x 14 columns]\n\n\n\n# Rearranging columns \n# Sorting As You Want (ASY)\ncol_first = ['YEAR','MONTH']\ncol_rest = df_pd.columns.difference(col_first, sort=False).to_list()\ndf_pd2 = df_pd [col_first +col_rest]\ndf_pd2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   YEAR         14967 non-null  int64  \n 1   MONTH        14967 non-null  object \n 2   INVOICENO    14967 non-null  object \n 3   DATE         14967 non-null  object \n 4   COUNTRY      14967 non-null  object \n 5   PRODUCTID    14967 non-null  object \n 6   SHOP         14967 non-null  object \n 7   GENDER       14967 non-null  object \n 8   SIZE_US      14967 non-null  float64\n 9   SIZE_EUROPE  14967 non-null  object \n 10  SIZE_UK      14967 non-null  float64\n 11  UNITPRICE    14967 non-null  int64  \n 12  DISCOUNT     14967 non-null  float64\n 13  SALEPRICE    14967 non-null  float64\ndtypes: float64(4), int64(2), object(8)\nmemory usage: 1.6+ MB\n\n\n\n\n\n\n\n3.9.4 4th Verb - rename () Function\n\ndplyrpandas\n\n\n\ndf |&gt;\n    rename(INVOICE = INVOICENO,\n    PRODUCT = PRODUCTID) |&gt;\n    glimpse()\n\nRows: 14,967\nColumns: 14\n$ INVOICE     &lt;dbl&gt; 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396, 52…\n$ DATE        &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United State…\n$ PRODUCT     &lt;chr&gt; \"2152\", \"2230\", \"2160\", \"2234\", \"2222\", \"2173\", \"2200\", \"2…\n$ SHOP        &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5…\n$ GENDER      &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Femal…\n$ SIZE_US     &lt;dbl&gt; 11.0, 11.5, 9.5, 9.5, 9.0, 10.5, 9.0, 10.0, 10.5, 9.0, 10.…\n$ SIZE_EUROPE &lt;chr&gt; \"44\", \"44-45\", \"42-43\", \"40\", \"39-40\", \"43-44\", \"39-40\", \"…\n$ SIZE_UK     &lt;dbl&gt; 10.5, 11.0, 9.0, 7.5, 7.0, 10.0, 7.0, 9.5, 10.0, 7.0, 9.5,…\n$ UNITPRICE   &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ SALEPRICE   &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 13…\n\n\n\n\n\n(df_pd.rename(columns = {\"PRODUCTID\": \"PRODUCT\", \"INVOICENO\": \"INVOICE\"})\n     .pipe(lambda x: x.info())\n)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   INVOICE      14967 non-null  object \n 1   DATE         14967 non-null  object \n 2   COUNTRY      14967 non-null  object \n 3   PRODUCT      14967 non-null  object \n 4   SHOP         14967 non-null  object \n 5   GENDER       14967 non-null  object \n 6   SIZE_US      14967 non-null  float64\n 7   SIZE_EUROPE  14967 non-null  object \n 8   SIZE_UK      14967 non-null  float64\n 9   UNITPRICE    14967 non-null  int64  \n 10  DISCOUNT     14967 non-null  float64\n 11  YEAR         14967 non-null  int64  \n 12  MONTH        14967 non-null  object \n 13  SALEPRICE    14967 non-null  float64\ndtypes: float64(4), int64(2), object(8)\nmemory usage: 1.6+ MB\n\n\n\n\n\n\n\n3.9.5 5th Verb - mutate () Function\n\ndplyrpandas\n\n\n\ndf |&gt;\n    mutate(NECOLUMN = 5,\n    SALESPRICE2 = UNITPRICE*(1-DISCOUNT)) |&gt;\n    glimpse()\n\nRows: 14,967\nColumns: 16\n$ INVOICENO   &lt;dbl&gt; 52389, 52390, 52391, 52392, 52393, 52394, 52395, 52396, 52…\n$ DATE        &lt;date&gt; 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-01, 2014-01-0…\n$ COUNTRY     &lt;chr&gt; \"United Kingdom\", \"United States\", \"Canada\", \"United State…\n$ PRODUCTID   &lt;chr&gt; \"2152\", \"2230\", \"2160\", \"2234\", \"2222\", \"2173\", \"2200\", \"2…\n$ SHOP        &lt;chr&gt; \"UK2\", \"US15\", \"CAN7\", \"US6\", \"UK4\", \"US15\", \"GER2\", \"CAN5…\n$ GENDER      &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Femal…\n$ SIZE_US     &lt;dbl&gt; 11.0, 11.5, 9.5, 9.5, 9.0, 10.5, 9.0, 10.0, 10.5, 9.0, 10.…\n$ SIZE_EUROPE &lt;chr&gt; \"44\", \"44-45\", \"42-43\", \"40\", \"39-40\", \"43-44\", \"39-40\", \"…\n$ SIZE_UK     &lt;dbl&gt; 10.5, 11.0, 9.0, 7.5, 7.0, 10.0, 7.0, 9.5, 10.0, 7.0, 9.5,…\n$ UNITPRICE   &lt;dbl&gt; 159, 199, 149, 159, 159, 159, 179, 169, 139, 149, 129, 169…\n$ DISCOUNT    &lt;dbl&gt; 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1…\n$ YEAR        &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014…\n$ MONTH       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ SALEPRICE   &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 13…\n$ NECOLUMN    &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5…\n$ SALESPRICE2 &lt;dbl&gt; 159.0, 159.2, 119.2, 159.0, 159.0, 159.0, 179.0, 169.0, 13…\n\n\n\n\n\ndf_pd['NEWCOLUMN'] = 5\ndf_pd.info()     \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   INVOICENO    14967 non-null  object \n 1   DATE         14967 non-null  object \n 2   COUNTRY      14967 non-null  object \n 3   PRODUCTID    14967 non-null  object \n 4   SHOP         14967 non-null  object \n 5   GENDER       14967 non-null  object \n 6   SIZE_US      14967 non-null  float64\n 7   SIZE_EUROPE  14967 non-null  object \n 8   SIZE_UK      14967 non-null  float64\n 9   UNITPRICE    14967 non-null  int64  \n 10  DISCOUNT     14967 non-null  float64\n 11  YEAR         14967 non-null  int64  \n 12  MONTH        14967 non-null  object \n 13  SALEPRICE    14967 non-null  float64\n 14  NEWCOLUMN    14967 non-null  int64  \ndtypes: float64(4), int64(3), object(8)\nmemory usage: 1.7+ MB\n\n\n\ndf_pd.drop(columns = ['NEWCOLUMN'], axis = 1, inplace = True)   \ndf_pd.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 14 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   INVOICENO    14967 non-null  object \n 1   DATE         14967 non-null  object \n 2   COUNTRY      14967 non-null  object \n 3   PRODUCTID    14967 non-null  object \n 4   SHOP         14967 non-null  object \n 5   GENDER       14967 non-null  object \n 6   SIZE_US      14967 non-null  float64\n 7   SIZE_EUROPE  14967 non-null  object \n 8   SIZE_UK      14967 non-null  float64\n 9   UNITPRICE    14967 non-null  int64  \n 10  DISCOUNT     14967 non-null  float64\n 11  YEAR         14967 non-null  int64  \n 12  MONTH        14967 non-null  object \n 13  SALEPRICE    14967 non-null  float64\ndtypes: float64(4), int64(2), object(8)\nmemory usage: 1.6+ MB\n\n\n\ndf_pd['SALEPRICE2']=df_pd['UNITPRICE']*(1-df_pd['DISCOUNT'])\ndf_pd.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14967 entries, 0 to 14966\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   INVOICENO    14967 non-null  object \n 1   DATE         14967 non-null  object \n 2   COUNTRY      14967 non-null  object \n 3   PRODUCTID    14967 non-null  object \n 4   SHOP         14967 non-null  object \n 5   GENDER       14967 non-null  object \n 6   SIZE_US      14967 non-null  float64\n 7   SIZE_EUROPE  14967 non-null  object \n 8   SIZE_UK      14967 non-null  float64\n 9   UNITPRICE    14967 non-null  int64  \n 10  DISCOUNT     14967 non-null  float64\n 11  YEAR         14967 non-null  int64  \n 12  MONTH        14967 non-null  object \n 13  SALEPRICE    14967 non-null  float64\n 14  SALEPRICE2   14967 non-null  float64\ndtypes: float64(5), int64(2), object(8)\nmemory usage: 1.7+ MB\n\n\n\n# Using the assign() function\n(df_pd[['PRODUCTID', 'UNITPRICE', 'DISCOUNT']]\\\n    .assign(SALEPRICE3 =lambda x: x.UNITPRICE*(1-x.DISCOUNT)) \\\n    .head(5)\n)\n\n  PRODUCTID  UNITPRICE  DISCOUNT  SALEPRICE3\n0      2152        159       0.0       159.0\n1      2230        199       0.2       159.2\n2      2160        149       0.2       119.2\n3      2234        159       0.0       159.0\n4      2222        159       0.0       159.0\n\n\n\n\n\n\n\n3.9.6 6th Verbs - group_by () and summarize () Functions\n     Figure 3.1 presents Split Apply Combine principle in group_by () and summarize () functions.\n\n\n\n\n\n\nFigure 3.1: Split Apply Combine Principle\n\n\n\n\ndplyrpandas\n\n\n\ndf |&gt;\n    group_by(COUNTRY) |&gt;\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE))\n\n# A tibble: 4 × 2\n  COUNTRY        AVGPRICE\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Canada             165.\n2 Germany            164.\n3 United Kingdom     166.\n4 United States      163.\n\n\n\ndf |&gt;\n    group_by(COUNTRY) |&gt;\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE),\n    AVGSALEPRICE = mean (SALEPRICE, na.rm = TRUE))\n\n# A tibble: 4 × 3\n  COUNTRY        AVGPRICE AVGSALEPRICE\n  &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;\n1 Canada             165.         144.\n2 Germany            164.         144.\n3 United Kingdom     166.         146.\n4 United States      163.         144.\n\n\n\n# Summary Statistics \ndf %&gt;%\n  select(UNITPRICE, SALEPRICE) %&gt;%\n  summarize(across(where(is.numeric), \n                   .fns = list(N = ~length(.),\n                               Mean = mean, \n                               Std = sd,\n                               Median = median, \n                               P25 = ~quantile(.,0.25), \n                               P75 = ~quantile(., 0.75)\n                               )\n                   )) %&gt;%\n  pivot_longer(everything(), names_sep='_', names_to=c('variable', '.value'))  \n\n# A tibble: 2 × 7\n  variable      N  Mean   Std Median   P25   P75\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 UNITPRICE 14967  164.  22.9    159  149    179\n2 SALEPRICE 14967  144.  35.2    149  125.   169\n\n\n\n\n\ndf_pd.groupby(['COUNTRY'])['UNITPRICE'].mean()\n\nCOUNTRY\nCanada            164.691057\nGermany           164.163934\nUnited Kingdom    165.614853\nUnited States     163.490316\nName: UNITPRICE, dtype: float64\n\n\n\ndf_pd.groupby(['COUNTRY'])[['UNITPRICE', 'SALEPRICE']].mean()\n\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\n\n\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(np.mean) \n\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\n\n\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(\"mean\") \n\n                 UNITPRICE   SALEPRICE\nCOUNTRY                               \nCanada          164.691057  144.228963\nGermany         164.163934  143.574658\nUnited Kingdom  165.614853  145.505872\nUnited States   163.490316  143.727421\n\n\n\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"))\n\n                AVG_UNITPRICE  AVG_LISTPRICE\nCOUNTRY                                     \nCanada             164.691057     144.228963\nGermany            164.163934     143.574658\nUnited Kingdom     165.614853     145.505872\nUnited States      163.490316     143.727421\n\n\n\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"),\n    TOTALN=(\"SALEPRICE\",\"size\"), # size function for n\n    TOTALOBS=(\"SALEPRICE\",\"count\") # count function for n\n )\n\n                AVG_UNITPRICE  AVG_LISTPRICE  TOTALN  TOTALOBS\nCOUNTRY                                                       \nCanada             164.691057     144.228963    2952      2952\nGermany            164.163934     143.574658    4392      4392\nUnited Kingdom     165.614853     145.505872    1737      1737\nUnited States      163.490316     143.727421    5886      5886\n\n\n\n# Defining a Function\ndef percentile(n):\n    def percentile_(x):\n        return x.quantile(n)\n    percentile_.__name__ ='percentile_{:02.0f}'.format(n*100)\n    return percentile_\n\n# Some summary statistics \ndf_pd[['UNITPRICE', 'SALEPRICE']] \\\n    .agg(['count', 'mean', 'std', 'median', percentile(0.25), percentile (0.75)]) \\\n    .transpose() \\\n    .reset_index() \\\n    .rename(columns = {\"index\": \"variables\", \"percentile_25\": \"P25\", \"percentile_75\": \"P75\", 'count': \"N\"}) \\\n    .round(3) \n\n   variables        N     mean     std  median    P25    P75\n0  UNITPRICE  14967.0  164.171  22.941   159.0  149.0  179.0\n1  SALEPRICE  14967.0  143.988  35.181   149.0  125.1  169.0\n\n\n\n# Summary Statistics \nagg_dict = {\n    \"N\": \"count\",\n    'Mean':\"mean\",\n    \"Std. Dev\" : \"std\",\n    'P25': lambda x: x.quantile(0.25),\n    'Median': 'median',\n    'p75': lambda x: x.quantile(0.75)\n}\n\ndf_pd[['UNITPRICE', 'SALEPRICE']].agg(agg_dict)",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#reshaping-data",
    "href": "eda.html#reshaping-data",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.10 Reshaping Data",
    "text": "3.10 Reshaping Data\n\n     Before we discuss about reshaping of the data, we need to discuss about tidy format of the data. Data can come in many shapes, but not all shapes are useful for data analysis. In most cases, tidy format of the data is most useful for analysis. Therefore, if the data is untidy, we need to make it tidy first. There are three interrelated rules which make a dataset tidy (Wickham, Çetinkaya-Rundel, and Grolemund 2023). These rules are given below. Figure 3.2 visually represents tidy principle.\n\nEach variable must have its own column\nEach observation must have its own row\nEach value must have its own cell\n\n\n\n\n\n\n\nFigure 3.2: Tidy Principle\n\n\n\n     For analysis, many times we need to change the format of our dataset and we call it reshaping. Data come primarily in two shapes -wide and long. Sometimes wide format is called “record” format and long format is called “stacked” format. In wide format data, there is one row for each subject (units of observation). Data is long when there are multiple rows for each subject (units of observations).\n     This reshaping can be two types - a) long to wide and 2) wide to long. Long-to-wide means reshaping a long data, which has many rows, into wide format, which has many variables. In wide-to-long format, we do otherwise. For analytical purpose, reshaping data is useful; so, we need to know how to do the reshaping.\n     Whether a given dataset (e.g., Table 3.2) is in wide or long format depends on our research questions (on what variables we are interested in and how we conceive of our data). If we are interested in variable Temp and Month variable is the unit of obsevation, then the dataset in Table 3.2 is in long format because Month is repeated in mutiple rows.\n\n\nairquality = airquality\n\nexamp = airquality |&gt;\n    slice(1:10)\n\n\n\n\n\nTable 3.2: Which Format - Long or Wide\n\n\n\n\n\n\n\nOzone\nSolar.R\nWind\nTemp\nMonth\nDay\n\n\n\n\n41\n190\n7.4\n67\n5\n1\n\n\n36\n118\n8.0\n72\n5\n2\n\n\n12\n149\n12.6\n74\n5\n3\n\n\n18\n313\n11.5\n62\n5\n4\n\n\nNA\nNA\n14.3\n56\n5\n5\n\n\n28\nNA\n14.9\n66\n5\n6\n\n\n23\n299\n8.6\n65\n5\n7\n\n\n19\n99\n13.8\n59\n5\n8\n\n\n8\n19\n20.1\n61\n5\n9\n\n\nNA\n194\n8.6\n69\n5\n10\n\n\n\n\n\n\n\n\n\n\n\n\n3.10.1 Long-to-Wide Format\n\n     To make a long dataset to wide, we can use pivot_wider() function from tidyr package in R and pivot() function from pandas in python.\n\n\ntidyrpandas\n\n\n\ntidyr::us_rent_income\n\n# A tibble: 104 × 5\n   GEOID NAME       variable estimate   moe\n   &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 01    Alabama    income      24476   136\n 2 01    Alabama    rent          747     3\n 3 02    Alaska     income      32940   508\n 4 02    Alaska     rent         1200    13\n 5 04    Arizona    income      27517   148\n 6 04    Arizona    rent          972     4\n 7 05    Arkansas   income      23789   165\n 8 05    Arkansas   rent          709     5\n 9 06    California income      29454   109\n10 06    California rent         1358     3\n# ℹ 94 more rows\n\n\n\ntidyr::us_rent_income |&gt;\n    pivot_wider(\n        names_from = variable, \n        values_from = c(estimate, moe)\n    )\n\n# A tibble: 52 × 6\n   GEOID NAME                 estimate_income estimate_rent moe_income moe_rent\n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 01    Alabama                        24476           747        136        3\n 2 02    Alaska                         32940          1200        508       13\n 3 04    Arizona                        27517           972        148        4\n 4 05    Arkansas                       23789           709        165        5\n 5 06    California                     29454          1358        109        3\n 6 08    Colorado                       32401          1125        109        5\n 7 09    Connecticut                    35326          1123        195        5\n 8 10    Delaware                       31560          1076        247       10\n 9 11    District of Columbia           43198          1424        681       17\n10 12    Florida                        25952          1077         70        3\n# ℹ 42 more rows\n\n\n\n\n\n# install palmerpenguins package\n# pip install palmerpenguins\nimport palmerpenguins\npenguins = palmerpenguins.load_penguins()\n\n\npenguins[[\"island\", \"bill_length_mm\"]] \\\n    .pivot(columns = \"island\", values = \"bill_length_mm\") \\\n    .fillna(0)\n\nisland  Biscoe  Dream  Torgersen\n0          0.0    0.0       39.1\n1          0.0    0.0       39.5\n2          0.0    0.0       40.3\n3          0.0    0.0        0.0\n4          0.0    0.0       36.7\n..         ...    ...        ...\n339        0.0   55.8        0.0\n340        0.0   43.5        0.0\n341        0.0   49.6        0.0\n342        0.0   50.8        0.0\n343        0.0   50.2        0.0\n\n[344 rows x 3 columns]\n\n\n\n\n\n\n\n3.10.2 Wide-to-Long Format\n\n     To make a wide dataset to long, we can use pivot_longer() function from tidyr package in R and melt() function from pandas in python.\n\n\ntidyrpandas\n\n\n\nrelig_income\n\n# A tibble: 18 × 11\n   religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Agnostic      27        34        60        81        76       137        122\n 2 Atheist       12        27        37        52        35        70         73\n 3 Buddhist      27        21        30        34        33        58         62\n 4 Catholic     418       617       732       670       638      1116        949\n 5 Don’t k…      15        14        15        11        10        35         21\n 6 Evangel…     575       869      1064       982       881      1486        949\n 7 Hindu          1         9         7         9        11        34         47\n 8 Histori…     228       244       236       238       197       223        131\n 9 Jehovah…      20        27        24        24        21        30         15\n10 Jewish        19        19        25        25        30        95         69\n11 Mainlin…     289       495       619       655       651      1107        939\n12 Mormon        29        40        48        51        56       112         85\n13 Muslim         6         7         9        10         9        23         16\n14 Orthodox      13        17        23        32        32        47         38\n15 Other C…       9         7        11        13        13        14         18\n16 Other F…      20        33        40        46        49        63         46\n17 Other W…       5         2         3         4         2         7          3\n18 Unaffil…     217       299       374       365       341       528        407\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\n\nrelig_income |&gt;\n    pivot_longer(\n        cols = !c(religion),\n        names_to = \"income\",\n        values_to = \"count\"\n    )\n\n# A tibble: 180 × 3\n   religion income             count\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n 1 Agnostic &lt;$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic &gt;150k                 84\n10 Agnostic Don't know/refused    96\n# ℹ 170 more rows\n\n\n\n\n\npenguins.melt(value_vars=[\"bill_length_mm\", \"bill_depth_mm\",\"flipper_length_mm\", \"body_mass_g\"],\n              id_vars = ['species', 'island', 'sex', 'year']\n              )\n\n        species     island     sex  year        variable   value\n0        Adelie  Torgersen    male  2007  bill_length_mm    39.1\n1        Adelie  Torgersen  female  2007  bill_length_mm    39.5\n2        Adelie  Torgersen  female  2007  bill_length_mm    40.3\n3        Adelie  Torgersen     NaN  2007  bill_length_mm     NaN\n4        Adelie  Torgersen  female  2007  bill_length_mm    36.7\n...         ...        ...     ...   ...             ...     ...\n1371  Chinstrap      Dream    male  2009     body_mass_g  4000.0\n1372  Chinstrap      Dream  female  2009     body_mass_g  3400.0\n1373  Chinstrap      Dream    male  2009     body_mass_g  3775.0\n1374  Chinstrap      Dream    male  2009     body_mass_g  4100.0\n1375  Chinstrap      Dream  female  2009     body_mass_g  3775.0\n\n[1376 rows x 6 columns]",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#merging-datasets",
    "href": "eda.html#merging-datasets",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.11 Merging Datasets",
    "text": "3.11 Merging Datasets\n\n     Many times, for analysis purposes, we need to join two datasets. This process is also called merging3. There are different types of joining. So, it is important to learn about those joining techniques. In Figure 3.3 shows the joining technique and functions using dplyr in R. Below all of these joining functions are explained.\n\nleft_join(): The merged dataset contains all observations fromthe first (or left) dataset and only matched observations from the second (or right) dataset\nright_join(): The merged dataset contains only matched observations from the first (or left) dataset and all observations from the second (or right) dataset\ninner_join(): The merged dataset contains only matched observations from both datasets\nsemi_join(): The merged dataset contains matched observations from the first (or left) dataset. Please note that semi_join() differs from inner_join() in that inner_join() will return one row of first dataset (x) for each matching row of second dataset (y), whereas semi_join() will never duplicate rows of x.\nfull_join(): The merged dataset contains all observations from both datasets\nanti_join(): The merged dataset contains only not matched observations from the first (or left) dataset and contains only the variable from the left dataset\n\n\n\n\n\n\n\nFigure 3.3: Joining Datasets\n\n\n\n     Table 3.3 compares the dplyr joining functions with equivalent joining functions from pandas.\n\n\n\n\nTable 3.3: Joining Functions - dplyr vs pandas\n\n\n\n\n\n\n\ndplyr\npandas\nDescription\n\n\n\n\nleft_join()\npd.merge(df1, df2, on='key', how='left')\nJoin matching rows from df2 to df1, keeping all rows from df1.\n\n\nright_join()\npd.merge(df1, df2, on='key', how='right')\nJoin matching rows from df1 to df2, keeping all rows from df2.\n\n\ninner_join()\npd.merge(df1, df2, on='key', how='inner')\nJoin matching rows from both dataframes (default behavior of merge()).\n\n\nfull_join()\npd.merge(df1, df2, on='key', how='outer')\nJoin all rows from both dataframes, filling missing values with NaN.\n\n\nsemi_join()\nNo direct equivalent, but can be achieved using filtering\nKeep rows in df1 where a match exists in df2.\n\n\nanti_join()\nNo direct equivalent, but can be achieved using filtering\nKeep rows in df1 where no match exists in df2.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndplyrpandas\n\n\n\ndata1 = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\nglimpse(data1)\n\nRows: 6,910\nColumns: 14\n$ companyname     &lt;chr&gt; \"AMERICAN AIRLINES GROUP INC\", \"AMERICAN AIRLINES GROU…\n$ stateincorp     &lt;chr&gt; \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", \"DE\", …\n$ ticker          &lt;chr&gt; \"AAL\", \"AAL\", \"AAL\", \"AAL\", \"AAL\", \"AAL\", \"AAL\", \"AAL\"…\n$ year            &lt;dbl&gt; 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, …\n$ sic             &lt;dbl&gt; 4512, 4512, 4512, 4512, 4512, 4512, 4512, 4512, 4512, …\n$ totalassets     &lt;dbl&gt; 25175.000, 25438.000, 25088.000, 23848.000, 23510.000,…\n$ costofgoodssold &lt;dbl&gt; 20232.000, 16935.000, 18138.000, 20420.000, 20529.000,…\n$ netincome       &lt;dbl&gt; -2071.000, -1468.000, -471.000, -1979.000, -1876.000, …\n$ sale            &lt;dbl&gt; 23766.000, 19917.000, 22170.000, 24022.000, 24855.000,…\n$ advertising     &lt;dbl&gt; 153.000, 153.000, 165.000, 186.000, 153.000, 166.000, …\n$ sellingadmin    &lt;dbl&gt; 3024.000, 2720.000, 2729.000, 2907.000, 2892.000, 4672…\n$ mktvalue        &lt;dbl&gt; 2976.3858, 2571.1835, 2597.5755, 117.3438, 266.5571, 6…\n$ commonequity    &lt;dbl&gt; -2935.000, -3489.000, -3945.000, -7111.000, -7987.000,…\n$ totalliability  &lt;dbl&gt; 28110.000, 28927.000, 29033.000, 30959.000, 31497.000,…\n\n\n\ndata2 = read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\")\nglimpse(data2)\n\nRows: 43,147\nColumns: 13\n$ ticker                    &lt;chr&gt; \"MLP\", \"MLP\", \"MLP\", \"RCPIQ\", \"RCPIQ\", \"RCPI…\n$ cik_code                  &lt;dbl&gt; 63330, 63330, 63330, 776008, 776008, 776008,…\n$ auditor                   &lt;chr&gt; \"Accuity LLP\", \"Accuity LLP\", \"Accuity LLP\",…\n$ INTERNALCONTROL_EFFECTIVE &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"N…\n$ year_ended_date           &lt;date&gt; 2017-12-31, 2018-12-31, 2019-12-31, 2004-12…\n$ financials_date           &lt;date&gt; 2017-12-31, 2018-12-31, 2019-12-31, 2004-12…\n$ fiscal_year               &lt;dbl&gt; 2017, 2018, 2019, 2004, 2005, 2006, 2007, 20…\n$ signature_date            &lt;date&gt; 2018-02-23, 2019-03-01, 2020-02-28, 2005-04…\n$ NUMBEROFCONTROLWEAKNESS   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0,…\n$ audit_fees                &lt;dbl&gt; 206000, 209000, 213000, 224209, 382454, 4155…\n$ non_audit_fees            &lt;dbl&gt; 29000, 30000, 30000, 188081, 92869, 61351, 3…\n$ year                      &lt;dbl&gt; 2017, 2018, 2019, 2004, 2005, 2006, 2007, 20…\n$ big4                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n# left_join\nleft_join(data1, data2, by = c(\"ticker\", \"year\"))\n\n# A tibble: 6,969 × 25\n   companyname        stateincorp ticker  year   sic totalassets costofgoodssold\n   &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1 AMERICAN AIRLINES… DE          AAL     2008  4512       25175           20232\n 2 AMERICAN AIRLINES… DE          AAL     2009  4512       25438           16935\n 3 AMERICAN AIRLINES… DE          AAL     2010  4512       25088           18138\n 4 AMERICAN AIRLINES… DE          AAL     2011  4512       23848           20420\n 5 AMERICAN AIRLINES… DE          AAL     2012  4512       23510           20529\n 6 AMERICAN AIRLINES… DE          AAL     2013  4512       42278           19084\n 7 AMERICAN AIRLINES… DE          AAL     2014  4512       43771           29511\n 8 AMERICAN AIRLINES… DE          AAL     2015  4512       48415           25416\n 9 AMERICAN AIRLINES… DE          AAL     2016  4512       51274           25695\n10 AMERICAN AIRLINES… DE          AAL     2017  4512       51396           28262\n# ℹ 6,959 more rows\n# ℹ 18 more variables: netincome &lt;dbl&gt;, sale &lt;dbl&gt;, advertising &lt;dbl&gt;,\n#   sellingadmin &lt;dbl&gt;, mktvalue &lt;dbl&gt;, commonequity &lt;dbl&gt;,\n#   totalliability &lt;dbl&gt;, cik_code &lt;dbl&gt;, auditor &lt;chr&gt;,\n#   INTERNALCONTROL_EFFECTIVE &lt;chr&gt;, year_ended_date &lt;date&gt;,\n#   financials_date &lt;date&gt;, fiscal_year &lt;dbl&gt;, signature_date &lt;date&gt;,\n#   NUMBEROFCONTROLWEAKNESS &lt;dbl&gt;, audit_fees &lt;dbl&gt;, non_audit_fees &lt;dbl&gt;, …\n\n\n\n# left_join\nleft_join(data1 |&gt; distinct(ticker, year, .keep_all = TRUE), \n          data2 |&gt; distinct(ticker, year, .keep_all = TRUE), \n          by = c(\"ticker\", \"year\")\n          )\n\n# A tibble: 6,910 × 25\n   companyname        stateincorp ticker  year   sic totalassets costofgoodssold\n   &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1 AMERICAN AIRLINES… DE          AAL     2008  4512       25175           20232\n 2 AMERICAN AIRLINES… DE          AAL     2009  4512       25438           16935\n 3 AMERICAN AIRLINES… DE          AAL     2010  4512       25088           18138\n 4 AMERICAN AIRLINES… DE          AAL     2011  4512       23848           20420\n 5 AMERICAN AIRLINES… DE          AAL     2012  4512       23510           20529\n 6 AMERICAN AIRLINES… DE          AAL     2013  4512       42278           19084\n 7 AMERICAN AIRLINES… DE          AAL     2014  4512       43771           29511\n 8 AMERICAN AIRLINES… DE          AAL     2015  4512       48415           25416\n 9 AMERICAN AIRLINES… DE          AAL     2016  4512       51274           25695\n10 AMERICAN AIRLINES… DE          AAL     2017  4512       51396           28262\n# ℹ 6,900 more rows\n# ℹ 18 more variables: netincome &lt;dbl&gt;, sale &lt;dbl&gt;, advertising &lt;dbl&gt;,\n#   sellingadmin &lt;dbl&gt;, mktvalue &lt;dbl&gt;, commonequity &lt;dbl&gt;,\n#   totalliability &lt;dbl&gt;, cik_code &lt;dbl&gt;, auditor &lt;chr&gt;,\n#   INTERNALCONTROL_EFFECTIVE &lt;chr&gt;, year_ended_date &lt;date&gt;,\n#   financials_date &lt;date&gt;, fiscal_year &lt;dbl&gt;, signature_date &lt;date&gt;,\n#   NUMBEROFCONTROLWEAKNESS &lt;dbl&gt;, audit_fees &lt;dbl&gt;, non_audit_fees &lt;dbl&gt;, …\n\n\n\n\n\ndataset1 = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\ndataset2 = pd.read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\",encoding=\"latin-1\")\n\n\npd.merge(dataset1, dataset2, on=['ticker', 'year'], how='left')\n\n                      companyname stateincorp  ... non_audit_fees  big4\n0     AMERICAN AIRLINES GROUP INC          DE  ...       990000.0   1.0\n1     AMERICAN AIRLINES GROUP INC          DE  ...      1446000.0   1.0\n2     AMERICAN AIRLINES GROUP INC          DE  ...      1455000.0   1.0\n3     AMERICAN AIRLINES GROUP INC          DE  ...      2645000.0   1.0\n4     AMERICAN AIRLINES GROUP INC          DE  ...      1989000.0   1.0\n...                           ...         ...  ...            ...   ...\n6964         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n6965         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n6966         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n6967         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n6968         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n\n[6969 rows x 25 columns]\n\n\n\ndataset1_drop = dataset1.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\ndataset2_drop = dataset2.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\npd.merge(dataset1_drop, dataset2_drop, on=['ticker', 'year'], how='left')\n\n                      companyname stateincorp  ... non_audit_fees  big4\n0     AMERICAN AIRLINES GROUP INC          DE  ...       990000.0   1.0\n1     AMERICAN AIRLINES GROUP INC          DE  ...      1446000.0   1.0\n2     AMERICAN AIRLINES GROUP INC          DE  ...      1455000.0   1.0\n3     AMERICAN AIRLINES GROUP INC          DE  ...      2645000.0   1.0\n4     AMERICAN AIRLINES GROUP INC          DE  ...      1989000.0   1.0\n...                           ...         ...  ...            ...   ...\n6905         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n6906         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n6907         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n6908         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n6909         INFOSONICS CORP -OLD          MD  ...            NaN   NaN\n\n[6910 rows x 25 columns]\n\n\n\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})\ndf2 = pd.DataFrame({'id': [2, 3, 4], 'other_value': ['X', 'Y', 'Z']})\n\n# Left join\nleft_join = pd.merge(df1, df2, on='id', how='left')\n\n# Inner join\ninner_join = pd.merge(df1, df2, on='id')\n\n# Semi join\nsemi_join = df1[df1['id'].isin(df2['id'])]\n\n# Anti join\nanti_join = df1[~df1['id'].isin(df2['id'])]",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#conclusions",
    "href": "eda.html#conclusions",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "3.12 Conclusions",
    "text": "3.12 Conclusions\n\n\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "eda.html#footnotes",
    "href": "eda.html#footnotes",
    "title": "3  Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "You can also use British spelling - summarise ()↩︎\nIndexing involves obtaining individual elements.↩︎\nIn database context, it is “merging”, but commonly it is called “joining”.↩︎",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploratory Data Analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "predictive.html#conclusion",
    "href": "predictive.html#conclusion",
    "title": "7  Predictive Modeling - Linear Regression",
    "section": "7.4 Conclusion",
    "text": "7.4 Conclusion",
    "crumbs": [
      "Predictive and Prescriptive Analytics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predictive Modeling - Linear Regression</span>"
    ]
  }
]