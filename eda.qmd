---
title: "Exploratory Data Analysis (EDA)"
format: html
---

### Learning Objectives of the Chapter {.unnumbered}

::: {style="text-align: justify"}
At the End of the Chapter, Students should be Able to -

-   Learn about the purpose of Exploratory Data Analysis (EDA)

-   Understand different techniques of transforming and cleaning data

-   Learn about Different R and Python Packages for EDA

-   Understand how to use six verbs for EDA

-   Perform EDA on some real world data sets. 

-   Learn about how to interpret results from EDA
:::


## Introduction 

::: {style="text-align: justify"}
     In descriptive statistics, we summarize the data using different metrics such as mean, median, standard deviation, minimum value, maximum value, and percentile. Descriptive statisics is also called summary statistics.
:::

## Data Collection & Importing 


## Data Cleaning 


## Packages for Exploratory Data Analysis (EDA)

::: {style="text-align: justify"}
     In order to use `pyjanitor`, the data frame must be pandas because `pyjanitor` extends pandas data frame functionality. 
:::


::: {.panel-tabset}

## dplyr

```{r}
#| warning: false
# loading packages
library(tidyverse)
library(lubridate)
library(janitor)
```

```{r}
#| include: false
library(reticulate)
Sys.unsetenv("RETICULATE_PYTHON")
reticulate::use_virtualenv("C:/Users/mshar/OneDrive - Southern Illinois University/ANALYTICS_FOR_ACCOUNTING_DATA/accounting_analytics_book", required = TRUE)
#Sys.setenv('RETICULATE_PYTHON' = 'C:\\Users\\mshar\\AppData\\Local\\Programs\\Python\\Python311\\python.exe')
#py_install("pyjanitor")
#py_install("polars")
#Sys.setenv('RETICULATE_PYTHON' = '~/.venv/quarto_book_python/Scripts/python.exe')
```


## pandas 
```{python}
# loading the package
import numpy as np
import pandas as pd
# from pyjanitor package 
# pip install pyjanitor
import janitor 
from janitor import clean_names, remove_empty
```

:::

## Importing the Dataset 

::: {.panel-tabset}

## dplyr
```{r}
#| warning: false
# importing data frame 
df = read_csv("https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv")
```

## pandas 
```{python}
# importing data frame 
df_pd = pd.read_csv("https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv")
```



:::

## Meta Data

::: {style="text-align: justify"}
     Meta data is data about the data. Before we put the data into analysis, we need to learn about our dataset. This learning invovles knowing about the number of rows, number of columns, the types of the fields, the appropriateness of those types, the missing values in the dataset and so on. 
:::

::: {.panel-tabset}

## dplyr 

```{r}
glimpse(df)
```


```{r}
map_df(df, ~sum(is.na(.))) |>
     glimpse()
```

```{r}
ncol(df)
nrow(df)
```

```{r}
head(df)
```

```{r}
tail(df)
```

```{r}
dplyr::sample_n(df, 10)
```


## Pandas 

```{python}
df_pd.info()
```

```{python}
df_pd.shape
```

```{python}
print('The total number of rows and columns of the product data is \
 {} and {} respectively.'.format(df_pd.shape[0], df_pd.shape[1]))
```


```{python}
print(f'The total number of rows and columns of the product data is \
 {df_pd.shape[0]} and {df_pd.shape[1]} respectively.')
```

```{python}
df_pd.columns
```

```{python}
df_pd.head()
```

```{python}
df_pd.tail()
```

```{python}
df_pd.isna().sum()
```


```{python}
df_pd.dtypes
```

```{python}
df_pd.sample(n=10)
```


:::

## Cleaning the Dataset 

::: {.panel-tabset}

## dplyr

```{r}
 df |>
     rename_all(toupper) |>
     janitor::clean_names() |>
     rename_all(toupper) |>
     glimpse()
```

```{r}
df = df |>
     rename_all(toupper) |>
     janitor::clean_names() |>
     rename_all(toupper)
glimpse(df)
```


## panads 

```{python}
df_pd.columns.str.upper().to_list()
```

```{python}
(df_pd
     .pipe(remove_empty)
     .pipe(lambda x: x.clean_names(case_type = "upper"))
     .pipe(lambda x: x.rename(columns = {'SIZE_US_': 'SIZE_US', 'SIZE_EUROPE_':"SIZE_EUROPE", "SIZE_UK_":"SIZE_UK"}))
     .pipe(lambda x: x.info())
     )
```

```{python}
# Changing the names of the columns to uppercase
df_pd.rename(columns = str.upper, inplace = True)
df_pd.columns
```


```{python}
#| warning: false
new_column = df_pd.columns \
 .str.replace("(", '').str.replace(")", "") \
 .str.replace(' ','_') # Cleaning the names of the variables
new_column
```


```{python}
df_pd.columns = new_column
df_pd.columns
df_pd.rename(columns=str.upper, inplace = True)
df_pd.columns 

```


::: 


### Changing the Types of Variables 

::: {.panel-tabset}

## dplyr

```{r}
df |>
    mutate (DATE = lubridate::mdy(DATE)) |>
    glimpse()
```

     From the above, it is now evident the the type of the `DATE` variable now is `date`. 

```{r}
df |>
    mutate (DATE = lubridate::mdy(DATE)) |>
    mutate (PRODUCTID = as.character(PRODUCTID)) |>
    glimpse()
```

     From the above, it is now evident the the type of the `DATE` and `PRODUCTID` variable now is date (`date`) and character (`chr`) respectively. We can now incorparte the changes into the data frame. 

```{r}
df = df |>
    mutate (DATE = lubridate::mdy(DATE)) |>
    mutate (PRODUCTID = as.character(PRODUCTID)) 
glimpse(df)
```


## pandas 

```{python}
(
    df_pd
    .pipe(lambda x: x.assign(DATE = pd.to_datetime(x['DATE'])))
    .pipe(lambda x: x.info())
)

```


```{python}
# converting integer to object
df_pd.INVOICENO = df_pd.INVOICENO.astype(str)
df_pd[['MONTH', 'PRODUCTID']] = df_pd[['MONTH', 'PRODUCTID']].astype(str)
df_pd.info()
```



:::

## Some Other Useful Functions 
     There are some other useful functions that can be used to explore the dataset for analysis. Some of those useful functions are discussed below. 

:::{.panel-tabset}


## dplyr

```{r}
df|> count(YEAR)
```

```{r}
df|> count(COUNTRY)
```

```{r}
df|> distinct(COUNTRY)
```

## pandas 
```{python}
df_pd['YEAR'].value_counts()
```

```{python}
df_pd['YEAR'].unique()
```


:::


## Six Verbs for EDA 

     @tbl-compareDplyrPandas shows the comparable functions in both `dplyr` and `pandas` packages. These functions are very much important to perform exploratory data analysis in both `R` and `Python`. `group_by` (`groupby` in pandas) and `summarize ()`^[You can also use British spelling - `summarise ()`] (`agg ()` in pandas) are often used together; therefore, they are in the same group in @tbl-compareDplyrPandas. 

```{r}
#| include: false
tidyverse_pandas = tibble::tribble(
  ~`Verb Number`,~`tidyverse`, ~ `pandas`, 
  '1','filter ()', 'query () or loc () or iloc ()',
  '2','arrange ()', 'sort_values ()',
  '3','select ()', 'filter () or loc ()',
  '4','rename ()', 'rename ()',
  '5','mutate ()', 'assign ()',
  '6','group_by ()', 'groupby ()',
  '6','summarize ()', 'agg ()'
)

```

```{r}
#| label: tbl-compareDplyrPandas
#| tbl-cap: Tidyverse and Pandas Equivalent Functions 
#| echo: false
#| warning: false
# These are R code to prepare Table 2 using KableExtra 
library(kableExtra)
kbl(tidyverse_pandas, booktabs = TRUE 
    #,caption = "Tidyverse and Pandas Equivalent Functions"
    ) %>% 
  kable_styling(latex_options = c ('striped', 'hold_positions'))
```

### 1st Verb - filter () Function 

     Filter functions are used to subset a data frame based on rows, meaning that retaining rows that satisfy given conditions. Filtering rows is also called slicing^[Indexing involves obtaining individual elements.] becasue we obtain a set of elements by filtering.  

:::{.panel-tabset}

## dplyr 

```{r}
df |> filter (YEAR == "2015")
```

```{r}
df |> filter (COUNTRY %in% c("United States", "Canada"))
```

```{r}
df |> filter (COUNTRY == "United States", YEAR == "2016")
```

```{r}
df |> filter (COUNTRY == "United States", YEAR %in% c("2015","2016"))
```

```{r}
df |> filter (COUNTRY %in% c("United States", "Canada"), YEAR == "2014")
```


## pandas 

```{python}
df_pd.query("YEAR == 2015")
```

```{python}
df_pd.query('COUNTRY== "United States" | COUNTRY == "Canada"')
```

```{python}
df_pd.query("COUNTRY in ['United States', 'Canada']")
```

```{python}
df_pd.query("COUNTRY== 'United States' & YEAR== 2016")
```

```{python}
df_pd.query("COUNTRY== 'United States' & YEAR in [2015,2016]")
```

```{python}
df_pd[df_pd['COUNTRY'] == "United States"]
```

```{python}
df_pd.loc[(df_pd['COUNTRY']=="United States")]
```


```{python}
df_pd.loc[df_pd['COUNTRY'].isin(["United States", "Canada"])]
```

```{python}
df_pd.loc[df_pd['COUNTRY']\
 .isin(["United States", "Canada"]) &(df_pd['YEAR']==2014)]
```

```{python}
df_pd.loc[(df_pd['COUNTRY']=="United States") &(df_pd ["YEAR"] ==2014)]
```

```{python}
df_pd.loc[df_pd['COUNTRY'] == "United States", :]
```

```{python}
df_pd.loc[
    df_pd['COUNTRY']=='United States',
    ['COUNTRY', "UNITPRICE", "SALEPRICE"]]
```

:::

### 2nd Verb - arrange () Function 

     In arrange functions, we order the rows of a data frame by the values of given columns. It is like sorting or odering the data. 

:::{.panel-tabset}

## dplyr 
```{r}
df |>
    arrange(DATE)     
```

```{r}
df |>
    arrange(desc(DATE))     
```


```{r}
df |>
    arrange(MONTH, SALEPRICE)     
```



## pandas 
```{python}
df_pd.sort_values(by =['DATE'])   
```

```{python}
df_pd.sort_values(by =['DATE'], ascending = False)   
```

```{python}
df_pd.sort_values(by =['MONTH', 'SALEPRICE'])
```


:::


### 3rd Verb - select () Function 
     Select functions help to select or obtain columns from the data frame. When there are a lot of columns in our dataset, select functions become very useful. 

:::{.panel-tabset}

## dplyr 
```{r}
df |> select(DATE, UNITPRICE, DISCOUNT)   
```


```{r}
df |> select(1:2, 5:8)  
```

```{r}
df |>
    select(starts_with('SIZE'))
```

```{r}
df |>
    select(ends_with('PRICE'))
```


```{r}
df |>
    select(contains("_"))
```

```{r}
df |>
    select(matches("SIZE"))
```

```{r}
df |>
    select(matches("PRICE$"))
```

```{r}
# starts with letter S
df |>
    select(matches("^S"))
```


```{r}
df |>
    select(where(is.character))
```

```{r}
df |>
    select(where(is.numeric))
```

```{r}
df |>
    select(MONTH, YEAR, everything())
```

```{r}
# any_of () vs all_of ()
df |>
    select(any_of(c("PRICE", "SIZE")))
```


```{r}
# Dropping columns 
df |>
    select(-DATE)
```



## pandas 
```{python}
df_pd['DATE']   
```

```{python}
df_pd[['DATE', 'UNITPRICE']]   
```

```{python}
df_pd.loc[:,['DATE', 'UNITPRICE']]   
```


```{python}
df_pd.iloc[:,5:8]
```

```{python}
df_pd.iloc[:,[3,5,8]]
```

```{python}
df_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])
```

```{python}
df_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])
```

```{python}
 #RegularExpression(Regex)
df_pd.filter(regex ="PRICE$") #Ends with Price
```

```{python}
df_pd.filter(regex ="ˆSIZE") #Starts with SIZE
```


```{python}
df_pd.filter(regex ="PRICE") #Contains the word Price
```

```{python}
df_pd.select_dtypes('object')
```

```{python}
df_pd.select_dtypes('int')
```

```{python}
df_pd.loc[:,df_pd.columns.str.startswith('SIZE')]
```

```{python}
df_pd.loc[:,df_pd.columns.str.contains('PRICE')]
```

```{python}
df_pd.loc[:,df_pd.columns.str.endswith('PRICE')]
```


```{python}
# Dropping columns 
df_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)
```

```{python}
# Dropping columns 
df_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\
    .pipe(lambda x: x.info())
```

```{python}
# Rearranging columns 
# Sorting Alphabetically
df_pd.reindex(sorted(df_pd.columns), axis =1)
```

```{python}
# Rearranging columns 
# Sorting As You Want (ASY)
col_first = ['YEAR','MONTH']
col_rest = df_pd.columns.difference(col_first, sort=False).to_list()
df_pd2 = df_pd [col_first +col_rest]
df_pd2.info()
```


:::


### 4th Verb - rename () Function 

:::{.panel-tabset}

## dplyr 
```{r}
df |>
    rename(INVOICE = INVOICENO,
    PRODUCT = PRODUCTID) |>
    glimpse()
     
```

## pandas 
```{python}
(df_pd.rename(columns = {"PRODUCTID": "PRODUCT", "INVOICENO": "INVOICE"})
     .pipe(lambda x: x.info())
)
```


:::

### 5th Verb - mutate () Function 

:::{.panel-tabset}

## dplyr 
```{r}
df |>
    mutate(NECOLUMN = 5,
    SALESPRICE2 = UNITPRICE*(1-DISCOUNT)) |>
    glimpse()
     
```

## pandas 
```{python}
df_pd['NEWCOLUMN'] = 5
df_pd.info()     
```

```{python}
df_pd.drop(columns = ['NEWCOLUMN'], axis = 1, inplace = True)   
df_pd.info() 
```

```{python}
df_pd['SALEPRICE2']=df_pd['UNITPRICE']*(1-df_pd['DISCOUNT'])
df_pd.info()
```


```{python}
# Using the assign() function
(df_pd[['PRODUCTID', 'UNITPRICE', 'DISCOUNT']]\
    .assign(SALEPRICE3 =lambda x: x.UNITPRICE*(1-x.DISCOUNT)) \
    .head(5)
)
```


:::

### 6th Verbs - group_by () and summarize () Functions 

     @fig-splitapplycombine presents Split Apply Combine principle in `group_by ()` and `summarize ()` functions. 

![Split Apply Combine Principle](images/split-apply-combine.png){#fig-splitapplycombine}


:::{.panel-tabset}

## dplyr 
```{r}
df |>
    group_by(COUNTRY) |>
    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE))
     
```

```{r}
df |>
    group_by(COUNTRY) |>
    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE),
    AVGSALEPRICE = mean (SALEPRICE, na.rm = TRUE))
     
```


```{r}
# Summary Statistics 
df %>%
  select(UNITPRICE, SALEPRICE) %>%
  summarize(across(where(is.numeric), 
                   .fns = list(N = ~length(.),
                               Mean = mean, 
                               Std = sd,
                               Median = median, 
                               P25 = ~quantile(.,0.25), 
                               P75 = ~quantile(., 0.75)
                               )
                   )) %>%
  pivot_longer(everything(), names_sep='_', names_to=c('variable', '.value'))  
```



## pandas 
```{python}
df_pd.groupby(['COUNTRY'])['UNITPRICE'].mean()
     
```

```{python}
df_pd.groupby(['COUNTRY'])[['UNITPRICE', 'SALEPRICE']].mean()
     
```


```{python}
#| warning: false
df_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \
    .agg(np.mean) 
```

```{python}
df_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \
    .agg("mean") 
```

```{python}
df_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \
    .agg(AVG_UNITPRICE =("UNITPRICE","mean"),
    AVG_LISTPRICE =("SALEPRICE","mean"))
```

```{python}
df_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \
    .agg(AVG_UNITPRICE =("UNITPRICE","mean"),
    AVG_LISTPRICE =("SALEPRICE","mean"),
    TOTALN=("SALEPRICE","size"), # size function for n
    TOTALOBS=("SALEPRICE","count") # count function for n
 )
```

```{python}
# Defining a Function
def percentile(n):
    def percentile_(x):
        return x.quantile(n)
    percentile_.__name__ ='percentile_{:02.0f}'.format(n*100)
    return percentile_

# Some summary statistics 
df_pd[['UNITPRICE', 'SALEPRICE']] \
    .agg(['count', 'mean', 'std', 'median', percentile(0.25), percentile (0.75)]) \
    .transpose() \
    .reset_index() \
    .rename(columns = {"index": "variables", "percentile_25": "P25", "percentile_75": "P75", 'count': "N"}) \
    .round(3) 
```

```{python}
#| eval: false
# Summary Statistics 
agg_dict = {
    "N": "count",
    'Mean':"mean",
    "Std. Dev" : "std",
    'P25': lambda x: x.quantile(0.25),
    'Median': 'median',
    'p75': lambda x: x.quantile(0.75)
}

df_pd[['UNITPRICE', 'SALEPRICE']].agg(agg_dict)
```


:::

## Reshaping Data 

::: {style="text-align: justify"}
     Before we discuss about reshaping of the data, we need to discuss about tidy format of the data. Data can come in many shapes, but not all shapes are useful for data analysis. In most cases, tidy format of the data is most useful for analysis. Therefore, if the data is untidy, we need to make it tidy first. There are three interrelated rules which make a dataset tidy [@wickham2023r]. These rules are given below. @fig-tidyprinciple visually represents tidy principle. 

1. Each variable must have its own column

2. Each observation must have its own row

3. Each value must have its own cell 

![Tidy Principle](images/tidy-1.png){#fig-tidyprinciple}

     For analysis, many times we need to change the format of our dataset and we call it reshaping. Data come primarily in two shapes -*wide* and *long*. Sometimes *wide* format is called "record" format and *long* format is called "stacked" format. In `wide` format data, there is one row for each subject (units of observation). Data is `long` when there are multiple rows for each subject (units of observations). 

     This reshaping can be two types - a) long to wide and 2) wide to long. Long-to-wide means reshaping a long data, which has many rows, into wide format, which has many variables. In wide-to-long format, we do otherwise. For analytical purpose, reshaping data is useful; so, we need to know how to do the reshaping. 

     Whether a given dataset (e.g., @tbl-formatofdata) is in wide or long format depends on our research questions (on what variables we are interested in and how we conceive of our data). If we are interested in variable `Temp` and `Month` variable is the unit of obsevation, then the dataset in @tbl-formatofdata is in `long` format because `Month` is repeated in mutiple rows. 

:::

```{r}
#| warning: false
airquality = airquality

examp = airquality |>
    slice(1:10)
```


```{r}
#| label: tbl-formatofdata
#| tbl-cap: Which Format - Long or Wide 
#| echo: false
#| warning: false

kbl(examp, booktabs = TRUE 
    ) %>% 
  kable_styling(latex_options = c ('striped', 'hold_positions'))
```


### Long-to-Wide Format  

::: {style="text-align: justify"}
     To make a long dataset to wide, we can use `pivot_wider()` function from `tidyr` package in R and `pivot()` function from `pandas` in python. 
:::


:::{.panel-tabset}

## tidyr
```{r}
tidyr::us_rent_income
```

```{r}
tidyr::us_rent_income |>
    pivot_wider(
        names_from = variable, 
        values_from = c(estimate, moe)
    )

```


## pandas 

```{python}
#| warning: false
# install palmerpenguins package
# pip install palmerpenguins
import palmerpenguins
penguins = palmerpenguins.load_penguins()
```

```{python}
penguins[["island", "bill_length_mm"]] \
    .pivot(columns = "island", values = "bill_length_mm") \
    .fillna(0)
```


:::


### Wide-to-Long Format  

::: {style="text-align: justify"}
     To make a wide dataset to long, we can use `pivot_longer()` function from `tidyr` package in R and `melt()` function from `pandas` in python.
:::


:::{.panel-tabset}

## tidyr
```{r}
relig_income
```

```{r}
relig_income |>
    pivot_longer(
        cols = !c(religion),
        names_to = "income",
        values_to = "count"
    )
```

## pandas 

```{python}
penguins.melt(value_vars=["bill_length_mm", "bill_depth_mm","flipper_length_mm", "body_mass_g"],
              id_vars = ['species', 'island', 'sex', 'year']
              )
```

:::


## Merging Datasets 

::: {style="text-align: justify"}
     Many times, for analysis purposes, we need to join two datasets. This process is also called merging^[In database context, it is "merging", but commonly it is called "joining".]. There are different types of joining. So, it is important to learn about those joining techniques. In @fig-joindatasets shows the joining technique and functions using `dplyr` in R. Below all of these joining functions are explained. 

* `left_join()`: The merged dataset contains **all** observations fromthe **first** (or left) dataset and only **matched** observations from the **second** (or right) dataset

* `right_join()`: The merged dataset contains only **matched** observations from the **first** (or left) dataset and **all** observations from the **second** (or right) dataset

* `inner_join()`: The merged dataset contains *only* **matched** observations from both datasets

* `semi_join()`: The merged dataset contains **matched** observations from the **first** (or left) dataset. Please note that `semi_join()` differs from `inner_join()` in that `inner_join()` will return one row of first dataset (x) for each matching row of second dataset (y), whereas `semi_join()` will never duplicate rows of x.   

* `full_join()`: The merged dataset contains **all** observations from both datasets

* `anti_join()`: The merged dataset contains only **not matched** observations from the **first** (or left) dataset and contains only the variable from the **left** dataset

![Joining Datasets](images/join_diagram.png){#fig-joindatasets width="40%"}

```{r}
#| include: false
df_join <- tribble(
  ~dplyr, ~pandas, ~Description,
  "left_join()", "pd.merge(df1, df2, on='key', how='left')", "Join matching rows from df2 to df1, keeping all rows from df1.",
  "right_join()", "pd.merge(df1, df2, on='key', how='right')", "Join matching rows from df1 to df2, keeping all rows from df2.",
  "inner_join()", "pd.merge(df1, df2, on='key', how='inner')", "Join matching rows from both dataframes (default behavior of merge()).",
  "full_join()", "pd.merge(df1, df2, on='key', how='outer')", "Join all rows from both dataframes, filling missing values with NaN.",
  "semi_join()", "No direct equivalent, but can be achieved using filtering", "Keep rows in df1 where a match exists in df2.",
  "anti_join()", "No direct equivalent, but can be achieved using filtering", "Keep rows in df1 where no match exists in df2."
)
```

     @tbl-joinRPython compares the `dplyr` joining functions with equivalent joining functions from `pandas`.

```{r}
#| label: tbl-joinRPython
#| tbl-cap: Joining Functions - dplyr vs pandas  
#| echo: false
#| warning: false

kbl(df_join, booktabs = TRUE 
    ) %>% 
  kable_styling(latex_options = c ('striped', 'hold_positions'))
```


:::

:::{.panel-tabset}

## dplyr

```{r}
#| warning: false
data1 = read_csv("https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv")
glimpse(data1)
```

```{r}
#| warning: false
data2 = read_csv("https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv")
glimpse(data2)
```

```{r}
# left_join
left_join(data1, data2, by = c("ticker", "year"))
```

```{r}
# left_join
left_join(data1 |> distinct(ticker, year, .keep_all = TRUE), 
          data2 |> distinct(ticker, year, .keep_all = TRUE), 
          by = c("ticker", "year")
          )
```


## pandas 

```{python}
#| warning: false
dataset1 = pd.read_csv("https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv")
dataset2 = pd.read_csv("https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv",encoding="latin-1")
```

```{python}
pd.merge(dataset1, dataset2, on=['ticker', 'year'], how='left')
```

```{python}
#| warning: false
dataset1_drop = dataset1.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)
dataset2_drop = dataset2.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)
pd.merge(dataset1_drop, dataset2_drop, on=['ticker', 'year'], how='left')
```

```{python}
df1 = pd.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})
df2 = pd.DataFrame({'id': [2, 3, 4], 'other_value': ['X', 'Y', 'Z']})

# Left join
left_join = pd.merge(df1, df2, on='id', how='left')

# Inner join
inner_join = pd.merge(df1, df2, on='id')

# Semi join
semi_join = df1[df1['id'].isin(df2['id'])]

# Anti join
anti_join = df1[~df1['id'].isin(df2['id'])]
```

:::

## Conclusions 
