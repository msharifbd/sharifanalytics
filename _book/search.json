[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytics for Accounting Data",
    "section": "",
    "text": "Welcome to the online home of Analytics for Accounting Data, which is slated to be published future. It will be available for purchase in both paperback and hardback, with pre-ordering available now on both Amazon and online.\n\n\nPreface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2\n\n\n\n# To get more idea about how to develop a book in Quarto\n# Please see the following links -\n# https://bradcongelio.com/nfl-analytics-with-r-book/\n# https://github.com/bcongelio/nfl-analytics-with-r-book\n# https://github.com/hadley/r4ds/\n# https://r4ds.hadley.nz/\n\n\n\nAbout the Book\nThe book is written for the students in undergraduate and graduate programs.\n\n\nAbout the Author\n\n\n\n\nSharif Islam, DBA, CPA, CMA is an Assistant professor in School of Accountancy in Southern Illinois University Carbondale (SIUC). He is a licensed CPA in Illinois and a Certified Management Accountant (CMA). He teaches Machine Learning, Analytics for Accounting Data, Auditing, and Accounting Information Systems. He did his doctorate from Louisiana Tech University in Computer Information Systems and Accounting. He published research in Accounting Horizons, Journal of Accounting and Public Policy, Journal of Emerging Technologies in Accounting, Issues in Accounting Education, Advances in Accounting and Managerial Auditing Journal. His research interests lie at the intersection of Accounting and Data Science.\n\n\n\nHow to Read the Book\n\n\nAcknowledgment\n\nTo prepare the book, I took help from many sources on the internet and published materials. Many of them are cited in the book. I acknowledge the contribution of all of those resources that help me to prepare the books for the students.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "advanced_analytics.html",
    "href": "advanced_analytics.html",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#what-is-web-scraping",
    "href": "advanced_analytics.html#what-is-web-scraping",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.1 What is Web Scraping?",
    "text": "9.1 What is Web Scraping?\n\n     Web scraping refers to the techniques of accessing websites and collecting information from them. Having web scraping knowledge is important nowadays because a vast amount of data is available on websites and in many occasions we need to access, collect, and analyze those data. Web scraping is also called “web harvesting” or “web data extraction”.\n     Web scraping is employed in different kinds of practical applications. For example, companies scrape websites of their competitors to keep track of their pricing, which can help companies to form a competitive pricing strategy. Moreover, marketers and analysts scrape different social media platforms to analyze public sentiment about their products, brands, or events, which help them to gauge public opinions and ultimately tailor their products or services to meet or exceed customers’ expectations.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#legal-and-ethical-consideration-of-web-scraping",
    "href": "advanced_analytics.html#legal-and-ethical-consideration-of-web-scraping",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.2 Legal and Ethical Consideration of Web Scraping",
    "text": "9.2 Legal and Ethical Consideration of Web Scraping\n\n     As good citizens on the internet, it is incumbent on us to respect the policies of the websites we plan to scrape. Therefore, before we decide to scrape a website, we must take into consideration the legal and ethical aspects of scraping.\n\n\n9.2.1 Legal Framework of Web Scraping\n\n     Before scraping a website, we must evaluate the following legal considerations -\n\nTerms of Service: Please check the terms of service of the website because some sites explicitly prohibit scraping and violating terms of service might result in legal action.\nCopyright Law: In most cases, data published online is protected by copyright. As such, it is important to know beforehand what you can legally collect from the website by scraping and how you can use the scraped data.\nComputer Fraud and Abuse Act (CFAA): In the US, the Computer Fraud and Abuse Act (CFAA) was enacted in 1986. The CFAA prohibits intentionally accessing a computer without authorization or in excess of authorization. You might violate CFAA if a website has taken steps to block scraping and you circumvent those measures.\nData Protection Law: Beacause of different kinds of data protection law such as General Data Protection Regulation (GDPR) in Europe or similar law in other jurisdictions, it has become very critical to deal with personal data. If you scrape personal data, you must comply with such laws, which typically include requirements for consent, data minimization, and secure handling of the data.\n\n\n\n\n9.2.2 Ethical Considerations of Web Scraping\n\n     In addition to legal considerations, you should also behave ethically when you try to scrape a website. Ethical considerations though aligns with legal considerations, they extend to the idea of good citizenship on the web. Some important ethical considerations during webscraping include -\n\nWeb scraping might be equivalent to Distriubted of Denial of Service (DDOS) attack if too many requests are sent to the targeted websites, thus disrupting the regular functioning of the website. Therefore, while web scraping, we should scrape in such a way so that it does not disrupt the usage of the website by other legitimate users. Further, you should not try to scrape a website if it prohibits web scraping. Some websites have robots.txt file, which defines what can be scraped from the website. So please invesitage a website well before you decide to scrape it.\nHow the scraped data will be used is an important considerations even if the data are publicly available. Using the web scraped data in a way that is detrimental to individual or businesses is unethical. Further, you should also consider the ramifications of publishing or sharing the scraped data.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#understanding-html-and-css-selectors",
    "href": "advanced_analytics.html#understanding-html-and-css-selectors",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.3 Understanding HTML and CSS Selectors",
    "text": "9.3 Understanding HTML and CSS Selectors\n\n     Websites are usually created by using HTML - HyperText Markup Language, which describes the structure of a web page and includes cues for the apperance of a website. Therefore, having some knowledge on HTML will help you to scrape a website. HTML document uses different kinds of tags to identify or refer to different elements. A typical HTML document has following elements -\n&lt;!DOCTYPE&gt; : Defines the document type\n&lt;html&gt; : Defines the HTML document\n&lt;head&gt; : Contains metadata or information for the document\n&lt;body&gt; : Defines the document body such as text, images, and other media\n     More about HTML tags can be found here. Here is an example of a basic HTML structure -\n\n&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;\n&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were\n&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and\n&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;\nand they lived at the bottom of a well.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\n\n     In addition to HTML tags, CSS (Cascading Style Sheets) selectors are used to style different elements in the website. In web scraping, we use CSS selectors to identify the data we want to extract. There are different types of CSS selectors:\n\nElement Selector: Selects all elements of a specific type. For example, p selects all &lt;p&gt; elements.\nID Selector: Selects a single element with a specific id. The ID selector is defined with a hash (#). For example, #navbar selects the element with id=\"navbar\".\nClass Selector: Selects all elements with a specific class. The class selector is defined with a dot (.). For example, .menu-item selects all elements with class=\"menu-item\".\nAttribute Selector: Selects elements with a specific attribute or attribute value. For example, [href] selects all elements with an href attribute.\n\n     Below is an example of CSS selectors -\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;style&gt;\n        #header {\n            background-color: #f2f2f2;\n        }\n        .highlight {\n            font-weight: bold;\n        }\n        a[href^=\"https\"] {\n            color: green;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div id=\"header\"&gt;This is the header&lt;/div&gt;\n    &lt;p class=\"highlight\"&gt;This paragraph is highlighted.&lt;/p&gt;\n    &lt;a href=\"https://example.com\"&gt;This link is green because it uses HTTPS.&lt;/a&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n     In the above code, #header selects the &lt;div&gt; with the ID of “header,” .highlight selects any element with the “highlight” class, and a[href^=\"https\"] selects anchor tags (&lt;a&gt;) whose href attribute value begins with “https”. Understanding how to use these CSS selectors are very important while web scraping websites.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#an-overview-of-beautiful-soup",
    "href": "advanced_analytics.html#an-overview-of-beautiful-soup",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.4 An Overview of Beautiful Soup",
    "text": "9.4 An Overview of Beautiful Soup\n\n     Beautifulsoup is a python module that is widely used to scrape and parse websites. Beautifulsoup has many useful functions that can be easily used to extract data from HTML. Figure 9.1 shows the basic work process Beautifulsoup uses. It is clear from Figure 9.1 that using Beautifulsoup, we can extract data by finding HTML tag names, by CSS class names, and so on.\n\n\n\n\n\n\nFigure 9.1: Beautiful Soup Process\n\n\n\n     The following python code can be run to install and import Beautifulsoup module.\n\n# installing beautifulsoup \npip install beautifulsoup4\n\n# importing beautifulsoup\nfrom bs4 import BeautifulSoup\n\n     When we use BeautifulSoup to scrape a website, one of the most critical tasks is to identify the tags or CSS selectors from which we want to extract text or data. These targets are called Document Object Model (DOM). The DOM is a programming interface for web documents. Visualize HTML code of a webpage as an upside-down tree. Each HTML element - headings, paragraphs, and links - is a node in the tree. Figure 9.2 shows a basic tree structure of an HTML page.\n\n\n\n\n\n\nFigure 9.2: Tree Structure of HTML Page\n\n\n\n\n\n9.4.1 An Example of Web Scraping\n\n     Below we provide a small example of webscraping. We create a webpage called html, which includes different tags and CSS selectors.\n\n# an HTML file data \n\nhtml = \"\"\"\n&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;\n&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were\n&lt;a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n&lt;a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\"&gt;Lacie&lt;/a&gt; and\n&lt;a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\"&gt;Tillie&lt;/a&gt;;\nand they lived at the bottom of a well.&lt;/p&gt;\n&lt;/body&gt;&lt;/html&gt;\"\"\"\n\n     Then we import BeautifulSoup from beautifulsoup.\n\n# importing beautiful soup \nfrom bs4 import BeautifulSoup\n\n     Next, we convert the html into beautifulsoup object and name it soup. In BeautifulSoup ()function, we use the built-in parser called html.parser. We can also use other parsers such as lxml or html5lib. Each of these parsers has their own pros and cons. For example, lxml is the fastest and html.parser does not need extra dependencies.\n\n# Converting HTML data into Beautiful Soup Object \nsoup = BeautifulSoup(html, \"html.parser\")\n\n     The prettify() function will turn a soup object into a nicely formatted Unicode string, witha a separate line for each tag and each string.\n\nsoup.prettify()\n\n'&lt;html&gt;\\n &lt;head&gt;\\n  &lt;title&gt;\\n   The Dormouse\\'s story\\n  &lt;/title&gt;\\n &lt;/head&gt;\\n &lt;body&gt;\\n  &lt;p class=\"title\"&gt;\\n   &lt;b&gt;\\n    The Dormouse\\'s story\\n   &lt;/b&gt;\\n  &lt;/p&gt;\\n  &lt;p class=\"story\"&gt;\\n   Once upon a time there were three little sisters; and their names were\\n   &lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;\\n    Elsie\\n   &lt;/a&gt;\\n   ,\\n   &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;\\n    Lacie\\n   &lt;/a&gt;\\n   and\\n   &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;\\n    Tillie\\n   &lt;/a&gt;\\n   ;\\nand they lived at the bottom of a well.\\n  &lt;/p&gt;\\n &lt;/body&gt;\\n&lt;/html&gt;\\n'\n\n\n     We can use get_text() function to see the text element of the tags. text is a property (attribute) of soup object, which calls get_text function.\n\nsoup.get_text()\n\n\"\\nThe Dormouse's story\\n\\nThe Dormouse's story\\nOnce upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.\\n\"\n\n\n\nsoup.text\n\n\"\\nThe Dormouse's story\\n\\nThe Dormouse's story\\nOnce upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.\\n\"\n\n\n\nprint(soup.text)\n\n\nThe Dormouse's story\n\nThe Dormouse's story\nOnce upon a time there were three little sisters; and their names were\nElsie,\nLacie and\nTillie;\nand they lived at the bottom of a well.\n\n\n\n     To see the title of the document, we run the following codes -\n\n# Navigating to Specific Tags \nsoup.head.title\n\n&lt;title&gt;The Dormouse's story&lt;/title&gt;\n\n\n\n# Getting Text from a Specific Tag\nsoup.head.title.text\n\n\"The Dormouse's story\"\n\n\n     To see the text, from a tag, we run the following code -\n\nsoup.body.a.text\n\n'Elsie'\n\n\n     To see the text, from p tag, we run the following code -\n\nsoup.body.p.text\n\n\"The Dormouse's story\"",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#searching-the-elements-of-tags",
    "href": "advanced_analytics.html#searching-the-elements-of-tags",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.5 Searching the Elements of Tags",
    "text": "9.5 Searching the Elements of Tags\n\n     The find_all() function from beautifulsoup takes an HTML tag as an string argument and returns the list of elements that match the tag. For example, if we want to have all a tags in html data above, we will run the following code. Please note that there is another similar function called find(), which will return the first tag element.\n\nsoup.find_all('a')\n\n[&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n &lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt;,\n &lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;]\n\n\n\nsoup.find('a')\n\n&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;\n\n\n     We can also search for tags of a specific class as well by providing class_ argument. Beasutiful soup uses class_ because class is a reserved keyword in python. For example, let’s search for p tags that have element story.\n\nsoup.find_all(\"p\", class_ = \"title\")\n\n[&lt;p class=\"title\"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;]\n\n\n\nsoup.find(\"p\", class_=\"story\")\n\n&lt;p class=\"story\"&gt;Once upon a time there were three little sisters; and their names were\n&lt;a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"&gt;Elsie&lt;/a&gt;,\n&lt;a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"&gt;Lacie&lt;/a&gt; and\n&lt;a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"&gt;Tillie&lt;/a&gt;;\nand they lived at the bottom of a well.&lt;/p&gt;\n\n\n\nsoup.find(\"p\", class_=\"story\").get_text()\n\n'Once upon a time there were three little sisters; and their names were\\nElsie,\\nLacie and\\nTillie;\\nand they lived at the bottom of a well.'",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#sec-usebeautifulsoup",
    "href": "advanced_analytics.html#sec-usebeautifulsoup",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.6 Scrape a Website Using BeautifulSoup",
    "text": "9.6 Scrape a Website Using BeautifulSoup\n\n     We have mastered some basic knowledge of Beautifulsoup. Therefore, it is now time to put our knowledge into practice. We are going to parse a website, which includes information about books. We would like to extract some data from the website. The data include - book url, title of the book, ratings of the book, price, and availability of the book. Before we start scraping the website, we need to identify the tags or CSS selectors that are relevant for our targeted data. Figure 9.3 shows how we can identify the tags or selectors relevant for our search. We should hover our cursor over the information that we plan to extract and then click right button of the mouse (on Windows) and click \"inspect\". Then we can see all tags and CSS selectors and other tags of the website. Figure 9.3 visualizes the whole process.\n\n\n\n\n\n\nFigure 9.3: How to Find the HTML tags and CSS Class\n\n\n\n     First, we need to import necessary python modules. We use requests module to get the website information.\n\n# importing requests \nimport requests\n# importing beautifulsoup\nfrom bs4 import BeautifulSoup\n# importing pandas \nimport pandas as pd\n\n     Then, we convert the data into soup object.\n\n# Fetch the website page \nurl = 'https://books.toscrape.com/catalogue/page-1.html'\nhtml = requests.get(url)\npage = html.text\n# Converting it into Soup Object \nsoup = BeautifulSoup(page, \"html.parser\")\n\n     After inspecting the tags and CSS selectors, we identify that article tag and product_pod class contains the information that we would like to extract. We use the find function from beautifulsuop to see our expected data. As noted before, find function identifies the first instance of the elements whereas find_all identifies all elements of the parsed HTML.\n\nsoup.find(\"article\", class_=\"product_pod\")\n\n&lt;article class=\"product_pod\"&gt;\n&lt;div class=\"image_container\"&gt;\n&lt;a href=\"a-light-in-the-attic_1000/index.html\"&gt;&lt;img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"../media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;p class=\"star-rating Three\"&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;i class=\"icon-star\"&gt;&lt;/i&gt;\n&lt;/p&gt;\n&lt;h3&gt;&lt;a href=\"a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\"&gt;A Light in the ...&lt;/a&gt;&lt;/h3&gt;\n&lt;div class=\"product_price\"&gt;\n&lt;p class=\"price_color\"&gt;Â£51.77&lt;/p&gt;\n&lt;p class=\"instock availability\"&gt;\n&lt;i class=\"icon-ok\"&gt;&lt;/i&gt;\n    \n        In stock\n    \n&lt;/p&gt;\n&lt;form&gt;\n&lt;button class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\" type=\"submit\"&gt;Add to basket&lt;/button&gt;\n&lt;/form&gt;\n&lt;/div&gt;\n&lt;/article&gt;\n\n\n\nsoup.find_all(\"article\", class_=\"product_pod\")\n\n     Next, we check the url of each book. The a tag defines a hyperlink and the href is an attribute of a tag. Below, we use a tag to identify the link of each book.\n\nbooks = soup.find_all(\"article\", class_=\"product_pod\")\n\n\nsource_url = \"https://books.toscrape.com/catalogue\"\n\n\n# Book url \nfor h in soup.find_all(\"article\", class_=\"product_pod\"):\n    print(source_url+\"/\"+h.find('a')['href'])\n\nhttps://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\nhttps://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\nhttps://books.toscrape.com/catalogue/soumission_998/index.html\nhttps://books.toscrape.com/catalogue/sharp-objects_997/index.html\nhttps://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\nhttps://books.toscrape.com/catalogue/the-requiem-red_995/index.html\nhttps://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\nhttps://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\nhttps://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\nhttps://books.toscrape.com/catalogue/the-black-maria_991/index.html\nhttps://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html\nhttps://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html\nhttps://books.toscrape.com/catalogue/set-me-free_988/index.html\nhttps://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\nhttps://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html\nhttps://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\nhttps://books.toscrape.com/catalogue/olio_984/index.html\nhttps://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\nhttps://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html\nhttps://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html\n\n\n\n# Book url (Alternative) \nfor h in soup.find_all(\"article\", class_=\"product_pod\"):\n    print(h.h3.find('a')['href'])\n\na-light-in-the-attic_1000/index.html\ntipping-the-velvet_999/index.html\nsoumission_998/index.html\nsharp-objects_997/index.html\nsapiens-a-brief-history-of-humankind_996/index.html\nthe-requiem-red_995/index.html\nthe-dirty-little-secrets-of-getting-your-dream-job_994/index.html\nthe-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\nthe-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\nthe-black-maria_991/index.html\nstarving-hearts-triangular-trade-trilogy-1_990/index.html\nshakespeares-sonnets_989/index.html\nset-me-free_988/index.html\nscott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\nrip-it-up-and-start-again_986/index.html\nour-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\nolio_984/index.html\nmesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\nlibertarianism-for-beginners_982/index.html\nits-only-the-himalayas_981/index.html\n\n\n\n# Book Title \nfor h in soup.find_all(\"article\", class_=\"product_pod\"):\n    print(h.h3.find('a')['title'])\n\nA Light in the Attic\nTipping the Velvet\nSoumission\nSharp Objects\nSapiens: A Brief History of Humankind\nThe Requiem Red\nThe Dirty Little Secrets of Getting Your Dream Job\nThe Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\nThe Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\nThe Black Maria\nStarving Hearts (Triangular Trade Trilogy, #1)\nShakespeare's Sonnets\nSet Me Free\nScott Pilgrim's Precious Little Life (Scott Pilgrim #1)\nRip it Up and Start Again\nOur Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\nOlio\nMesaerion: The Best Science Fiction Stories 1800-1849\nLibertarianism for Beginners\nIt's Only the Himalayas\n\n\n\n# ratings \nsoup.find('p', class_='star-rating')['class'][1]\n\n'Three'\n\n\n\n# price \nsoup.find('p', class_='price_color').get_text().replace(\"Â\",'')\n\n'£51.77'\n\n\n\n# availability \nsoup.find('p', class_='instock availability').get_text().replace('\\n','').strip()\n\n'In stock'\n\n\n\n9.6.1 Putting All of the Above Actions Together\n     In Section 9.6, we identify and extract individual tags and data that we want to extract. Now, we will put all of them together and create a data frame. For this purpose, we will use for loop.\n\n# Fetch the Page \nurl = 'https://books.toscrape.com/catalogue/page-1.html'\nhtml = requests.get(url)\npage = html.text\n# Parse HTML Content\nsoup = BeautifulSoup(page, \"html.parser\")\n\n# Information We need \n\nbook_url = []\ntitle = []\nratings = []\nprice = []\navailability = []\n\n# Extract listings from the page\nbooks = soup.find_all(\"article\", class_=\"product_pod\")\nsource_url = \"https://books.toscrape.com/catalogue\"\n\nfor book in books:\n    # extract book url \n    book_url_text = source_url+\"/\"+book.find('a')['href']\n    book_url.append(book_url_text)\n\n    # extract title \n    title_text = book.h3.find('a')['title']\n    title.append(title_text)\n\n    # extract ratings \n    ratings_text = book.find('p', class_='star-rating')['class'][1]\n    ratings.append(ratings_text)\n\n    # extract price \n    price_text = book.find('p', class_='price_color').get_text().replace(\"Â\",'')\n    price.append(price_text)\n\n    # extract availability \n    availability_text = book.find('p', class_='instock availability').get_text().replace('\\n','').strip()\n    availability.append(availability_text)\n\n# Creating the Data Frame \n\npd.DataFrame({\n    'book_url':book_url,\n    'title':title,\n    'ratings':ratings,\n    'price':price,\n    'availability':availability\n})\n\n\n\n\n\n\n\n\n\nbook_url\ntitle\nratings\nprice\navailability\n\n\n\n\n0\nhttps://books.toscrape.com/catalogue/a-light-i...\nA Light in the Attic\nThree\n£51.77\nIn stock\n\n\n1\nhttps://books.toscrape.com/catalogue/tipping-t...\nTipping the Velvet\nOne\n£53.74\nIn stock\n\n\n2\nhttps://books.toscrape.com/catalogue/soumissio...\nSoumission\nOne\n£50.10\nIn stock\n\n\n3\nhttps://books.toscrape.com/catalogue/sharp-obj...\nSharp Objects\nFour\n£47.82\nIn stock\n\n\n4\nhttps://books.toscrape.com/catalogue/sapiens-a...\nSapiens: A Brief History of Humankind\nFive\n£54.23\nIn stock\n\n\n5\nhttps://books.toscrape.com/catalogue/the-requi...\nThe Requiem Red\nOne\n£22.65\nIn stock\n\n\n6\nhttps://books.toscrape.com/catalogue/the-dirty...\nThe Dirty Little Secrets of Getting Your Dream...\nFour\n£33.34\nIn stock\n\n\n7\nhttps://books.toscrape.com/catalogue/the-comin...\nThe Coming Woman: A Novel Based on the Life of...\nThree\n£17.93\nIn stock\n\n\n8\nhttps://books.toscrape.com/catalogue/the-boys-...\nThe Boys in the Boat: Nine Americans and Their...\nFour\n£22.60\nIn stock\n\n\n9\nhttps://books.toscrape.com/catalogue/the-black...\nThe Black Maria\nOne\n£52.15\nIn stock\n\n\n10\nhttps://books.toscrape.com/catalogue/starving-...\nStarving Hearts (Triangular Trade Trilogy, #1)\nTwo\n£13.99\nIn stock\n\n\n11\nhttps://books.toscrape.com/catalogue/shakespea...\nShakespeare's Sonnets\nFour\n£20.66\nIn stock\n\n\n12\nhttps://books.toscrape.com/catalogue/set-me-fr...\nSet Me Free\nFive\n£17.46\nIn stock\n\n\n13\nhttps://books.toscrape.com/catalogue/scott-pil...\nScott Pilgrim's Precious Little Life (Scott Pi...\nFive\n£52.29\nIn stock\n\n\n14\nhttps://books.toscrape.com/catalogue/rip-it-up...\nRip it Up and Start Again\nFive\n£35.02\nIn stock\n\n\n15\nhttps://books.toscrape.com/catalogue/our-band-...\nOur Band Could Be Your Life: Scenes from the A...\nThree\n£57.25\nIn stock\n\n\n16\nhttps://books.toscrape.com/catalogue/olio_984/...\nOlio\nOne\n£23.88\nIn stock\n\n\n17\nhttps://books.toscrape.com/catalogue/mesaerion...\nMesaerion: The Best Science Fiction Stories 18...\nOne\n£37.59\nIn stock\n\n\n18\nhttps://books.toscrape.com/catalogue/libertari...\nLibertarianism for Beginners\nTwo\n£51.33\nIn stock\n\n\n19\nhttps://books.toscrape.com/catalogue/its-only-...\nIt's Only the Himalayas\nTwo\n£45.17\nIn stock\n\n\n\n\n\n\n\n\n\n\n9.6.2 Doing the Same Things for All Pages\n     In Section 9.6.1, we scrape the first page of the website, but now we would like to scrape all pages of the website.\n\nurl1 = 'https://books.toscrape.com/catalogue/page-'\npages = range(51)\nurl2 = '.html'\n\n# Information We need \nbook_url = []\ntitle = []\nratings = []\nprice = []\navailability = []\n# Some other Information \nsource_url = \"https://books.toscrape.com/catalogue\"\n\nfor page in pages:\n    url = url1+str(page)+url2\n    r = requests.get(url)\n    soup = BeautifulSoup(r.text, 'html.parser')\n    books = soup.find_all(\"article\", class_=\"product_pod\")\n\n    for book in books:\n        # extract book url \n        book_url_text = source_url+\"/\"+book.find('a')['href']\n        book_url.append(book_url_text)\n\n        # extract title \n        title_text = book.h3.find('a')['title']\n        title.append(title_text)\n\n        # extract ratings \n        ratings_text = book.find('p', class_='star-rating')['class'][1]\n        ratings.append(ratings_text)\n\n        # extract price \n        price_text = book.find('p', class_='price_color').get_text().replace(\"Â\",'')\n        price.append(price_text)\n\n        # extract availability \n        availability_text = book.find('p', class_='instock availability').get_text().replace('\\n','').strip()\n        availability.append(availability_text)\n    \n\n\n\n# Creating the Data Frame \npd.DataFrame({\n    'book_url':book_url,\n    'title':title,\n    'ratings':ratings,\n    'price':price,\n    'availability':availability\n})\n\n\n\n\n\n\n\n\n\nbook_url\ntitle\nratings\nprice\navailability\n\n\n\n\n0\nhttps://books.toscrape.com/catalogue/a-light-i...\nA Light in the Attic\nThree\n£51.77\nIn stock\n\n\n1\nhttps://books.toscrape.com/catalogue/tipping-t...\nTipping the Velvet\nOne\n£53.74\nIn stock\n\n\n2\nhttps://books.toscrape.com/catalogue/soumissio...\nSoumission\nOne\n£50.10\nIn stock\n\n\n3\nhttps://books.toscrape.com/catalogue/sharp-obj...\nSharp Objects\nFour\n£47.82\nIn stock\n\n\n4\nhttps://books.toscrape.com/catalogue/sapiens-a...\nSapiens: A Brief History of Humankind\nFive\n£54.23\nIn stock\n\n\n...\n...\n...\n...\n...\n...\n\n\n995\nhttps://books.toscrape.com/catalogue/alice-in-...\nAlice in Wonderland (Alice's Adventures in Won...\nOne\n£55.53\nIn stock\n\n\n996\nhttps://books.toscrape.com/catalogue/ajin-demi...\nAjin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)\nFour\n£57.06\nIn stock\n\n\n997\nhttps://books.toscrape.com/catalogue/a-spys-de...\nA Spy's Devotion (The Regency Spies of London #1)\nFive\n£16.97\nIn stock\n\n\n998\nhttps://books.toscrape.com/catalogue/1st-to-di...\n1st to Die (Women's Murder Club #1)\nOne\n£53.98\nIn stock\n\n\n999\nhttps://books.toscrape.com/catalogue/1000-plac...\n1,000 Places to See Before You Die\nFive\n£26.08\nIn stock\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n\n\n9.7 Conclusion",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "advanced_analytics.html#conclusion",
    "href": "advanced_analytics.html#conclusion",
    "title": "9  Web Scraping and Textual Analytics",
    "section": "9.7 Conclusion",
    "text": "9.7 Conclusion",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Web Scraping and Textual Analytics</span>"
    ]
  },
  {
    "objectID": "fraud.html",
    "href": "fraud.html",
    "title": "10  Fraud Detection and Risk Management",
    "section": "",
    "text": "10.1 Techniques for Fraud Detection",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fraud Detection and Risk Management</span>"
    ]
  },
  {
    "objectID": "fraud.html#risk-assessment-models",
    "href": "fraud.html#risk-assessment-models",
    "title": "10  Fraud Detection and Risk Management",
    "section": "10.2 Risk Assessment Models",
    "text": "10.2 Risk Assessment Models",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fraud Detection and Risk Management</span>"
    ]
  },
  {
    "objectID": "fraud.html#case-studies-in-fraud-detection",
    "href": "fraud.html#case-studies-in-fraud-detection",
    "title": "10  Fraud Detection and Risk Management",
    "section": "10.3 Case Studies in Fraud Detection",
    "text": "10.3 Case Studies in Fraud Detection",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fraud Detection and Risk Management</span>"
    ]
  },
  {
    "objectID": "performance_measurement.html",
    "href": "performance_measurement.html",
    "title": "11  Performance Measurement and Management",
    "section": "",
    "text": "11.1 Key Performance Indicators (KPIs)",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Performance Measurement and Management</span>"
    ]
  },
  {
    "objectID": "performance_measurement.html#balanced-scorecard",
    "href": "performance_measurement.html#balanced-scorecard",
    "title": "11  Performance Measurement and Management",
    "section": "11.2 Balanced Scorecard",
    "text": "11.2 Balanced Scorecard",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Performance Measurement and Management</span>"
    ]
  },
  {
    "objectID": "performance_measurement.html#financial-ratio-analysis",
    "href": "performance_measurement.html#financial-ratio-analysis",
    "title": "11  Performance Measurement and Management",
    "section": "11.3 Financial Ratio Analysis",
    "text": "11.3 Financial Ratio Analysis",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Performance Measurement and Management</span>"
    ]
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "12  Regulatory and Ethical Considerations",
    "section": "",
    "text": "12.1 Compliance and Regulatory Requirements",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Regulatory and Ethical Considerations</span>"
    ]
  },
  {
    "objectID": "ethics.html#ethichal-use-of-data",
    "href": "ethics.html#ethichal-use-of-data",
    "title": "12  Regulatory and Ethical Considerations",
    "section": "12.2 Ethichal Use of Data",
    "text": "12.2 Ethichal Use of Data",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Regulatory and Ethical Considerations</span>"
    ]
  },
  {
    "objectID": "ethics.html#data-privacy-security",
    "href": "ethics.html#data-privacy-security",
    "title": "12  Regulatory and Ethical Considerations",
    "section": "12.3 Data Privacy & Security",
    "text": "12.3 Data Privacy & Security",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Regulatory and Ethical Considerations</span>"
    ]
  },
  {
    "objectID": "future.html",
    "href": "future.html",
    "title": "13  Future Directions in Accounting Analytics",
    "section": "",
    "text": "13.1 Emerging Technologies",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Future Directions in Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "future.html#trends-predictions",
    "href": "future.html#trends-predictions",
    "title": "13  Future Directions in Accounting Analytics",
    "section": "13.2 Trends & Predictions",
    "text": "13.2 Trends & Predictions",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Future Directions in Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "future.html#preparing-for-the-future-accounting-analytics",
    "href": "future.html#preparing-for-the-future-accounting-analytics",
    "title": "13  Future Directions in Accounting Analytics",
    "section": "13.3 Preparing for the Future Accounting Analytics",
    "text": "13.3 Preparing for the Future Accounting Analytics",
    "crumbs": [
      "Practical Considerations and Future Directions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Future Directions in Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alles, Michael G. 2015. “Drivers of the Use and Facilitators and\nObstacles of the Evolution of Big Data by the Audit Profession.”\nAccounting Horizons 29 (2): 439–49. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/439/2188.\n\n\nAlles, Michael, and Glen L. Gray. 2016. “Incorporating Big Data in\nAudits: Identifying Inhibitors and a Research Agenda to\nAddress Those Inhibitors.” International Journal of\nAccounting Information Systems 22: 44–59. https://www.sciencedirect.com/science/article/pii/S1467089516300811.\n\n\nAmerican Institute of Certified Public Accountants (AICPA). 2015.\n“Audit Analytics and Continuous\nAudit: Looking Toward the\nFuture.”\n\n\n———. 2017. “Description Criteria for\nManagement’s Description of the\nEntity’s Cybersecurity Risk\nManagement Program.”\n\n\nAppelbaum, Deniz. 2016. “Securing Big Data Provenance for\nAuditors: The Big Data Provenance Black Box as Reliable\nEvidence.” Journal of Emerging Technologies in\nAccounting 13 (1): 17–36. https://publications.aaahq.org/jeta/article-abstract/13/1/17/9219.\n\n\nAppelbaum, Deniz, Alexander Kogan, and Miklos A. Vasarhelyi. 2017.\n“Big Data and Analytics in the Modern Audit\nEngagement: Research Needs.” Auditing: A Journal\nof Practice & Theory 36 (4): 1–27. https://publications.aaahq.org/ajpt/article-abstract/36/4/1/6016.\n\n\nBarr-Pulliam, Dereck, Helen L. Brown-Liburd, and Amanda G. Carlson.\n2023. “Do Audit Data\nAnalytics Influence Juror\nPerceptions of Audit Quality and\nAuditor Negligence?” Current Issues\nin Auditing 17 (2): P1–10. https://publications.aaahq.org/cia/article/17/2/P1/10096.\n\n\nBarton, Dominic, and David Court. 2012. “Making Advanced Analytics\nWork for You.” Harvard Business Review 90 (10): 78–83.\nhttp://www.buyukverienstitusu.com/s/1870/i/Making_Advanced_Analytics_Work_For_You.pdf.\n\n\nBollen, Johan, Huina Mao, and Xiaojun Zeng. 2011. “Twitter Mood\nPredicts the Stock Market.” Journal of Computational\nScience 2 (1): 1–8. https://www.sciencedirect.com/science/article/pii/S187775031100007X.\n\n\nCao, Min, Roman Chychyla, and Trevor Stewart. 2015. “Big Data\nAnalytics in Financial Statement Audits.” Accounting\nHorizons 29 (2): 423–29. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/423/2177.\n\n\nColumbus. 2017. “53% Of Companies\nAre Adopting Big\nData Analytics.” Forbes. https://www.forbes.com/sites/louiscolumbus/2017/12/24/53-of-companies-are-adopting-big-data-analytics/?sh=6c98f39939a1.\n\n\nCrawley, Michael, and James Wahlen. 2014. “Analytics in\nEmpirical/Archival Financial Accounting Research.” Business\nHorizons 57 (5): 583–93. https://www.sciencedirect.com/science/article/pii/S0007681314000792.\n\n\nDai, Jun, and Miklos A. Vasarhelyi. 2016. “Imagineering\nAudit 4.0.” Journal of Emerging Technologies in\nAccounting 13 (1): 1–15. https://publications.aaahq.org/jeta/article-abstract/13/1/1/9242.\n\n\nDavis, Angela K., Jeremy M. Piger, and Lisa M. Sedor. 2012.\n“Beyond the Numbers: Measuring the\nInformation Content of Earnings\nPress Release Language.”\nContemporary Accounting Research 29 (3): 845–68. https://doi.org/10.1111/j.1911-3846.2011.01130.x.\n\n\nDeloitte. 2016. “Tax Data Analytics\nA New Era for Tax\nPlanning and Compliance.” https://www2.deloitte.com/content/dam/Deloitte/us/Documents/Tax/us-tax-data-analytics-a-new-era-for-tax-planning-and-compliance.pdf.\n\n\nFeldman, Ronen, Suresh Govindaraj, Joshua Livnat, and Benjamin Segal.\n2010. “Management’s Tone Change, Post Earnings Announcement Drift\nand Accruals.” Review of Accounting Studies 15 (4):\n915–53. https://doi.org/10.1007/s11142-009-9111-x.\n\n\nKrahel, John Peter, and William R. Titera. 2015. “Consequences of\nBig Data and Formalization on Accounting and Auditing Standards.”\nAccounting Horizons 29 (2): 409–22. https://publications.aaahq.org/accounting-horizons/article/29/2/409/2149.\n\n\nLehavy, Reuven, Feng Li, and Kenneth Merkley. 2011. “The Effect of\nAnnual Report Readability on Analyst Following and the Properties of\nTheir Earnings Forecasts.” The Accounting Review 86 (3):\n1087–1115. https://publications.aaahq.org/accounting-review/article-abstract/86/3/1087/3300.\n\n\nLi, Feng. 2008. “Annual Report Readability, Current Earnings, and\nEarnings Persistence.” Journal of Accounting and\nEconomics 45 (2-3): 221–47. https://www.sciencedirect.com/science/article/pii/S0165410108000141.\n\n\n———. 2010. “The Information Content of\nForward‐Looking Statements in\nCorporate Filings—A\nNaïve Bayesian Machine\nLearning Approach.” Journal of\nAccounting Research 48 (5): 1049–1102. https://doi.org/10.1111/j.1475-679X.2010.00382.x.\n\n\nLi, Feng, Russell Lundholm, and Michael Minnis. 2013. “A\nMeasure of Competition Based on\n10‐K Filings.” Journal of\nAccounting Research 51 (2): 399–436. https://doi.org/10.1111/j.1475-679X.2012.00472.x.\n\n\nProtiviti. 2017. “Embracing Analytics in\nAuditing.” https://www.protiviti.com/sites/default/files/2022-06/2017-internal-audit-capabilities-and-needs-survey-protiviti.pdf.\n\n\nProvost, Foster, and Tom Fawcett. 2013. “Data Science\nand Its Relationship to Big Data\nand Data-Driven Decision\nMaking.” Big Data 1 (1): 51–59. https://doi.org/10.1089/big.2013.1508.\n\n\nRichins, Greg, Andrea Stapleton, Theophanis C. Stratopoulos, and\nChristopher Wong. 2017. “Big Data Analytics: Opportunity or Threat\nfor the Accounting Profession?” Journal of Information\nSystems 31 (3): 63–79. https://publications.aaahq.org/jis/article-abstract/31/3/63/1114.\n\n\nRose, Anna M., Jacob M. Rose, Kerri-Ann Sanderson, and Jay C. Thibodeau.\n2017. “When Should Audit Firms Introduce Analyses of Big Data into\nthe Audit Process?” Journal of Information Systems 31\n(3): 81–99. https://publications.aaahq.org/jis/article-abstract/31/3/81/1123.\n\n\nSchneider, Gary P., Jun Dai, Diane J. Janvrin, Kemi Ajayi, and Robyn L.\nRaschke. 2015. “Infer, Predict, and Assure:\nAccounting Opportunities in Data Analytics.”\nAccounting Horizons 29 (3): 719–42. https://publications.aaahq.org/accounting-horizons/article-abstract/29/3/719/2262.\n\n\nSivarajah, Uthayasankar, Muhammad Mustafa Kamal, Zahir Irani, and\nVishanth Weerakkody. 2017. “Critical Analysis of Big\nData Challenges and Analytical Methods.” Journal\nof Business Research 70: 263–86. https://www.sciencedirect.com/science/article/pii/S014829631630488X.\n\n\nThe Economist. 2017. “The World’s Most Valuable Resource Is No\nLonger Oil, but Data.” https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data.\n\n\nVasarhelyi, Miklos A., Alexander Kogan, and Brad M. Tuttle. 2015.\n“Big Data in Accounting: An Overview.”\nAccounting Horizons 29 (2): 381–96. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/381/2184.\n\n\nVerver, John. 2015. “Six Audit Analytics\nSuccess Factors.” Internal\nAuditor 72 (3).\n\n\nWarren, J. Donald, Kevin C. Moffitt, and Paul Byrnes. 2015. “How\nBig Data Will Change Accounting.” Accounting Horizons 29\n(2): 397–407. https://publications.aaahq.org/accounting-horizons/article/29/2/397/2168.\n\n\nYoon, Kyunghee, Lucas Hoogduin, and Li Zhang. 2015. “Big Data as\nComplementary Audit Evidence.” Accounting Horizons 29\n(2): 431–38. https://publications.aaahq.org/accounting-horizons/article/29/2/431/2215.\n\n\nZhang, Juan, Xiongsheng Yang, and Deniz Appelbaum. 2015. “Toward\nEffective Big Data Analysis in Continuous Auditing.”\nAccounting Horizons 29 (2): 469–76. https://publications.aaahq.org/accounting-horizons/article/29/2/469/2160.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "dashboard.html",
    "href": "dashboard.html",
    "title": "5  Dashboard for Visualization",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#what-is-dashboard",
    "href": "dashboard.html#what-is-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.1 What is Dashboard?",
    "text": "5.1 What is Dashboard?\n\n     A dashboard for visualization is a user interface that displays a collection of visual data representations, such as charts, graphs, tables, and metrics, to provide users with an interactive and comprehensive overview of key information. Dashboards are commonly used in business, data science, finance, healthcare, and other fields to monitor performance, track metrics, and explore data trends in real time.\n     The main goal of a dashboard is to present complex data in an easy-to-understand format, enabling users to quickly grasp insights and make data-driven decisions. Dashboards often combine multiple visual elements into a single screen or page, allowing users to view different aspects of the data simultaneously. They typically include interactive features like filters, drill-downs, and tooltips, which allow users to interact with the data and explore deeper insights without needing to understand the underlying data structures. Some benefits of dashboards are -\n\nData Aggregation: Dashboards bring together data from various sources, providing a unified view of different datasets.\nVisualization Elements: They use visual elements such as bar charts, line graphs, pie charts, heatmaps, and more to represent data in a visually appealing and informative way.\nInteractivity: Users can interact with the visual elements, applying filters, adjusting time frames, or drilling down into specific data points to gain more detailed insights.\nReal-Time Data: Dashboards can display real-time data, updating visualizations dynamically to reflect the latest information, which is especially useful for monitoring live systems or business performance.\nCustomization: Dashboards are highly customizable, allowing users to tailor the layout, visualizations, and data to meet their specific needs.\n\n     Dashboards for visualization are widely used for performance monitoring (e.g., tracking KPIs), data exploration (e.g., identifying trends), and decision-making (e.g., comparing metrics). They make data analysis more accessible to a wider audience by simplifying the complexity of raw data into clear and actionable insights.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#importance-of-dashboard",
    "href": "dashboard.html#importance-of-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.2 Importance of Dashboard",
    "text": "5.2 Importance of Dashboard",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#interactive-dashboard",
    "href": "dashboard.html#interactive-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.3 Interactive Dashboard",
    "text": "5.3 Interactive Dashboard\n     For interactive visualization, see Figure 5.3.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#quarto---r-and-python-dashboard",
    "href": "dashboard.html#quarto---r-and-python-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.4 Quarto - R and Python Dashboard",
    "text": "5.4 Quarto - R and Python Dashboard\n\n     Quarto Dashboard is a powerful and flexible open-source tool to create interactive dashboard in R or Python. Quarto dashboards are easy to create and support a wide variety of visualization and interactive components1. More about quarto dashboard can be learned from Quarto Dashboard Website. To learn more about interactivity on quarto dashboard using Shiny, please visit webpages for R and Python. However, it is recommended to use Python for quarto dashboard if you want to include interactive applications on your dashboard.\n     There are several components of quarto dashboard:\n\nNavigation Bar - Top page bar with icon, title of the dashboard, name of author and links to sub-pages\nPages, Rows, Columns, and Tabsets - Using markdown headings (#) - pages, rows and columns are defined. Tabsets are used to further divide the content within a row or column\nCards, Sidebar, and Toolbars - Cards are containers for text, images, charts, and interactive elements and useful for organizing information into distinct sections within a dashboard. Typically, the contents of cards map to cells in the dashboard. Sidebar is another layout component of quarto dashboard, which contain navigation menus, filters, and controls that allow users to adjust or explore the data presented in the main content. Toolbars ….\n\n     The first step to create a quarto dashboard is to structure YAML in .qmd file. A quarto dashboard YAML look like -\n\n\n\n\n\n\nFigure 5.1: Quarto Dashboard YAML\n\n\n\n     In quarto dashboard, each level 1 header (#) introduces a new page, each level 2 header (##) introduces a new row within the given page, and each code chunk within a given row introduces a new column.\n\n\n\n\n\n\nFigure 5.2: Quarto Dashboard Structure\n\n\n\n     Some other attributes (Table 5.1) that can be added to the quarto dashboard’s rows or columns include -\n\n\n\nTable 5.1: Some Additional Attributes\n\n\n\n\n\n\n\n\n\nAttribute\nExplanation\n\n\n\n\n{width=} and {height=}\nSet the size of columns, rows, and boxes\n\n\n{orientation=}\nsets the dashboard layout to either rows or columns. This is a global option set in the YAML. However, if your dashboard has multiple pages and you want to specify the orientation for each page, remove orientation: from the YAML and use this attribute instead\n\n\n{.tabset}\ndivide columns, rows, or charts into tabs\n\n\n{.sidebar}\ntypically creates a sidebar on the page\n\n\n{.hidden}\nexcludes a specific page from the navigation bar",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#vizro---python-dashboard",
    "href": "dashboard.html#vizro---python-dashboard",
    "title": "5  Dashboard for Visualization",
    "section": "5.5 Vizro - Python Dashboard",
    "text": "5.5 Vizro - Python Dashboard\n\n     Built on top of Dash and Plottly, Vizro is a powerful python module to create a dashboard. A vizro dashboard consists of several objects. The first object is Page. Each page contains several other sub-objects such as Comonents, which can include Graphs and Tables, Filters, which can be sliders, dropdown boxes and other buttons, and optional Actions. To learn more about Vizro, we can explore Vizro document website and developer website. The key benefits of Vizro include:\n\nLow-code and Configuration - Vizro only needs a few lines of code code, thus replacing thousand lines of codes and saving valuable time\nIn-built Best Practices - Vizro already incorporates standards for visual design and software development.\nSelf-service Visualization - Vizro readily assemble dashboards without advanced design or coding experience\nOptional High-code Extensions - Vizro enables limitless customization for advanced users with complex needs\nModularity - Vizro leverage components that are simple to swap, reuse, maintain, share and scale\nFlexibilit and Scalability - Vizro enables data science ready and to develop python based data visualization applications\n\n\n\n# Dashboard Creation \nfrom vizro import Vizro\nimport vizro.models as vm\nimport vizro.plotly.express as px\n\n# Data Visualization \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\n# loading dataset \nimport palmerpenguins\ndf = palmerpenguins.load_penguins()\n\n\n5.5.1 Standalone Page on Vizro Dashboard (Example # 01)\n\nVizro._reset()\n\nfirst_page = vm.Page(\n    title= \" \", \n    components= [\n        vm.Graph(\n            id = 'boxplot', \n            figure = px.box (df, x = 'species', y = 'bill_length_mm', \n            color = 'species', \n            labels={'species':'Species', 'bill_length_mm':'Bill Length (mm)'})\n        ),\n    ],\n)\n\ndashboard = vm.Dashboard(pages=[first_page])\nVizro().build(dashboard).run()\n\n\n\n\n        \n        \n\n\nFigure 5.3: A Sample of Vizro Dashboard\n\n\n\n\n\n\n5.5.2 Standalone Page on Vizro Dashboard (Example # 02)\n\n# loading dataset \ndf = px.data.gapminder()\ngapminder_data = (\n        df.groupby(by=[\"continent\", \"year\"]).\n            agg({\"lifeExp\": \"mean\", \"pop\": \"sum\", \"gdpPercap\": \"mean\"}).reset_index()\n    )\n\n\nVizro._reset()\nsecond_page = vm.Page(\n    title=\"First Page\",\n    components=[\n        vm.Card(\n            text=\"\"\"\n                # First dashboard page\n                This pages shows the inclusion of markdown text in a page and how components\n                can be structured using Layout.\n            \"\"\",\n        ),\n        vm.Graph(\n            figure=px.box(gapminder_data, x=\"continent\", y=\"lifeExp\", color=\"continent\",\n                            labels={\"lifeExp\": \"Life Expectancy\", \"continent\": \"Continent\"}),\n        ),\n        vm.Graph(\n            figure=px.line(gapminder_data, x=\"year\", y=\"gdpPercap\", color=\"continent\",\n                            labels={\"year\": \"Year\", \"continent\": \"Continent\",\n                            \"gdpPercap\":\"GDP Per Cap\"}, title=''),\n        ),\n\n    ],\n)\n\ndashboard2 = vm.Dashboard(pages=[second_page])\nVizro().build(dashboard2).run()\n\n\n\n5.5.3 Multiple Pages on Vizro Dashboard\n\nVizro._reset()\n\nthird_page = vm.Page(\n    title=\"First Page\",\n    layout=vm.Layout(grid=[[0, 0], [1, 2], [1, 2], [1, 2]]),\n    components=[\n        vm.Card(\n            text=\"\"\"\n                # First dashboard page\n                This pages shows the inclusion of markdown text in a page and how components\n                can be structured using Layout.\n            \"\"\",\n        ),\n        vm.Graph(\n            figure=px.box(gapminder_data, x=\"continent\", y=\"lifeExp\", color=\"continent\",\n                            labels={\"lifeExp\": \"Life Expectancy\", \"continent\": \"Continent\"}),\n        ),\n        vm.Graph(\n            figure=px.line(gapminder_data, x=\"year\", y=\"gdpPercap\", color=\"continent\",\n                            labels={\"year\": \"Year\", \"continent\": \"Continent\",\n                            \"gdpPercap\":\"GDP Per Cap\"}),\n            ),\n    ],\n)\n\ndashboard3 = vm.Dashboard(pages=[third_page])\nVizro().build(dashboard3).run()\n\n\nVizro._reset()\n\nsecond_page = vm.Page(\n    title=\"Second Page\",\n    components=[\n        vm.Card(\n            text=\"\"\"\n                # First dashboard page\n                This pages shows the inclusion of markdown text in a page and how components\n                can be structured using Layout.\n            \"\"\",\n        ),\n        vm.Graph(\n            figure=px.box(gapminder_data, x=\"continent\", y=\"lifeExp\", color=\"continent\",\n                            labels={\"lifeExp\": \"Life Expectancy\", \"continent\": \"Continent\"}),\n        ),\n        vm.Graph(\n            figure=px.line(gapminder_data, x=\"year\", y=\"gdpPercap\", color=\"continent\",\n                            labels={\"year\": \"Year\", \"continent\": \"Continent\",\n                            \"gdpPercap\":\"GDP Per Cap\"}, title=''),\n        ),\n\n    ],\n)\n\nthird_page = vm.Page(\n    title=\"Third Page\",\n    layout=vm.Layout(grid=[[0, 0], [1, 2], [1, 2], [1, 2]]),\n    components=[\n        vm.Card(\n            text=\"\"\"\n                # First dashboard page\n                This pages shows the inclusion of markdown text in a page and how components\n                can be structured using Layout.\n            \"\"\",\n        ),\n        vm.Graph(\n            figure=px.box(gapminder_data, x=\"continent\", y=\"lifeExp\", color=\"continent\",\n                            labels={\"lifeExp\": \"Life Expectancy\", \"continent\": \"Continent\"}),\n        ),\n        vm.Graph(\n            figure=px.line(gapminder_data, x=\"year\", y=\"gdpPercap\", color=\"continent\",\n                            labels={\"year\": \"Year\", \"continent\": \"Continent\",\n                            \"gdpPercap\":\"GDP Per Cap\"}),\n            ),\n    ],\n)\n\ndashboard4 = vm.Dashboard(pages=[second_page, third_page])\nVizro().build(dashboard4).run()",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#conclusions",
    "href": "dashboard.html#conclusions",
    "title": "5  Dashboard for Visualization",
    "section": "5.6 Conclusions",
    "text": "5.6 Conclusions",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#exercises",
    "href": "dashboard.html#exercises",
    "title": "5  Dashboard for Visualization",
    "section": "5.7 Exercises",
    "text": "5.7 Exercises\n\nCreate a dashboard from Adidas (Ticker:ADR) sales data (Adidas US Sales Datasets.csv).",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "dashboard.html#footnotes",
    "href": "dashboard.html#footnotes",
    "title": "5  Dashboard for Visualization",
    "section": "",
    "text": "Shiny widgets and functionality can be incorporated in the quarto dashboard. Therefore, quarto dashboard is a powerful tool for creating interactive visualization.↩︎",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Dashboard for Visualization</span>"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "1  Overview of Accounting Analytics",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#data-analytics-big-data",
    "href": "overview.html#data-analytics-big-data",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.1 Data Analytics & Big Data",
    "text": "1.1 Data Analytics & Big Data\n\n     Given the availability of vast amount of data, companies in numerous industries exploit such data for competitive advantage, aiming to either increase revenues or decrease costs. Data Driven Decisions (DDD) are making significant differences in productivity, on Return on Assets (ROA), Return on Equity (ROE), asset utilization, and on market value (Provost and Fawcett 2013). Firms using data analytics in their operations can outperform their competitors by 5% in productivity and 6% in profitability (Barton and Court 2012). In 2017, 53% companies have adopted big data, as compared to only 17% in 2015 (Columbus 2017). Additionally, regulators are increasingly calling for organizations to use analytics (Protiviti 2017). This evolving landascape in industry emphasizes the significance of data analytics in organizations.\n     Analytics is a means of extracting value from data. Analytics is the assessment of data with technology tools. Today, there are more powerful analytics tools to more efficiently and effectively analyze a broader range of data and types of data than in the past. Thus, there is an increased opportunity for enhanced insights about what stories data can tell to address business issues and transform the way decisions are made.\n     The meaning of (big) data analytics varies across different disciplines and there is substantive confusion between the slightly differing characterizations of “big data,” “business intelligence,” and “data analytics” (Vasarhelyi, Kogan, and Tuttle 2015). Though many people consider big data in terms of quantities, it is also related to large-scale analysis of large amounts of data to generate insights and knowledge (Verver 2015). Big data is characterized by four Vs: Volume; Velocity; Variety; and Veracity. Volume refers to the size of the dataset, velocity to the speed of data generation, variety to the multiplicity of data sources, and veracity to the elimination of noise and obtaining truthful information from big data. Sometimes big data are characterized by six Vs: Volume, Velocity, Variety, Veracity, Variability, and Value; or, even seven Vs: Volume, Velocity, Variety, Veracity, Variability, Value, and Visualization (Sivarajah et al. 2017). Some people also identify sometimes eight Vs for big data. Figure 1.1 and Figure 1.2 depict the six and eight Vs of big data respectively.\n\n\n\n\n\n\nFigure 1.1: Six Vs of Big Data\n\n\n\n     Data analytics is defined as “the art and science of discovering and analyzing patterns, identifying anomalies, and extracting other useful information in data underlying or related to the subject matter of an audit through analysis, modeling, and visualization for the purpose of planning or performing the audit” (American Institute of Certified Public Accountants (AICPA) 2015, 105). Cao, Chychyla, and Stewart (2015) define big data analytics as the process of inspecting, cleaning, transforming, and modeling big data to discover and communicate useful information and patterns, suggest conclusions, and to provide support for decision-making.\n\n\n\n\n\n\nFigure 1.2: Eight Vs of Big Data\n\n\n\n      Data analytics promises significant potential in auditing. Therefore, in accounting, sometimes data analytics becomes synonymous with audit analytics. Audit analytics involves the application of data analytics in the audit. Specifically, American Institute of Certified Public Accountants (AICPA) (2017) defines audit data analytics as “the science and art of discovering and analyzing patterns, identifying anomalies and extracting other useful information in data underlying or related to the subject matter of an audit through analysis, modeling and visualization for the purpose of planning or performing the audit.” In other words, audit data analytics are techniques that can be used to perform a number of audit procedures such as risk assessment, tests of details, and substantive analytical procedure to gather audit evidence. The benefits of using audit data analytics include improved understanding of an entity’s operations and associated risk including the risk of fraud, increased potential for detecting material misstatements, and improved communications with those charged with governance of audited entities.\n\n\n1.1.1 Big Data Spectrum\n\n     The data dynamics and the way businesses are using data are relatively new to the world of business and something that accounting and business students must become more familiar with. Figure 1.3 depicts the big data spectrum. You’ll notice that the data in yellow at the left bottom portion of the spectrum has less of the four V’s. This typically is data that is sourced from enterprise resource planning (ERP) systems. This is also the type of data that a business analyst, especially an accountant, most often works with.\n     Continuing along the trajectory, you’ll see data generated from operating systems, such as call center records, email, voicemail, etc. From there, you’ll see data that comes from the web, including shopping cart information, web logs, browser history, promotion information, etc. This then moves beyond to the “biggest” category of data, which includes videos, radio-frequency identification (RFID), global positioning system (GPS) coordinates, social media feeds and more.\n\n\n\n\n\n\nFigure 1.3: Big Data Spectrum",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#importance-of-data-analytics-in-accounting",
    "href": "overview.html#importance-of-data-analytics-in-accounting",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.2 Importance of Data Analytics in Accounting",
    "text": "1.2 Importance of Data Analytics in Accounting\n\n     Data analytics is important for accounting profession because data gathering and analytics technologies have the potential to fundamentally change accounting and auditing task processes (Schneider et al. 2015). Scholars note that the emergence of data analytics will significantly change the infer/predict/assure (e.g., insight/foresight/oversight) tasks performed by accountants and auditors. Big data and analytics have increasingly important implications for accounting and will provide the means to improve managerial accounting, financial accounting, and financial reporting practices (Warren, Moffitt, and Byrnes 2015). It is further suggested that big data offers an unprecedented potential for diverse, voluminous datasets and sophisticated analyses. Research indicates that big data has great potential to produce better forecast estimates, going concern calculations, fraud, and other variables that are of concern to both internal and external auditors (M. G. Alles 2015). Moreover, auditors might reduce audit costs and enhance profitability and effectiveness by means of big data or data analytics. Sixty-six percent of internal audit departments currently utilize some form of data analytics as part of the audit process (Protiviti 2017).\n\n\n1.2.1 Data Analytics in Financial Accounting\n\n     Warren, Moffitt, and Byrnes (2015) note that “in financial accounting, big data will improve the quality and relevance of accounting information, thereby enhancing transparency and stakeholder decision-making. In reporting, big data can assist with the creation and refinement of accounting standards, helping to ensure that the accounting profession will continue to provide useful information as the dynamic, real-time, global economy evolves.” In particular, they suggest that big data could significantly impact the future of financial accounting and Generally Accepted Accounting Principles (GAAP). Big data can also help to supplement financial statement disclosures by accumulating, processing, and analyzing information about a given intangible of interest. Furthermore, big data or data analytics can help in narrowing the differences between accounting standards such US GAAP and International Financial Reporting Standards (IFRS) and facilitate different measurement processes such as Fair Value Accounting (FVA) by analyzing different kinds of unstructured data (Warren, Moffitt, and Byrnes 2015).\n     Crawley and Wahlen (2014) noted that data analytics allows researchers to explore a large amount of qualitative information disclosed by organizations, and examines the consequences of such disclosures. Moreover, data analytics now provides the opportunity to judge the informational content of qualitative financial information. For example, Davis, Piger, and Sedor (2012) found that the extent of optimism expressed in firms’ earnings announcements is positively associated with Return on Assets (ROA) and stock reactions. By the same token, Li (2010) suggested that the tone of forward-looking statements is positively associated with future earnings performance. In addition, Feldman et al. (2010) found that changes in disclosure tone is indicative of future changes in earnings. Interestingly, research shows that even information on social media such as Twitter can predict stock market responses (Bollen, Mao, and Zeng 2011).\n     Data analytics helps to relate textual data to earnings quality. For example, firms having more complicated and less transparent financial statement disclosures are more likely to have poor quality earnings, less persistent positive earnings and more persistent negative earnings (Li 2008). Li, Lundholm, and Minnis (2013) confirmed that firms discussing their competition frequently have ROAs that mean returns more severely than the firms discussing the competition infrequently.\n     With the help of textual data analytics, researchers recently documented the role that qualitative disclosures have in forming the information environment of organizations; such information environments include factors such as the number of analyst following a firm, characteristics of its investors, its trading activities, and the litigation it is involved with. Less readable 10-Ks are associated with greater number of analysts following the firm and a greater amount of effort needed to generate report about it (Lehavy, Li, and Merkley 2011). They also find that less readable 10-Ks are associated with greater dispersion, lower accuracy, and greater uncertainty in analyst’s earnings forecasts about a given firm.\n\n\n\n\n\n\n\nSummary Point\n\n\n\n\n\nData analytics in Financial Accounnting - A. has potential to enhance quality and relevance of accounting information, B. can supplemental financial statement disclosures, C. can facilitate different measurement processes, and D. allows to explore a large amount of qualitative information\n\n\n\n\n\n1.2.2 Data Analytics in Management Accounting\n\n     Warren et al. (2015, 397) noted that “in managerial accounting, big data will contribute to the development and evolution of effective management control systems and budgeting processes. In particular, they elaborate on how big data or data analytics can play a role in management control systems by discovering behaviors that have correlation with specific goal outcomes. Essentially, big data analytics can locate new kinds of behaviors that might impact goal outcomes by simplifying the identification of important motivational measurement tools linked to organizational goals. Moreover, by analyzing non-structured data, big data analytics can help discern employee morale, productivity, and customer satisfaction. Data analytics can also be used to improve “beyond budgeting practices” since traditional budgeting sometimes creates barriers to creativity and flexibility (Warren, Moffitt, and Byrnes 2015).\n     Richins et al. (2017) uggest that big data analytics could improve customer service quality. They suggest that most of the time organizations use structured data that are in their records to evaluate customer service quality; however, this approach does not take into account the customer perspective. Big data analytics allow organizations to evaluate this customer perspective by using unstructured data from social media or e-commerce sites, thus permitting organizations to have a holistic view of customer service quality.\n     Managers recognize that financial measures, alone, are insufficient to forecast future financial success or to use for performance management. Big data analytics provides opportunities to incorporate non-financial measures by incorporating unstructured data (Richins et al. 2017). Using big data analytics (particularly the analysis of unstructured data) accountants can identify the causes of underlying problems, understand ramifications, and develop plans to mitigate adverse impacts (Richins et al. 2017). Data analytics can also provide accountants with additional tools to monitor operations and product quality, discover opportunities to reduce costs, and contribute to decision-making (Dai and Vasarhelyi 2016).\n\n\n\n\n\n\n\nSummary Point\n\n\n\n\n\nData analytics in Management Accounnting - A. will contribute to the development of effective management control systems and budgeting processes, B. can enhance employee morale, productivity, and customer satisfaction, C. can enhance customer service quality by evaluating customer perspectives using unstructured data from social media or e-commerce, D. creates opportunities to incorporate non-financial measures by incorporating unstructured data, and E. provide accountants with additional tools to monitor operations and product quality, and discover opportunities to reduce costs.\n\n\n\n\n\n1.2.3 Data Analytics in Auditing\n\n     Data analytics has the potential to improve the effectiveness of auditing by providing new forms of audit evidence. Data analytics can be used in both auditing planning and in audit procedures, helping auditors to identify and assess risk by analyzing large volumes of data. Even organizations that have very immature capabilities indicate that a strong level of value is derived from including analytics in the audit process (Protiviti 2017).\n     Big data is being seen by practitioners as an essential part of assurance services (M. Alles and Gray 2016), but its application in auditing is not as straightforward as it is in marketing and medical research. Appelbaum (2016) and Cao, Chychyla, and Stewart (2015) identified several areas that are likely to benefit from the use of big data analytics. Some of the areas are:\n\nAt the engagement phase – supplementing auditors’ industry and client knowledge\nAt the planning phase – supplementing auditors’ risk assessment process\nAt the substantive test phase – verifying the management assertions\nAt the review phase – advanced data analytical tools as analytical procedures\nAt the continuous auditing phase – enhancing knowledge about the clients\n\n     Yoon, Hoogduin, and Zhang (2015) suggest that big data create great opportunities through providing audit evidence. They focused on the “sufficiency” and “appropriate” criteria and noted that though there are some issues about the propriety of big data due to different kinds of “noise,” big data can be used as complementary audit evidence. Additionally, they discussed how big data can be integrated with traditional audit evidence in order to add value in the process. Big data or data analytics can also help auditors to test the existence of assertions (e.g. fixed assets) using non-conventional data such as video recording (Warren, Moffitt, and Byrnes 2015). In the world of big data, potential types and sources of audit evidence have changed (Appelbaum 2016). For this reason, Krahel and Titera (2015) suggest that big data might change the focus of auditors, shifting emphasis from management to the verification of data.\n     Data quality and reliability or verifiability have become important issues in auditors’ evaluations of audit evidence. In this way, big data can be used as part of analytical procedures, which are required at the planning and review phase, but which are optional at the substantive procedure phase. However, many issues remain unresolved about how to use big data since analytical procedures and auditing standards are not very specific about the selection of analytical audit procedures; the choice depends on the professional judgment of auditors (Appelbaum, Kogan, and Vasarhelyi 2017). For this reason, auditors need to exercise increased professional skepticism in the big data era because in many cases sources of big data lack provenance and, subsequently, veracity, and sometimes auditors (particularly internal auditors) have little or no involvement in data quality evaluation of such sources (Appelbaum 2016). Considering the prediction that analytics will spell the demise of auditing, Richins et al. (2017) suggest that auditors in the big data era are still essential because they know “the language of business.” Particularly, they suggest that big data analytics cannot replace the professional judgment used by auditors, suggesting that analytics will instead complement auditors’ professional judgment.\n     M. Alles and Gray (2016) identify four potential advantages of incorporating big data into audit practices: strong predictive power to set expectations for financial statement audits, great opportunities to identify potential fraudulent activities, increased probabilities of discovering red flags, and the possibility of developing more predictive models for going concern assumptions. To that end, internal audit groups with dedicated analytics functions and organizations that have attained a managed or optimized to the state of analytics maturity are far more likely to conduct continuous auditing (Protiviti 2017). Though big data creates many opportunities for improving auditing, it also suffers from different shortcomings that hinder its application in Continuous Auditing (CA). For example, Zhang, Yang, and Appelbaum (2015) suggest big data characteristics such as volume, velocity, variety, and veracity creates problems in its application in CA through different gaps such as data consistency, data integrity, data identification, data aggregation, and data confidentiality.\n     Rose et al. (2017) found that the timing of the introduction of data analytics tools into the audit process affects the evaluation of evidence and professional judgment. Barr-Pulliam, Brown-Liburd, and Carlson (2023) found that jurors consider auditors more negligent when they use traditional auditing technique rather than audit data analytics techniques. Additionally, they confirmed that audit data analytics tools increase the perceptions of audit quality. Schneider et al. (2015) suggest that data analytics can be used by auditors to evaluate the internal control effectiveness and policy compliance. They further suggest that by analyzing unusual data flows, unexpected large volumes of data, high frequency transactions, or duplicate vendor payments, auditors can better detect fraud.\n\n\n\n1.2.4 Data Analytics in Tax Accounting\n\n     Traditionally, tax analytics has focused on hindsight, particularly dealing with data from transactions that have already occurred. However, recently tax organizations are looking to use data more for gaining insight and sometimes for foresight. Analytics can help to move tax toward insight and foresight, thus changing the mindset from “what do I need to do?” to “what do I need to know?” (Deloitte 2016).\n     Data analytics can help an organization and its tax function drive toward becoming an insight-driven organization, or IDO (Deloitte 2016). Various types of analytics can be applied to tax issues. Organizations have used tax analytics mostly in creating descriptive scorecards and visualizations (hindsight). These kinds of tax analytics help to determine where to allocate resources, focus on anomalies in results, and identify potential areas of risk.",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#types-of-data-analytics",
    "href": "overview.html#types-of-data-analytics",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.3 Types of Data Analytics",
    "text": "1.3 Types of Data Analytics\n\n     Data analytics can be classified in many ways, but usually there are four types of data analytics. Figure 1.4 depicts the types -\n\nDescriptive Analytics (Business Intelligence & Data Mining)\nDiagnostic Analytics\nPredictive Analytics (Forecasting)\nPrescriptive Analytics (Optimization & Simulation)\n\n\n1.3.1 Descriptive Analytics\n     Descriptive analytics looks at data and analyze past event for insight as to how to approach future events. It looks at past performance and understands the performance by mining historical data to understand the cause of success or failure in the past. Almost all management reporting such as sales, marketing, operations, and finance uses this type of analysis. Common types of descriptive analytis are -\n\nData Queries\nReports\nDescriptive Statistics\nData Dashboard\n\n\n\n\n\n\n\nFigure 1.4: Types of Data Analytics\n\n\n\n\n\n1.3.2 Diagnostic Analytics\n     In this analysis, we generally use historical data over other data to answer any question or for the solution of any problem. We try to find any dependency and pattern in the historical data of the particular problem. For example, companies go for this analysis because it gives a great insight into a problem, and they also keep detailed information about their disposal otherwise data collection may turn out individual for every problem and it will be very time-consuming. Common techniques used for Diagnostic Analytics are:\n\nData Discovery\nData Mining\nCorrelations\n\n\n\n1.3.3 Predictive Analytics\n     Predictive analytics turn the data into valuable, actionable information. predictive analytics uses data to determine the probable outcome of an event or a likelihood of a situation occurring. Predictive analytics holds a variety of statistical techniques from modeling, machine learning, data mining, and game theory that analyze current and historical facts to make predictions about a future event. Techniques that are used for predictive analytics are:\n\nLinear Regression\nTime Series Analysis and Forecasting\nData Mining\n\n\n\n1.3.4 Prescriptive Analytics\n     Prescriptive Analytics automatically synthesize big data, mathematical science, business rule, and machine learning to make a prediction and then suggests a decision option to take advantage of the prediction. Prescriptive analytics goes beyond predicting future outcomes by also suggesting action benefits from the predictions and showing the decision maker the implication of each decision option. Prescriptive Analytics not only anticipates what will happen and when to happen but also why it will happen. Further, Prescriptive Analytics can suggest decision options on how to take advantage of a future opportunity or mitigate a future risk and illustrate the implication of each decision option.",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#data-analytics-processes",
    "href": "overview.html#data-analytics-processes",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.4 Data Analytics Processes",
    "text": "1.4 Data Analytics Processes\n\n     As in in any scientific discipline, data analytics involves a rigorous step-by-step process. Each step demands different skills and know-how. However, to realize the full potential of data analytics, understanding the whole process is important. Figure 1.5 delineates the whole process.\n\n1.4.1 Defining the Questions\n     The first step in any data analysis process is to define your objective. In data analytics jargon, this is sometimes called the ‘problem statement’. Defining your objective means coming up with a hypothesis and figuring how to test it. For instance, your organization’s senior management might pose an issue, such as: “Why are we losing customers?” It’s possible, though, that this doesn’t get to the core of the problem. A data analyst’s job is to understand the business and its goals in enough depth that they can frame the problem the right way.\n\n\n1.4.2 Collecting the Data\n     \n\n\n1.4.3 Cleaning the Data\n     \n\n\n\n\n\n\nFigure 1.5: Data Analytics Process\n\n\n\n\n\n1.4.4 Analyzing the Data\n     \n\n\n1.4.5 Sharing Your Results",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#analytics-mindset",
    "href": "overview.html#analytics-mindset",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.5 Analytics Mindset",
    "text": "1.5 Analytics Mindset\n\n     Having an analytics mindset is important while performing data analytics processes. An analytics mindset is the ability to:\n\nAsk the right questions\nExtract, transform and load relevant data (i.e., the ETL process)\nApply appropriate data analytics techniques\nInterpret and share the results with stakeholders\n\n\n1.5.1 Ask the Right Questions\n     To drive better decisions, one must ask the right questions first and then seek answers in the data. Then one looks to find relevant data and the appropriate data sources to perform the analytics. These analytics will provide the insights and answer the business questions being asked, which then drive the decisions. Throughtout the process, there is a continuous feedback loop that makes the process iterative. An analytics mindset keeps asking questions until the right answers emerge. The right questions are those that lead to the right answers. Figure 1.6 depicts how analytics mindset works.\n\n\n\n\n\n\nFigure 1.6: Analytics Mindset Decision Making\n\n\n\n     Being able to ask the right question is not easy; rather it heavily relies on several factors. First, the analyst needs to understand who the relevant stakeholders are and their objectives. Knowing your audience and what they want to accomplish is critical to understanding value and how to identify a right question. Second, the analyst needs to have an understanding of the business and the underlying business processes — the overall business context. As an example, if you were asked to perform a competitive analysis across the high-tech industry and if you didn’t have a strong understanding of the industry and key performance indicators, you might not ask the right questions (e.g., select the right indicators to analyze).\n\n\n1.5.2 Extract, Transform, and Load Relevant Data (The ETL Process)\n     The first focus with this competency is understanding data characteristics and their relevance. In terms of data characteristics, we already discussed the four/six V’s and the big data spectrum. In determining relevance, this is a focus on what data aligns with the analysis you need to perform to answer your question.It is also important to understand the flow of data in an accounting information system to understand where your data is coming from and how it is generated. This understanding includes - type of accounting information systems, what modules are in the context, capabilities and the limitations of the data and so on. Once this understanding is established, the ETL process can begin. This starts with the extraction of data. For extraction, key things need to be known, including: what data to ask for, how to ask for data, how to manage data security, what format the data needs to be in.\n     The next step is transformation, which also is referred to as data cleansing. This involves converting data from one format to another to load it into an analytics tool. This includes making certain that only the data needed is extracted and that this data is complete and accurate. Data cleansing needs to be performed both before and after the data loading process. Loading data includes knowing which tool the data should be loaded into for the most efficient and effective analysis. For example, this might be driven by the amount of data and the capacity of a given analytics tool. Throughout the ETL process, it is important to maintain the integrity of your data. This is often done through data validation, for example, a control total of your data matching an account balance total in the general ledger.\n\n\n1.5.3 Apply Appropriate Data Analytics Techniques\n     In determining how to apply appropriate data analytics techniques, it is important to understand: the purpose of different types of data analytics techniques, how to determine which techniques are most appropriate for the objectives of your analysis. objectives might include a need to prove or disprove your expectation if one was developed. For example, during the planning phase of an audit, the auditor is required to assess risk. One way of doing this is by exploring the data and looking for anomalies. As the audit progresses into the execution phase, the auditor considers the risk of error or intentional misstatement, the effectiveness of controls and the amounts actually recorded. The objective is to confirm or disconfirm an expectation regarding recorded amounts.\n     There are many ways to analyze data. Some of the more fundamental analyses that you should be able to understand and apply include:\n– Ratios (e.g., gross margin or a day’s sales in accounts receivable)\n– Sorting (e.g., by industry or month)\n– Aggregation (e.g., total of an account balance)\n– Trends (e.g., the movement in inventory associated with both purchases and sales)\n– Comparisons (e.g., sales month to month)\n– Forecasting (e.g., budgeted expenses)\n     It is also important to gain familiarity with analytics tools. There are many tools capable of performing analytics and it isn’t necessary for you to know how to use each one, but you should have some hands-on experience with a few of the more fundamental tools that are most readily used by an analyst. Some of the fundamental tools include: Excel, Basic databases (Access), Visualization (Tableau, Power BI). It is also good to have a working knowledge or awareness of other tools, including those that might be specific to the career path you are choosing. Note that beyond these fundamental tools, there are other tools students should be familiar with, to a lesser extent (a working knowledge or awareness level).\n\n\n1.5.4 Interpret and Share the Results\n     As discussed previously, the end goal is to provide insights to your stakeholders based on the objectives that were identified. Your insights are derived from your interpretation of the analytics results. Therefore, it is important that you interpret the outcomes of your analysis appropriately, based on your question and expectation, if you had one. When you look at the results of your analysis, use your critical thinking and ask yourself:\n– What do you see?\n– Do you see what you expected to see?\n– Do your results make sense to you?\n– Is any further analysis required to meet your objective?\n     Once you have interpreted your results, you need to summarize them in a manner conducive to and compelling for your stakeholder. Visualization can be used as a technique and a way to present findings as well. You can make choices about displaying your analysis in a variety of ways, which might include tables, area charts, map charts, heat maps, Gantt charts, horizontal or vertical bar charts, pie charts, line charts, scatter plots, bubble charts and more. When making choices about which visualization is appropriate, there are many design principles to consider. These might include color, sizing, labeling, visual simplicity (e.g., elimination of visual clutter), etc",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "overview.html#data-analytics-skillset",
    "href": "overview.html#data-analytics-skillset",
    "title": "1  Overview of Accounting Analytics",
    "section": "1.6 Data Analytics Skillset",
    "text": "1.6 Data Analytics Skillset\n\n     Proficiency in data analytics involves a combination of skills. For example, a solid understanding in mathematics and statistics helps one to gain the foundational knowledge in data science.@fig-datascienceroadmap depicts the skills necessary to be proficient in data science.\n\n\n\n\n\n\nFigure 1.7: Data Science Skillset\n\n\n\n\n\n\n\n\nAlles, Michael G. 2015. “Drivers of the Use and Facilitators and Obstacles of the Evolution of Big Data by the Audit Profession.” Accounting Horizons 29 (2): 439–49. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/439/2188.\n\n\nAlles, Michael, and Glen L. Gray. 2016. “Incorporating Big Data in Audits: Identifying Inhibitors and a Research Agenda to Address Those Inhibitors.” International Journal of Accounting Information Systems 22: 44–59. https://www.sciencedirect.com/science/article/pii/S1467089516300811.\n\n\nAmerican Institute of Certified Public Accountants (AICPA). 2015. “Audit Analytics and Continuous Audit: Looking Toward the Future.”\n\n\n———. 2017. “Description Criteria for Management’s Description of the Entity’s Cybersecurity Risk Management Program.”\n\n\nAppelbaum, Deniz. 2016. “Securing Big Data Provenance for Auditors: The Big Data Provenance Black Box as Reliable Evidence.” Journal of Emerging Technologies in Accounting 13 (1): 17–36. https://publications.aaahq.org/jeta/article-abstract/13/1/17/9219.\n\n\nAppelbaum, Deniz, Alexander Kogan, and Miklos A. Vasarhelyi. 2017. “Big Data and Analytics in the Modern Audit Engagement: Research Needs.” Auditing: A Journal of Practice & Theory 36 (4): 1–27. https://publications.aaahq.org/ajpt/article-abstract/36/4/1/6016.\n\n\nBarr-Pulliam, Dereck, Helen L. Brown-Liburd, and Amanda G. Carlson. 2023. “Do Audit Data Analytics Influence Juror Perceptions of Audit Quality and Auditor Negligence?” Current Issues in Auditing 17 (2): P1–10. https://publications.aaahq.org/cia/article/17/2/P1/10096.\n\n\nBarton, Dominic, and David Court. 2012. “Making Advanced Analytics Work for You.” Harvard Business Review 90 (10): 78–83. http://www.buyukverienstitusu.com/s/1870/i/Making_Advanced_Analytics_Work_For_You.pdf.\n\n\nBollen, Johan, Huina Mao, and Xiaojun Zeng. 2011. “Twitter Mood Predicts the Stock Market.” Journal of Computational Science 2 (1): 1–8. https://www.sciencedirect.com/science/article/pii/S187775031100007X.\n\n\nCao, Min, Roman Chychyla, and Trevor Stewart. 2015. “Big Data Analytics in Financial Statement Audits.” Accounting Horizons 29 (2): 423–29. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/423/2177.\n\n\nColumbus. 2017. “53% Of Companies Are Adopting Big Data Analytics.” Forbes. https://www.forbes.com/sites/louiscolumbus/2017/12/24/53-of-companies-are-adopting-big-data-analytics/?sh=6c98f39939a1.\n\n\nCrawley, Michael, and James Wahlen. 2014. “Analytics in Empirical/Archival Financial Accounting Research.” Business Horizons 57 (5): 583–93. https://www.sciencedirect.com/science/article/pii/S0007681314000792.\n\n\nDai, Jun, and Miklos A. Vasarhelyi. 2016. “Imagineering Audit 4.0.” Journal of Emerging Technologies in Accounting 13 (1): 1–15. https://publications.aaahq.org/jeta/article-abstract/13/1/1/9242.\n\n\nDavis, Angela K., Jeremy M. Piger, and Lisa M. Sedor. 2012. “Beyond the Numbers: Measuring the Information Content of Earnings Press Release Language.” Contemporary Accounting Research 29 (3): 845–68. https://doi.org/10.1111/j.1911-3846.2011.01130.x.\n\n\nDeloitte. 2016. “Tax Data Analytics A New Era for Tax Planning and Compliance.” https://www2.deloitte.com/content/dam/Deloitte/us/Documents/Tax/us-tax-data-analytics-a-new-era-for-tax-planning-and-compliance.pdf.\n\n\nFeldman, Ronen, Suresh Govindaraj, Joshua Livnat, and Benjamin Segal. 2010. “Management’s Tone Change, Post Earnings Announcement Drift and Accruals.” Review of Accounting Studies 15 (4): 915–53. https://doi.org/10.1007/s11142-009-9111-x.\n\n\nKrahel, John Peter, and William R. Titera. 2015. “Consequences of Big Data and Formalization on Accounting and Auditing Standards.” Accounting Horizons 29 (2): 409–22. https://publications.aaahq.org/accounting-horizons/article/29/2/409/2149.\n\n\nLehavy, Reuven, Feng Li, and Kenneth Merkley. 2011. “The Effect of Annual Report Readability on Analyst Following and the Properties of Their Earnings Forecasts.” The Accounting Review 86 (3): 1087–1115. https://publications.aaahq.org/accounting-review/article-abstract/86/3/1087/3300.\n\n\nLi, Feng. 2008. “Annual Report Readability, Current Earnings, and Earnings Persistence.” Journal of Accounting and Economics 45 (2-3): 221–47. https://www.sciencedirect.com/science/article/pii/S0165410108000141.\n\n\n———. 2010. “The Information Content of Forward‐Looking Statements in Corporate Filings—A Naïve Bayesian Machine Learning Approach.” Journal of Accounting Research 48 (5): 1049–1102. https://doi.org/10.1111/j.1475-679X.2010.00382.x.\n\n\nLi, Feng, Russell Lundholm, and Michael Minnis. 2013. “A Measure of Competition Based on 10‐K Filings.” Journal of Accounting Research 51 (2): 399–436. https://doi.org/10.1111/j.1475-679X.2012.00472.x.\n\n\nProtiviti. 2017. “Embracing Analytics in Auditing.” https://www.protiviti.com/sites/default/files/2022-06/2017-internal-audit-capabilities-and-needs-survey-protiviti.pdf.\n\n\nProvost, Foster, and Tom Fawcett. 2013. “Data Science and Its Relationship to Big Data and Data-Driven Decision Making.” Big Data 1 (1): 51–59. https://doi.org/10.1089/big.2013.1508.\n\n\nRichins, Greg, Andrea Stapleton, Theophanis C. Stratopoulos, and Christopher Wong. 2017. “Big Data Analytics: Opportunity or Threat for the Accounting Profession?” Journal of Information Systems 31 (3): 63–79. https://publications.aaahq.org/jis/article-abstract/31/3/63/1114.\n\n\nRose, Anna M., Jacob M. Rose, Kerri-Ann Sanderson, and Jay C. Thibodeau. 2017. “When Should Audit Firms Introduce Analyses of Big Data into the Audit Process?” Journal of Information Systems 31 (3): 81–99. https://publications.aaahq.org/jis/article-abstract/31/3/81/1123.\n\n\nSchneider, Gary P., Jun Dai, Diane J. Janvrin, Kemi Ajayi, and Robyn L. Raschke. 2015. “Infer, Predict, and Assure: Accounting Opportunities in Data Analytics.” Accounting Horizons 29 (3): 719–42. https://publications.aaahq.org/accounting-horizons/article-abstract/29/3/719/2262.\n\n\nSivarajah, Uthayasankar, Muhammad Mustafa Kamal, Zahir Irani, and Vishanth Weerakkody. 2017. “Critical Analysis of Big Data Challenges and Analytical Methods.” Journal of Business Research 70: 263–86. https://www.sciencedirect.com/science/article/pii/S014829631630488X.\n\n\nThe Economist. 2017. “The World’s Most Valuable Resource Is No Longer Oil, but Data.” https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data.\n\n\nVasarhelyi, Miklos A., Alexander Kogan, and Brad M. Tuttle. 2015. “Big Data in Accounting: An Overview.” Accounting Horizons 29 (2): 381–96. https://publications.aaahq.org/accounting-horizons/article-abstract/29/2/381/2184.\n\n\nVerver, John. 2015. “Six Audit Analytics Success Factors.” Internal Auditor 72 (3).\n\n\nWarren, J. Donald, Kevin C. Moffitt, and Paul Byrnes. 2015. “How Big Data Will Change Accounting.” Accounting Horizons 29 (2): 397–407. https://publications.aaahq.org/accounting-horizons/article/29/2/397/2168.\n\n\nYoon, Kyunghee, Lucas Hoogduin, and Li Zhang. 2015. “Big Data as Complementary Audit Evidence.” Accounting Horizons 29 (2): 431–38. https://publications.aaahq.org/accounting-horizons/article/29/2/431/2215.\n\n\nZhang, Juan, Xiongsheng Yang, and Deniz Appelbaum. 2015. “Toward Effective Big Data Analysis in Continuous Auditing.” Accounting Horizons 29 (2): 469–76. https://publications.aaahq.org/accounting-horizons/article/29/2/469/2160.",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview of Accounting Analytics</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "4  Data Visualization",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#what-is-visualization",
    "href": "visualization.html#what-is-visualization",
    "title": "4  Data Visualization",
    "section": "4.1 What is Visualization?",
    "text": "4.1 What is Visualization?\n\n     To learn more about different kinds of visualization in R, you should visit - https://r-graph-gallery.com/ and https://www.kaggle.com/code/ruchiraperera/seaborn-vs-plotly-express.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#importance-of-visualization",
    "href": "visualization.html#importance-of-visualization",
    "title": "4  Data Visualization",
    "section": "4.2 Importance of Visualization",
    "text": "4.2 Importance of Visualization",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#visualization-packages-in-r-and-python",
    "href": "visualization.html#visualization-packages-in-r-and-python",
    "title": "4  Data Visualization",
    "section": "4.3 Visualization Packages in R and Python",
    "text": "4.3 Visualization Packages in R and Python\n\nRPython\n\n\n\n     ggplot2 is a powerful package for visualization in R. In addition, some other packages enhance the functionalities of ggplot2. These packages include - gganimate, ggthemes, ggpubr, ggridges, ggmap, ggrepel, ggextra, ggpattern, ggcorrplot and so on.\n\n\n# Loading tidyverse package\nlibrary(tidyverse)\n# Loading dataset \ntips = read_csv(\n    'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv'\n)\n\n\n\n\n     In Python, matplotlib and seaborn are two of the powerful packages for visualization. Additionally, plotly, plotnine, altair, and bokeh are some other python packages that enhances visualization in python.\n\n\n# Loading Necessary Python Packages \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\n# ggplot style \nplt.style.use('ggplot')\n# Loading dataset\ntips = sns.load_dataset('tips')",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#ggplot---grammar-of-graphics",
    "href": "visualization.html#ggplot---grammar-of-graphics",
    "title": "4  Data Visualization",
    "section": "4.4 ggplot - Grammar of Graphics",
    "text": "4.4 ggplot - Grammar of Graphics\n\n     In ggplot, a plot consists of at least four elements -\n\nData - the data frame\nAesthetic Mappings - aesthetic mappings map variable from the data frame to different kinds of aesthetics such as x coordinate, y coordinate, color, shape, size and so on.\nCoordinate System - the positioning of points\nGeom - geoms are geometirc objects such as points or lines.\n\n     You can also use cheatsheet of ggplot to know more about the ggplot. Another good source to learn more about visualization in R is The R Graph Library. Similarly, for Python, you can use The Python Graph Library.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#types-of-visualization",
    "href": "visualization.html#types-of-visualization",
    "title": "4  Data Visualization",
    "section": "4.5 Types of Visualization",
    "text": "4.5 Types of Visualization\n\n4.5.1 Bar Diagram (Bar Plot)\n\n4.5.1.1 One Categorical Variable\n\nRPython\n\n\n\ntips |&gt; \n    count (sex) |&gt;\n    ggplot(mapping = aes(x = sex, y = n))+\n    geom_bar(stat = 'identity', width = 0.5, fill = \"orangered3\") + \n    labs(x = 'Sex', y = 'Total Observations')\n\n\n\n\nBar Plot of Gender (geom_bar)\n\n\n\n\n      Either of the the following code will also produce the same visualization.\n\ntips |&gt; \n    ggplot(mapping = aes(x = sex))+\n    geom_bar(width = 0.5, fill = \"maroon\") + \n    labs(x = 'Sex', y = 'Total Observations')\n\n\ntips |&gt; \n    ggplot(mapping = aes(x = sex))+\n    stat_count(width = 0.5, fill = \"maroon\") + \n    labs(x = 'Sex', y = 'Total Observations')\n\n\n\n\nsns.countplot(data = tips, x = \"sex\", width=0.5)\nplt.xlabel('Sex')\nplt.ylabel('Total Observations')\n\n\n\n\nBar Plot of Gender (sns.countplot)\n\n\n\n\n\n\n\n\n\n4.5.1.2 One Categorical Variable and One Continuous Variable\n\n     Barplot can also be used for two variables - both discrete (categorical) variables or one discrete (categorical) and one continuous variable. Below is bar plot for one discrete (categorical) and one continuous variable.\n\n\nRPython\n\n\n\ntips |&gt; \n    group_by(sex) |&gt;\n    summarize(total_bill = mean(total_bill)) |&gt;\n    ggplot(aes(x = sex, y = total_bill)) + \n    geom_col(width =0.6, fill = \"pink\") + \n    labs(x = \"Sex\", y = \"Total Bill\") + \n    geom_text(aes(label = round(total_bill,2)), vjust = -0.2)\n\n\n\n\n\n\n\n\n     The following code will produce the same results.\n\ntips |&gt; \n    ggplot(mapping = aes(x = sex, y = total_bill))+\n    geom_bar(stat = 'summary', fun = \"mean\", position = \"dodge\",\n    width = 0.60, fill = \"pink\") + \n    labs(x = \"Sex\", y = \"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\nsns.barplot(data = tips, x = \"sex\", y = \"total_bill\",\n            width= 0.5, \n            errorbar= None)\nplt.xlabel('Sex')\nplt.ylabel('Total Bill')\n\n\n\n\nBar Plot of Gender (sns.barplot)\n\n\n\n\n     The following code will add text value on the bars in barplot.\n\nax = sns.barplot(data = tips, x = \"sex\", y = \"total_bill\",\n            width= 0.5, \n            errorbar= None)\n\nfor i in ax.containers:\n    ax.bar_label(i,)\n\nplt.xlabel('Sex')\nplt.ylabel('Total Bill')\n\n\n\n\n\n\n4.5.1.3 Two Categorical Variables\n     Below is a bar plot for both discrete (categorical) variables.\n\nRPython\n\n\n\ntips |&gt; \n    count (sex, day) |&gt;\n    ggplot(mapping = aes(x = sex, y = n, fill = day))+\n    geom_bar(stat = 'identity', position = \"dodge\") + \n    labs(x = \"Sex\", y = \"Total Observations\")\n\n\n\n\nBar Plot of Gender (geom_bar - unstacking bar)\n\n\n\n\n     The following code will also produce the same visualization.\n\ntips |&gt; \n    #count (sex, day) |&gt;\n    ggplot(mapping = aes(x = sex, fill = day))+\n    geom_bar(stat = 'count', position = \"dodge\") + \n    labs(x = \"Sex\", y = \"Total Observations\"\n         ,fill = \"Day\"\n    )\n\n\n\n\n\n\n\n\n\ntips |&gt; \n    count (sex, day) |&gt;\n    ggplot(mapping = aes(x = sex, y = n, fill = day))+\n    geom_bar(stat = 'identity', position = \"stack\") + # position = \"fill\"\n    labs(x = \"Sex\", y = \"Total Observations\")\n\n\n\n\n\n\n\n\n     The following code will also produce the same visualization.\n\ntips |&gt; \n    #count (sex, day) |&gt;\n    ggplot(mapping = aes(x = sex, fill = day))+\n    geom_bar(stat = 'count', position = \"stack\") + # position = \"fill\"\n    labs(x = \"Sex\", y = \"Total Observations\"\n         ,fill = \"Day\"\n    )\n\n\n\n\n\n\n\n\n\n\n\nsns.countplot(data = tips, x = \"sex\", hue = \"day\")\nplt.xlabel('Sex')\nplt.ylabel('Total Observations')\n\n\n\n\nBar Plot of Gender (sns.countplot - unstacking bar)\n\n\n\n\n     Stacked barchart cannot be created using seaborn. So, we use alternatives -\n\ntips[['sex', 'day']].value_counts().reset_index() \\\n    .pivot(index = \"sex\", columns = \"day\", values = 'count') \\\n    .plot(kind = \"bar\", stacked = True)\nplt.xticks(rotation = 360)\n\n(array([0, 1]), [Text(0, 0, 'Male'), Text(1, 0, 'Female')])\n\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Total Observations\")\nplt.legend(loc = \"upper right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.2 Histogram\n\n4.5.2.1 One Continuous Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill))+\n    geom_histogram(binwidth = 2.25, fill = \"orangered3\") + \n    labs(x = \"Total Bill\", y = \"Count\") \n\n\n\n\n\n\n\n\n     The following code will generate the same results with a little modification -\n\ntips |&gt;\n    ggplot(aes(x = total_bill))+\n    geom_histogram(binwidth = 2.25, fill = \"orangered3\", col = \"white\") + \n    labs(x = \"Total Bill\", y = \"Count\")\n\n\n\n\nsns.histplot(data = tips, x = \"total_bill\", binwidth=2.25)\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.2.2 One Continuous and One Categorical Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, fill = sex))+\n    geom_histogram(binwidth = 2.25)+\n    labs(x = \"Total Bill\")\n\n\n\n\n\n\n\n\n     The following code will generate the same results -\n\ntips |&gt;\n    ggplot(aes(x = total_bill, color = sex))+\n    geom_histogram(binwidth = 2.25)\n\n\n\n\nsns.histplot(data = tips, x = \"total_bill\", hue = \"sex\", binwidth=2.25)\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Count\")\n\n\n\n\n\n\n\n\n\nsns.FacetGrid(data=tips, col=\"sex\") \\\n    .map(sns.histplot, \"total_bill\", binwidth = 2.25)\n\n\n\n\n\n\n\n4.5.3 Density Plot\n\n4.5.3.1 One Continuous Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill))+\n    geom_density( size = 1, color = \"orangered3\"\n        #adjust = 0.2\n    ) + \n    labs(x = \"Total Bill\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data = tips, x = \"total_bill\"\n            #,bw_adjust = 0.20\n            )\nplt.xlabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3.2 Two Continuous Variables\n\nRPython\n\n\n\ntips |&gt;\n    select(1:2) |&gt;\n    pivot_longer(cols = everything(), names_to = \"types\", values_to = \"values\") |&gt;\n    ggplot(aes(x = values, col = types))+\n    geom_density(size = 1)\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data = tips[['total_bill', 'tip']])\nplt.xlabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.3.3 One Continuous Variable and One Categorical Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, fill = sex))+\n    geom_density(\n        #adjust = 0.2\n    )+ \n    labs(x = \"Total Bill\", y = \"Density\")\n\n\n\n\n\n\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, color = sex))+\n    geom_density(size = 1\n        #adjust = 0.2\n    )+ \n    labs(x = \"Total Bill\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data = tips, x = \"total_bill\", hue = \"sex\")\nplt.xlabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data = tips, x = \"total_bill\", hue = \"sex\", multiple = \"stack\")\nplt.xlabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.4 Point Plot\n\n4.5.4.1 One Categorical and One Continuous Variable\n\nRPython\n\n\n\ntips |&gt; \n    ggplot(aes(x = sex, y = total_bill, group = 1)) + \n    stat_summary(aes(sex, total_bill), geom = \"point\", fun.y = mean, size = 2, col = \"red\")+\n    stat_summary(aes(sex, total_bill), geom = \"line\", fun.y = mean, size = 1.5, col = \"red\",size = 2.1) + \n    labs(x = \"Sex\", y = \"Total Bill\")\n\n\n\n\nLine Plot of Gender (geom_line - mean)\n\n\n\n\n     The following code will also produce the same visualization.\n\ntips |&gt; \n    group_by(sex) |&gt;\n    summarize(total_bill = mean(total_bill)) |&gt;\n    ggplot(aes(x = sex, y = total_bill, group = 1)) + \n    geom_point(col = \"red\", size = 2)+\n    geom_line(col = \"red\", size = 2.1) + \n    labs(x = \"Sex\", y = \"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\nsns.pointplot(data = tips, x = \"sex\", y = \"total_bill\", errorbar=None)\nplt.xlabel('Sex')\nplt.ylabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.4.2 Two Categorical Variables and One Continuous Variable\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = sex, y = total_bill, group = smoker, color = smoker)) + \n    stat_summary(aes(x = sex, y = total_bill), geom = \"point\", fun.y = mean) + \n    stat_summary(aes(x = sex, y = total_bill), geom = \"line\", fun.y = mean, size = 1.1) + \n    labs(x = \"Sex\", y = \"Total Bill\" #, color = \"Smoker\"\n    )\n\n\n\n\n\n\n\n\n     The following code will also produce the same visualization.\n\ntips |&gt;\n    group_by(sex, smoker) |&gt;\n    summarize( total_bill = mean(total_bill)) |&gt;\n    ggplot(aes(x = sex, y = total_bill, group = smoker , color = smoker)) + \n    geom_point()+\n    geom_line(size = 1.1)+\n    labs(x = \"Sex\", y = \"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\nsns.pointplot(data = tips, x = \"sex\", y = \"total_bill\", \n              hue = \"smoker\", errorbar= None)\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.5 Box Plot\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = sex, y = total_bill))+\n    geom_boxplot(fill = \"pink\") + \n    labs (x = \"Sex\", y = \"Total Bill\")\n\n\n\n\n\n\n\n\n\ntips |&gt;\n    ggplot(aes(x = sex, y = total_bill))+\n    geom_boxplot(fill = \"pink\") + \n    labs (x = \"Sex\", y = \"Total Bill\") + \n    facet_wrap(~smoker)\n\n\n\n\n\n\n\ntips |&gt;\n    ggplot(aes(x = sex, y = total_bill))+\n    geom_boxplot(fill = \"pink\") + \n    labs (x = \"Sex\", y = \"Total Bill\") + \n    facet_grid(time~smoker)\n\n\n\n\n\n\n\n\n\n\n\nsns.boxplot(data = tips, x = \"sex\", y = \"total_bill\", color = \"pink\")\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Total Bill\")\n\n\n\n\n\n\n\n\n\nsns.catplot(data = tips, x = \"sex\", y = \"total_bill\", \n            color = \"pink\", kind = \"box\", row = \"smoker\"\n           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsns.catplot(data = tips, x = \"sex\", y = \"total_bill\", \n            color = \"pink\", kind = \"box\", row = \"smoker\"\n            ,col = \"time\"\n           )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.6 Scatter Plot\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, y = tip))+\n    geom_point(col = \"blue\")+\n    labs(x = \"Total Bill\", y = \"Tip\")\n\n\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = tips, x = \"total_bill\", y = \"tip\")\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.7 Regression Plot\n\nRPython\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, y = tip))+\n    geom_point(col = \"blue\")+\n    geom_smooth(method = \"lm\", col = \"orange\") + \n    labs(x = \"Total Bill\", y = \"Tip\")\n\n\n\n\n\n\n\n\n\ntips |&gt;\n    ggplot(aes(x = total_bill, y = tip, col = sex))+\n    geom_point(col = \"blue\")+\n    geom_smooth(method = \"lm\") + \n    labs(x = \"Total Bill\", y = \"Tip\")\n\n\n\n\n\n\n\n\n\n\n\nsns.lmplot(data = tips, x = \"total_bill\", y = \"tip\")\n\n\n\n\n\n\n\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")\n\n\n\n\n\n\n\n\n\nsns.regplot(data = tips, x = \"total_bill\", y = \"tip\")\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")\n\n\n\n\n\n\n\n\n\nsns.lmplot(data = tips, x = \"total_bill\", y = \"tip\", hue = \"sex\")\n\n\n\n\n\n\n\nplt.xlabel(\"Total Bill\")\nplt.ylabel(\"Tip\")",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#exercises-01",
    "href": "visualization.html#exercises-01",
    "title": "4  Data Visualization",
    "section": "4.6 Exercises # 01",
    "text": "4.6 Exercises # 01\n\nDownload student data from the url and create a pointplot (lineplot) of students average math score (math.grade) of gender (gender). Please note that the variable gender includes a label called other in addition to M and F; you should filter out obsevations of the label other before you create visualization.\nFrom the dataset in above (question 1), compare, using pointplot (lineplot), the average math (math.grade) and science score (sciences.grade) of different students based on gender (gender). You might need to use pivot_longer function to reshape the data frame before visualizing the relation.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#interactive-visualization",
    "href": "visualization.html#interactive-visualization",
    "title": "4  Data Visualization",
    "section": "4.7 Interactive Visualization",
    "text": "4.7 Interactive Visualization\n\n     Interactive Visualization involves graphical presentation of data that permits users to engage with the visual elements directly. Unlike static visulization, interactive visualization allows users to manipulate data, explore different aspects, and customize the visualization in real time. The primary objective of interactive visualization is to make data exploration more intuititve and dynamic. The benefits of interactive visualzation include - enhaned engagement, deeper insights, customization, and exploration and discovery.\n\n\nRPythonPlotnine\n\n\n\nlibrary(plotly)\n\n\np = ggplot(data = tips, aes(x = sex)) + \n    geom_bar(width = 0.5, fill = \"orangered3\") + \n    labs(x = \"Gender\", y = \"Total Observations\")\n\nggplotly(p)\n\n\n\n\n\n\np2 = tips |&gt;\n    ggplot(aes(x = time, y = total_bill, group = smoker, color = smoker))+\n    stat_summary(aes(x = time, y = total_bill), geom = \"point\", fun.y = mean) + \n    stat_summary(aes(x = time, y = total_bill), geom = \"line\", fun.y = mean, size = 1.1) + \n    labs (x = \"Time\", y = \"Total Bill\")\n\nggplotly(p2)\n\n\n\n\n\n\n\n\nimport plotly.express as px\n\n\nfig = px.histogram(tips, x = \"sex\") \\\n    .update_traces(marker_color = \"orangered\") \\\n    .update_xaxes(title = \"Sex\") \\\n    .update_yaxes(title = \"Count\")\nfig.show()\n\n                        \n                                            \n\n\n\npx.histogram(tips, x = \"sex\", y = \"total_bill\",histfunc='avg') \\\n    .update_traces(marker_color = \"orangered\") \\\n    .update_xaxes(title = \"Sex\") \\\n    .update_yaxes(title = \"Average Total Bill\") \\\n    .show()\n\n                        \n                                            \n\n\n\npx.histogram(tips, x=\"total_bill\",histnorm='probability density',\n             width=600, height=400) \\\n                .update_xaxes(title = \"Total Bill\") \\\n                .update_yaxes(title =\"Density\")\n\n                        \n                                            \n\n\n\n\n\n#import plotnine as p9\nfrom plotnine import *\nimport plotly.tools as tls\n\ndf = tips.groupby([\"sex\"])[\"total_bill\"].agg('mean').reset_index()\n\n\n(\n    ggplot(df, aes(x = \"sex\", y = \"total_bill\", group = 1)) + \n    geom_point(color = \"blue\")+\n    geom_line(color = \"orange\", size = 1.1) + \n    labs(x = \"Sex\", y = \"Average Total Bill\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\n\n\n\nplotly_fig = (\n    ggplot(df, aes(x = \"sex\", y = \"total_bill\", group = 1)) + \n    geom_point(color = \"blue\")+\n    geom_line(color = \"orange\", size = 1.1)\n)\ntls.mpl_to_plotly(plotly_fig.draw()).show()\n\n                        \n                                            \n\n\n\ndf2 = tips.groupby([\"sex\", \"smoker\"])[\"total_bill\"] \\\n    .agg('mean') \\\n    .round(2) \\\n    .reset_index()\n\n(\n    ggplot(df2, aes(x = \"sex\", y = \"total_bill\",  group = \"smoker\", color = \"smoker\")) + \n    geom_point()+\n    geom_line(size = 1.1) + \n    labs(x = \"Sex\", y = \"Average Total Bill\")\n)\n\n&lt;Figure Size: (640 x 480)&gt;",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#exercises-02",
    "href": "visualization.html#exercises-02",
    "title": "4  Data Visualization",
    "section": "4.8 Exercises # 02",
    "text": "4.8 Exercises # 02",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "foundations.html",
    "href": "foundations.html",
    "title": "2  Foundations of Accounting Data",
    "section": "",
    "text": "2.1 Types of Accounting Data",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundations of Accounting Data</span>"
    ]
  },
  {
    "objectID": "foundations.html#data-sources-and-collection-methods",
    "href": "foundations.html#data-sources-and-collection-methods",
    "title": "2  Foundations of Accounting Data",
    "section": "2.2 Data Sources and Collection Methods",
    "text": "2.2 Data Sources and Collection Methods",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundations of Accounting Data</span>"
    ]
  },
  {
    "objectID": "foundations.html#data-quality-integrity",
    "href": "foundations.html#data-quality-integrity",
    "title": "2  Foundations of Accounting Data",
    "section": "2.3 Data Quality & Integrity",
    "text": "2.3 Data Quality & Integrity\nsummary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2\n\n\n\nplot(mtcars[1:3])\n\n\n\n\n\n\n\n\nSee Figure 2.1 for first ggplot graph\n\nlibrary(tidyverse)\nmtcars %&gt;% \n  as_tibble() %&gt;% \n  ggplot(mapping = aes(x = mpg, y = disp))+\n  geom_point()+\n  geom_smooth()+\n  labs(x = 'Miles Per Gallon (mpg)',\n       y = 'Displacement in Cubic Inches (disp)')\n\n\n\n\n\n\n\nFigure 2.1: Scatter plot and line plot of the relation between mpg and disp\n\n\n\n\n\n\nlibrary(lubridate)\n\n\nimport os\nimport sys\nsys.version\n\n'3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]'\n\n\n\nfor x in os.listdir():\n  print (x)\n\n.git\n.gitattributes\n.gitignore\n.quarto\n.README.md.swp\n.README_BASE_1260.md.swp\n.README_LOCAL_1260.md.swp\n.README_REMOTE_1260.md.swp\naccounting_analytics_book\nadvanced_analytics.qmd\nchapter1_solution.qmd\nchapter2_solution.qmd\ncover.png\ndashboard.qmd\ndata_management.qmd\neda.qmd\nethics.qmd\nfoundations.qmd\nfoundations.rmarkdown\nfoundations_files\nfraud.qmd\nfuture.qmd\nimages\nindex.qmd\nintro.qmd\noverview.qmd\nperformance_measurement.qmd\npredictive.qmd\nprescriptive.qmd\nREADME.html\nREADME.md\nREADME_files\nreferences.bib\nreferences.qmd\nvisualization.qmd\n_book\n_quarto.yml",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundations of Accounting Data</span>"
    ]
  },
  {
    "objectID": "foundations.html#data-sources-and-collecfig-firstplottion-methods",
    "href": "foundations.html#data-sources-and-collecfig-firstplottion-methods",
    "title": "2  Foundations of Accounting Data",
    "section": "2.2 Data Sources and Collecfig-firstplottion Methods",
    "text": "2.2 Data Sources and Collecfig-firstplottion Methods",
    "crumbs": [
      "Introduction to Accounting Analytics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Foundations of Accounting Data</span>"
    ]
  }
]