[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analytics for Accounting Data",
    "section": "",
    "text": "Welcome\nWelcome to the book - Analytics for Accounting Data - which is slated to be published future. It will be available for purchase in both paperback and hardback, with pre-ordering available on both Amazon and online.\nThis is the online version of the book, which is free to use. The book is being developed. So it is recommended to use the book with caution. More chapters and materials wil be added to the book gradually.\nIf you want me to add some materials that are necessary for the students, please do not hesitate to ask. All kinds of comments, recommendations, suggestions, criticisms are welcome.\n\n\n\n\n\n\nWarning\n\n\n\nThis book is a work in progress.\n\n\n\n\nPreface\n\n\nAbout the Book\nThe book is written for the students in undergraduate and graduate programs.\n\n\nAbout the Author\n\n\n\n\nSharif Islam, DBA, CPA, CMA is an Assistant professor in School of Accountancy in Southern Illinois University Carbondale (SIUC). He is a licensed CPA in Illinois and a Certified Management Accountant (CMA). He teaches Auditing, Accounting Information Systems, Machine Learning, and Analytics for Accounting Data. He did his doctorate from Louisiana Tech University in Computer Information Systems and Accounting. His mansucripts are selected for “Best Research Paper Award” in several conferences of American Accounting Association (AAA). His research also got 2024 “Notable Contribution to the Literature Award” by AIS section of AAA. He published research in Accounting Horizons, Journal of Accounting and Public Policy, Journal of Emerging Technologies in Accounting, Issues in Accounting Education, Advances in Accounting and Managerial Auditing Journal. His research interests lie at the intersection of Accounting and Data Science.\n\n\n\nHow to Read the Book\n\n\nAcknowledgment\n\nTo prepare the book, I took help from many sources on the internet and published materials. Many of them are cited in the book. I acknowledge the contribution of all of those resources that help me to prepare the book for the students.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "data_management.html",
    "href": "data_management.html",
    "title": "6  Data Management in Accounting",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#data-management",
    "href": "data_management.html#data-management",
    "title": "6  Data Management in Accounting",
    "section": "6.1 Data Management",
    "text": "6.1 Data Management\n\n     Data management is the practice of collecting, keeping, and using data securely, efficiently, and cost-effectively. Data management is important for a variety of data-driven use cases including end-to-end business process execution, regulatory compliance, accurate analytics and AI, data migration, and digital transformation.\n     Managing digital data in an organization involves a broad range of tasks, policies, procedures, and practices. The work of data management has a wide scope, covering factors such as how to:\n\nCreate, access, and update data across a diverse data tier\nStore data across multiple clouds and on premises\nProvide high availability and disaster recovery\nUse data in a growing variety of apps, analytics, and algorithms\nEnsure data privacy and security\nArchive and destroy data in accordance with retention schedules and compliance requirements\n\n     Data management systems are built on data management platforms and can include databases, data lakes and data warehouses, big data management systems, data analytics, and more.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#relational-database",
    "href": "data_management.html#relational-database",
    "title": "6  Data Management in Accounting",
    "section": "6.2 Relational Database",
    "text": "6.2 Relational Database\n\n     Relational databases organize data into rows and columns, which form a table and different tables are connected to each other usign either primary key or foreign key. Here’s a simple example of two tables a small business might use to process orders for its products. The first table is a customer info table, so each record includes a customer’s name, address, shipping and billing information, phone number, and other contact information. Each bit of information (each attribute) is in its own column, and the database assigns a unique ID (a key) to each row. In the second table—a customer order table—each record includes the ID of the customer that placed the order, the product ordered, the quantity, the selected size and color, and so on—but not the customer’s name or contact information.\n     These two tables have only one thing in common: the ID column (the key). But because of that common column, the relational database can create a relationship between the two tables. Then, when the company’s order processing application submits an order to the database, the database can go to the customer order table, pull the correct information about the product order, and use the customer ID from that table to look up the customer’s billing and shipping information in the customer info table. The warehouse can then pull the correct product, the customer can receive timely delivery of the order, and the company can get paid.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#relational-database-management-systems-rdbms",
    "href": "data_management.html#relational-database-management-systems-rdbms",
    "title": "6  Data Management in Accounting",
    "section": "6.3 Relational Database Management Systems (RDBMS)",
    "text": "6.3 Relational Database Management Systems (RDBMS)\n\n     While a relational database organizes data based off a relational data model, a relational database management system (RDBMS) is a more specific reference to the underlying database software that enables users to maintain it. These programs allow users to create, update, insert, or delete data in the system, and they provide:\n\nData structure\nMulti-user access\nPrivilege control\nNetwork access\n\n     Examples of popular RDBMS systems include MySQL, PostgreSQL, and IBM DB2. Additionally, a relational database system differs from a basic database management system (DBMS) in that it stores data in tables while a DBMS stores information as files.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#creating-a-relational-database-using-postgresql",
    "href": "data_management.html#creating-a-relational-database-using-postgresql",
    "title": "6  Data Management in Accounting",
    "section": "6.4 Creating a Relational Database Using PostgreSQL",
    "text": "6.4 Creating a Relational Database Using PostgreSQL\n\n     PostgreSQL is a widely used open-source Relational Database Management System (RDBMS). Since its release in 1996, PostgreSQL has carved a niche as one of the most robust and reliable database systems in the Tech industry. PostgreSQL is known for its rich feature set, reliability, scalability, and strong community support network.\n\n\n6.4.1 Installing PostgreSQL\n\n     To install PostgreSQL locally on your computer, visit the installer by EDB, and download the newest version compatible with your operating system. To install PostgreSQL on Windows, you need to have administrator privileges. Below, we delineate the steps to install the PostgreSQL in windows.\n     Step 01: When the downloading is complete, double click the downloaded file and an installation wizard will appear and guide you through multiple steps. Click “Next” to specify directory.\n\n\n\nInstallation Wizard\n\n\n     Step 02: You can specify the location of PostgreSQL, or go with the default choice\n\n\n\nInstallation Directory\n\n\n     Step 03: To use PostgreSQL, you need to install PostgreSQL server. For our installation and uses purposes, we select the following servers - PostgreSQL Server, pgAdmin4, and Command Line Tools.\n\n\n\nServer Selection\n\n\n     Step 04: Then, we need to choose the storage directory. Alternatively, we can go with the default directory.\n\n\n\nStorage Directory\n\n\n     Step 05: At this point, you need to choose your passowrd. It is important to note that the password will be used to connect to the database. Therefore, you must remember your password.\n\n\n\nPassword Selection\n\n\n     Step 06: Next you need to select the port. The default port number is 5432. If you change the port, you also need to remember the port number because like passowrd, you need to use the port number to connect to the database.\n\n\n\nPort Selection\n\n\n     Step 07: We also need to select the geographical location of the database server. We can go with the default choice.\n\n\n\nLocale Selection\n\n\nThen if everything looks ok, you can move forward by clicking ‘Next’. However, it is recommended that you copy and save the information from the “Pre Installation Summary” in some safe place so that you can access them in future, if necessary.\n\n\n\nPre Installation Summary\n\n\n     Step 08: Then you can click “Next” in ‘Ready to Install’ windows and the installation process will start and it will take a while to finish the installation. Once the installing is done, please click ‘Finish’ button on ‘Complete the PostgreSQL Setup Wizard’ window.\n  \n\n\n\n\n\n\n\nTip\n\n\n\n\n\nSome good sources to learn more about PostgreSQL -\n\nW3 Schools\nPostgreSQL Tutorial",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#connecting-to-the-relational-database",
    "href": "data_management.html#connecting-to-the-relational-database",
    "title": "6  Data Management in Accounting",
    "section": "6.5 Connecting to the Relational Database",
    "text": "6.5 Connecting to the Relational Database\n\n     In Section 6.4.1, we described the steps of installing PostgreSQL database; now, we will try to connect to that database. There are several ways to connect the database we created. These ways include -\n\nSQL Shell (psql)\nCommand Shell (cmd)\npgAdmin 4\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nTo connect to PostgreSQL Database using pgAdmin 4, pleae use the following link -\n\nConnecting using pgAdmin 4\n\n\n\n\n\n6.5.1 Connection using SQL Shell (psql)\n\n      First, we will discuss SQL Shell (psql) method. Basically, SQL Shell (psql) is a terminal based program where you can write and execute SQL syntax in the command-line terminal. You will find the SQL Shell (psql) in the start menu under PostgreSQL. If you do not find it please search “psql” on windows search box.\n\n\n\nSQL Shell on Start Menu\n\n\n     Once the program is open, you should be able to see a window like below. Usually, the name of the server is localhost as shown below; however, if you choose a different server while installing the PostgreSQL, you must mention your server name. Then, click “enter” on your keyboard.\n\n\n\nServer\n\n\n     The default name of your database is postgres as shown below. If you choose a different name, please enter that name. Then, click “enter” on your keyboard.\n\n\n\nDatabase\n\n\n     The default port number is 5432. If you use the default port number, then please click “enter” on your keyboard. Otherwise, use the port number that you used while installing PostgreSQL and click “enter” on your keyboard.\n\n\n\nPort\n\n\n     Similarly, the default user name is postgres. Please click “enter” on your keyboard if your default name is the same.\n\n\n\nUsername\n\n\n     Next, you need to provide password that you set while installing the database.\n\n\n\nPassword\n\n\n\n\n\n6.5.2 Connection using Command Shell (cmd)\n\n     To use the command prompt (cmd) to connect to PostgreSQL database, you need to use the following steps -\n\nStep # 01 - Open command prompt by writing cmd in windows search bar as shown below. Then command prompt will open.\n\n\n\n\nSearch Bar on Windows\n\n\n\nStep # 02 - type cd C:\\Program Files\\PostgreSQL\\16\\bin on the command prompt and hit enter on the keyboard\nStep # 03 - type psql -h localhost -p 5432 -d postgres -U postgres\nStep # 04 - Now provide the password that you set while installing PostgreSQL\n\nThese steps should ensure that you are connected to the database.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#sec-dealdatabase",
    "href": "data_management.html#sec-dealdatabase",
    "title": "6  Data Management in Accounting",
    "section": "6.6 Dealing with the Database",
    "text": "6.6 Dealing with the Database\n\n     Once the database is created, new tables can be created. Also, different kinds of data manipulation can be performed. For example, after being connected to the database, one should check the database available by running the following code -\n\n\\l \n\n# or \n\n\\list\n\n     To connect to a database, one should run the following code -\n\n\\c name_of_the_database;\n\n     To see all tables in a database, one should run the following code -\n\n\\dt;\n\n\n\n6.6.1 Creating a Database and Tables inside the Database\n\n     To create a database (e.g., my_database), one can run the following code -\n\nCREATE DATABASE my_database;\n\n     To create a table, following code can be run -\n\n# we are going to create a table called - People\n\nCREATE TABLE People (\n pid int not null,\n prefix text,\n firstName text,\n lastName text,\n suffix text,\n homeCity text,\n DOB date,\n primary key(pid)\n);\n\n     Similarly, four more tables are created by running the following code -\n\n# Table - Customer \n\nCREATE TABLE Customers (\n pid int not null references People(pid),\n paymentTerms text,\n discountPct decimal(5,2),\n primary key(pid)\n);\n\n# Table - Agent \n\nCREATE TABLE Agents (\n pid int not null references People(pid),\n paymentTerms text,\n commissionPct decimal(5,2),\n primary key(pid)\n);\n\n# Table - Products \n\nCREATE TABLE Products (\n prodId char(3) not null,\n name text,\n city text,\n qtyOnHand int,\n priceUSD numeric(10,2),\n primary key(prodId)\n);\n\n# Table - Orders \n\nCREATE TABLE Orders (\n orderNum int not null,\n dateOrdered date not null,\n custId int not null references Customers(pid),\n agentId int not null references Agents(pid),\n prodId char(3) not null references Products(prodId),\n  quantityOrdered integer,\n totalUSD numeric(12,2),\n primary key(orderNum)\n);\n\n     Next, we will populate the tables with different kinds of data.\n\n# Inserting records into table - People \n\nINSERT INTO People (pid, prefix, firstName, lastName, suffix, homeCity, DOB)\nVALUES\n (001, 'Dr.', 'Neil', 'Peart', 'Ph.D.', 'Toronto', '1952-09-12'),\n (002, 'Ms.', 'Regina', 'Schock', NULL, 'Toronto', '1957-08-31'),\n (003, 'Mr.', 'Bruce', 'Crump', 'Jr.', 'Jacksonville', '1957-07-17'),\n (004, 'Mr.', 'Todd', 'Sucherman', NULL, 'Chicago', '1969-05-02'),\n (005, 'Mr.', 'Bernard', 'Purdie', NULL, 'Teaneck', '1939-06-11'),\n (006, 'Ms.', 'Demetra', 'Plakas', 'Esq.', 'Santa Monica', '1960-11-09'),\n (007, 'Ms.', 'Terri Lyne', 'Carrington', NULL, 'Boston', '1965-08-04'),\n (008, 'Dr.', 'Bill', 'Bruford', 'Ph.D.', 'Kent', '1949-05-17'),\n (009, 'Mr.', 'Alan', 'White', 'III', 'Pelton', '1949-06-14')\n;\n\n# Inserting records into table - Customers \n\nINSERT INTO Customers (pid, paymentTerms, discountPct)\nVALUES\n (001, 'Net 30' , 21.12),\n (004, 'Net 15' , 4.04),\n (005, 'In Advance', 5.50),\n (007, 'On Receipt', 2.00),\n (008, 'Net 30' , 10.00)\n;\n\n# Inserting records into table - Agents \n\nINSERT INTO Agents (pid, paymentTerms, commissionPct)\nVALUES\n (002, 'Quarterly', 5.00),\n (003, 'Annually', 10.00),\n (005, 'Monthly', 2.00),\n (006, 'Weekly', 1.00)\n;\n\n# Inserting records into table - Products \n\nINSERT INTO Products( prodId, name, city, qtyOnHand, priceUSD )\nVALUES\n('p01', 'Heisenberg Compensator', 'Dallas', 47,  67.50),\n('p02', 'Universal Translator', 'Newark', 2399, 5.50 ),\n('p03', 'Commodore PET', 'Duluth', 1979, 65.02 ),\n('p04', 'LCARS module', 'Duluth', 3, 47.00 ),\n('p05', 'Remo drumhead', 'Dallas', 8675309, 16.61 ),\n('p06', 'Trapper Keeper', 'Dallas', 1982, 2.00 ),\n('p07', 'Flux Capacitor', 'Newark', 1007, 1.00 ),\n('p08', 'HAL 9000 memory core', 'Newark', 200, 1.25 ),\n('p09', 'Red Barchetta',  'Toronto', 1, 379000.47 )\n;\n\n\n# Inserting records into table - Orders \n\nINSERT INTO Orders(orderNum, dateOrdered, custId, agentId, prodId, quantityOrdered, totalUSD)\nVALUES\n(1011, '2020-01-23', 001, 002, 'p01', 1100, 58568.40),\n(1012, '2020-01-23', 004, 003, 'p03', 1200, 74871.83),\n(1015, '2020-01-23', 005, 003, 'p05', 1000, 15696.45),\n(1016, '2020-01-23', 008, 003, 'p01', 1000, 60750.00),\n(1017, '2020-02-14', 001, 003, 'p03', 500, 25643.88),\n(1018, '2020-02-14', 001, 003, 'p04', 600, 22244.16),\n(1019, '2020-02-14', 001, 002, 'p02', 400, 1735.36),\n(1020, '2020-02-14', 004, 005, 'p07', 600, 575.76),\n(1021, '2020-02-14', 004, 005, 'p01', 1000, 64773.00),\n(1022, '2020-03-15', 001, 003, 'p06', 450, 709.92),\n(1023, '2020-03-15', 001, 002, 'p05', 500, 6550.984),\n(1024, '2020-03-15', 005, 002, 'p01', 880, 56133.00),\n(1025, '2020-04-01', 008, 003, 'p07', 888, 799.20),\n(1026, '2020-05-01', 008, 005, 'p03', 808, 47282.54)\n;\n\n\n\n\n6.6.2 Querying the Database\n\n     Once the database is created, we need to retrieve data from the database for which we need to write queries. For example, we want retireve prodid, name, and city from the table products. Then, we need to write the following query -\n\n# SQL Query = \nSELECT prodid, name, city FROM products;\n\n# Output \n\n prodid |          name          |  city\n--------+------------------------+---------\n p01    | Heisenberg Compensator | Dallas\n p02    | Universal Translator   | Newark\n p03    | Commodore PET          | Duluth\n p04    | LCARS module           | Duluth\n p05    | Remo drumhead          | Dallas\n p06    | Trapper Keeper         | Dallas\n p07    | Flux Capacitor         | Newark\n p08    | HAL 9000 memory core   | Newark\n p09    | Red Barchetta          | Toronto\n(9 rows)\n\n     Similarly, if we want to collect all fields from products table for the city Dallas, then we can write the following queries -\n\n# SQL Query = \nSELECT * FROM products WHERE city = 'Dallas';\n\n\n# Output \n\n prodid |          name          |  city  | qtyonhand | priceusd\n--------+------------------------+--------+-----------+----------\n p01    | Heisenberg Compensator | Dallas |        47 |    67.50\n p05    | Remo drumhead          | Dallas |   8675309 |    16.61\n p06    | Trapper Keeper         | Dallas |      1982 |     2.00\n(3 rows)\n\n\n\n\n6.6.3 Saving a Database from PostgreSQL\n\n     In order to share a database with stakeholders, we need to save the a particular database. Therefore, knowing how to save the database is very important. We must use the Command Prompt (cmd) to save a database. First, we need to open a command prompt following step # 01 in Section 6.5.2. Then we need to write the following codes on command prompt -\n\n# First line of code and then hit enter on the Keyboard\ncd ..\n# Second line of code and then hit enter on the Keyboard\ncd ..\n# Third line of code and then hit enter on the Keyboard\ncd \\Program Files\\PostgreSQL\\16\\bin\n# Fourth line of code and then hit enter on the Keyboard\npg_dump -h localhost -d name_of_your_database -U postgres -p 5433 -F tar &gt;K:\\name_of_your_database.tar\n# Fifth Line of code - provide your password\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nYou can watch the youtube video to learn more about how to save a database from PostgreSQL - https://www.youtube.com/watch?v=sa5VXDG_aW8\n\n\n\n     In the fourth line of code above - pg_dump -h localhost -d name_of_your_database -U postgres -p 5433 -F tar &gt;K:\\name_of_your_database.tar- the “name_of_your_database” is the database name that you want to save to share and “K:\\\\” is the directory (folder) address (path) on which you want to save your database and “name_of_your_database” is the name of the database that you would like to assign to the saved database and “.tar” is the file extension. It is important to note that when you run the fourth line of code above, it might show the message - “Access is denied”. In such situation, you should change the path (address) of the directory in which you would like to save the database.\n\n\n\n6.6.4 Uploading a Database to PostgreSQL\n\n     Sometimes, you might need to upload a database to PostgreSQL. Therefore, you should know how you can upload the database to PostgreSQL. First, you should create a database following the process as described in Section 6.6.1. For example, we create a database called my_database in PostgreSQL. Assume that the database that we want to upload to PostgreSQL is in the path - K:\\ and the name of the database is my_database_upload. Therefore, the complete path of the database to be uploaded is - “K:\\my_database_upload.tar”. Then, you should open a command prompt as described in Section 6.5.2. Then run the following code -\n\n# First line of code and then hit enter on the Keyboard\ncd ..\n# Second line of code and then hit enter on the Keyboard\ncd ..\n# Third line of code and then hit enter on the Keyboard\ncd \\Program Files\\PostgreSQL\\16\\bin\n# Fourth line of code and then hit enter on the Keyboard\npg_restore -h localhost -p 5433 -d my_database -U postgres -v \"K:\\my_database_upload.tar\"\n# Fifth Line of code - provide your password\n\n     In the fourth line of code above - pg_restore -h localhost -p 5433 -d my_database -U postgres -v \"K:\\my_database_upload.tar\"- “my_database” is the name of the database that you created on the PostgreSQL and “K:\\my_database_upload.tar” is the path of the database to be uploaded.\n     Once you upload the database to the PostgreSQL , you can check whether the database is uploaded by following Section 6.6.",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#querying-postgresql-database-using-r",
    "href": "data_management.html#querying-postgresql-database-using-r",
    "title": "6  Data Management in Accounting",
    "section": "6.7 Querying PostgreSQL Database Using R",
    "text": "6.7 Querying PostgreSQL Database Using R\n\n# Importing Necessary Packages\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(askpass)\nlibrary(tidyverse)\n\n     In order to connect to the local postgreSQL database, you can run the following code. However, there is a problem that if you publish this connection code, your password will be shared; therefore, it is better not to share the password.\n\n# Connecting to PostgreSQL Database \ncon = dbConnect(RPostgres::Postgres()\n                 , host='localhost'\n                 , port='5433'\n                 , dbname='my_database',\n                 , user='postgres'\n                 , password=\"YourPassword\") \n\n     For the below code, first we set the password in environment variable in R environment. Since I use IDE Positron (or VS Code), I get access to the environment by running the following code. Then, the password is stored by running the code - POSTGRESS_PASSWORD=\"YOURPASSWORD\". Replace, the word “YOURPASSWORD” with your own password and save the file.\n\nlibrary(usethis)\nusethis::edit_r_environ()\n\n\n# Connecting to PostgreSQL Database \ncon = dbConnect(RPostgres::Postgres()\n                 , host='localhost'\n                 , port='5433'\n                 , dbname='my_database',\n                 , user='postgres'\n                 , password= Sys.getenv(\"POSTGRESS_PASSWORD\")\n                 )\n\n     Alternatively, you can use askpass package to input your password without sharing your password with third party.\n\n# Connecting to PostgreSQL Database \ncon = dbConnect(RPostgres::Postgres()\n                 , host='localhost'\n                 , port='5433'\n                 , dbname='my_database',\n                 , user='postgres'\n                 , password=askpass::askpass(\"Enter your database password (R):\")\n                 )\n\n\n# Checking all Tables in the Database \ndbListTables(con)\n\n[1] \"agents\"    \"customers\" \"orders\"    \"people\"    \"products\" \n\n\n\n# To see a table \ncon %&gt;%\n  tbl ('orders') \n\n# Source:   table&lt;\"orders\"&gt; [?? x 7]\n# Database: postgres  [postgres@localhost:5433/my_database]\n   ordernum dateordered custid agentid prodid quantityordered totalusd\n      &lt;int&gt; &lt;date&gt;       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;            &lt;int&gt;    &lt;dbl&gt;\n 1     1011 2020-01-23       1       2 p01               1100   58568.\n 2     1012 2020-01-23       4       3 p03               1200   74872.\n 3     1015 2020-01-23       5       3 p05               1000   15696.\n 4     1016 2020-01-23       8       3 p01               1000   60750 \n 5     1017 2020-02-14       1       3 p03                500   25644.\n 6     1018 2020-02-14       1       3 p04                600   22244.\n 7     1019 2020-02-14       1       2 p02                400    1735.\n 8     1020 2020-02-14       4       5 p07                600     576.\n 9     1021 2020-02-14       4       5 p01               1000   64773 \n10     1022 2020-03-15       1       3 p06                450     710.\n# ℹ more rows\n\n\n\ncon %&gt;%\n  tbl ('orders') %&gt;%\n  collect ()\n\n# A tibble: 14 × 7\n   ordernum dateordered custid agentid prodid quantityordered totalusd\n      &lt;int&gt; &lt;date&gt;       &lt;int&gt;   &lt;int&gt; &lt;chr&gt;            &lt;int&gt;    &lt;dbl&gt;\n 1     1011 2020-01-23       1       2 p01               1100   58568.\n 2     1012 2020-01-23       4       3 p03               1200   74872.\n 3     1015 2020-01-23       5       3 p05               1000   15696.\n 4     1016 2020-01-23       8       3 p01               1000   60750 \n 5     1017 2020-02-14       1       3 p03                500   25644.\n 6     1018 2020-02-14       1       3 p04                600   22244.\n 7     1019 2020-02-14       1       2 p02                400    1735.\n 8     1020 2020-02-14       4       5 p07                600     576.\n 9     1021 2020-02-14       4       5 p01               1000   64773 \n10     1022 2020-03-15       1       3 p06                450     710.\n11     1023 2020-03-15       1       2 p05                500    6551.\n12     1024 2020-03-15       5       2 p01                880   56133 \n13     1025 2020-04-01       8       3 p07                888     799.\n14     1026 2020-05-01       8       5 p03                808   47283.\n\n\n\n# Selecting some columns \ncon %&gt;%\n  tbl ('orders') %&gt;%\n  select (ordernum, dateordered, custid, agentid)\n\n# Source:   SQL [?? x 4]\n# Database: postgres  [postgres@localhost:5433/my_database]\n   ordernum dateordered custid agentid\n      &lt;int&gt; &lt;date&gt;       &lt;int&gt;   &lt;int&gt;\n 1     1011 2020-01-23       1       2\n 2     1012 2020-01-23       4       3\n 3     1015 2020-01-23       5       3\n 4     1016 2020-01-23       8       3\n 5     1017 2020-02-14       1       3\n 6     1018 2020-02-14       1       3\n 7     1019 2020-02-14       1       2\n 8     1020 2020-02-14       4       5\n 9     1021 2020-02-14       4       5\n10     1022 2020-03-15       1       3\n# ℹ more rows\n\n\n\n# Filtering some rows\ncon %&gt;% \n  tbl ('products') %&gt;%\n  filter (city == \"Dallas\")\n\n# Source:   SQL [3 x 5]\n# Database: postgres  [postgres@localhost:5433/my_database]\n  prodid name                   city   qtyonhand priceusd\n  &lt;chr&gt;  &lt;chr&gt;                  &lt;chr&gt;      &lt;int&gt;    &lt;dbl&gt;\n1 p01    Heisenberg Compensator Dallas        47     67.5\n2 p05    Remo drumhead          Dallas   8675309     16.6\n3 p06    Trapper Keeper         Dallas      1982      2  \n\n\n\n# showing the SQL query\ncon %&gt;% \n  tbl ('products') %&gt;%\n  filter (city == \"Dallas\") %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT \"products\".*\nFROM \"products\"\nWHERE (\"city\" = 'Dallas')\n\n\n\n# Disconnecting the database \ncon %&gt;% dbDisconnect ()",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#querying-postgresql-database-using-python",
    "href": "data_management.html#querying-postgresql-database-using-python",
    "title": "6  Data Management in Accounting",
    "section": "6.8 Querying PostgreSQL Database Using Python",
    "text": "6.8 Querying PostgreSQL Database Using Python\n\n# Importing Necessary Python Packages \nimport ibis\nimport getpass\nimport os \n\n\n# To know more about the next line of code\n# see - https://ibis-project.org/tutorials/ibis-for-dplyr-users\nibis.options.interactive = True\n\n\n# to connect to the database \nconn = ibis.postgres.connect(\n    user='postgres', \n    password = 'YourPassword',\n    host = \"localhost\",\n    port = 5433,\n    database = \"my_database\" # here our database name will be 'dvdrental'\n)\n\n     Like before, if you do not share your password, please get the password from environment.\n\n# to connect to the database \nconn = ibis.postgres.connect(\n    user='postgres', \n    password = os.getenv(\"POSTGRESS_PASSWORD\"),\n    host = \"localhost\",\n    port = 5433,\n    database = \"dvdrental\"\n)\n\n     Alternatively, you can use getpass python package, which is very similar to askpass package in R.\n\n# to connect to the database \nconn = ibis.postgres.connect(\n    user='postgres', \n    password = getpass.getpass(\"Enter your database password (Python): \"),\n    host = \"localhost\",\n    port = 5433,\n    database = \"dvdrental\"\n)\n\n\n# See all tables in a database \nconn.list_tables()\n\n['actor', 'actor_info', 'address', 'category', 'city', 'country', 'customer', 'customer_list', 'ff48_df', 'film', 'film_actor', 'film_category', 'film_list', 'inventory', 'language', 'nicer_but_slower_film_list', 'payment', 'rental', 'sales_by_film_category', 'sales_by_store', 'staff', 'staff_list', 'store']\n\n\n\n# To know the total number of rows \nconn.table('actor').count() # total rows \n\n\n\n\n\nconn.table('actor').columns # name of the columns \n\n['actor_id', 'first_name', 'last_name', 'last_update']\n\n\n\n# To see the SQL command \nconn.table('actor').compile()\n\n'SELECT * FROM \"actor\"'\n\n\n\n# Dealing with a table \nconn.table('country')\n\n┌────────────┬────────────────┬─────────────────────┐\n│ country_id │ country        │ last_update         │\n├────────────┼────────────────┼─────────────────────┤\n│ !int32     │ !string        │ !timestamp(6)       │\n├────────────┼────────────────┼─────────────────────┤\n│          1 │ Afghanistan    │ 2006-02-15 09:44:00 │\n│          2 │ Algeria        │ 2006-02-15 09:44:00 │\n│          3 │ American Samoa │ 2006-02-15 09:44:00 │\n│          4 │ Angola         │ 2006-02-15 09:44:00 │\n│          5 │ Anguilla       │ 2006-02-15 09:44:00 │\n│          6 │ Argentina      │ 2006-02-15 09:44:00 │\n│          7 │ Armenia        │ 2006-02-15 09:44:00 │\n│          8 │ Australia      │ 2006-02-15 09:44:00 │\n│          9 │ Austria        │ 2006-02-15 09:44:00 │\n│         10 │ Azerbaijan     │ 2006-02-15 09:44:00 │\n│          … │ …              │ …                   │\n└────────────┴────────────────┴─────────────────────┘\n\nconn.table('country').head(5).execute()\n\n   country_id         country         last_update\n0           1     Afghanistan 2006-02-15 09:44:00\n1           2         Algeria 2006-02-15 09:44:00\n2           3  American Samoa 2006-02-15 09:44:00\n3           4          Angola 2006-02-15 09:44:00\n4           5        Anguilla 2006-02-15 09:44:00\n\n\n\n## describe () or columns () equivalent to glimpse ()\nconn.table('country').describe\n\n&lt;bound method Table.describe of ┌────────────┬────────────────┬─────────────────────┐\n│ country_id │ country        │ last_update         │\n├────────────┼────────────────┼─────────────────────┤\n│ !int32     │ !string        │ !timestamp(6)       │\n├────────────┼────────────────┼─────────────────────┤\n│          1 │ Afghanistan    │ 2006-02-15 09:44:00 │\n│          2 │ Algeria        │ 2006-02-15 09:44:00 │\n│          3 │ American Samoa │ 2006-02-15 09:44:00 │\n│          4 │ Angola         │ 2006-02-15 09:44:00 │\n│          5 │ Anguilla       │ 2006-02-15 09:44:00 │\n│          6 │ Argentina      │ 2006-02-15 09:44:00 │\n│          7 │ Armenia        │ 2006-02-15 09:44:00 │\n│          8 │ Australia      │ 2006-02-15 09:44:00 │\n│          9 │ Austria        │ 2006-02-15 09:44:00 │\n│         10 │ Azerbaijan     │ 2006-02-15 09:44:00 │\n│          … │ …              │ …                   │\n└────────────┴────────────────┴─────────────────────┘&gt;\n\n\n\n# filtering rows    \nconn.table('country')[conn.table('country')['country']==\"Zambia\"] \\\n    .execute()\n\n   country_id country         last_update\n0         109  Zambia 2006-02-15 09:44:00\n\n\n\n# Selecting Columns \nconn.table('country')[[\"country_id\", \"country\"]]\n\n┌────────────┬────────────────┐\n│ country_id │ country        │\n├────────────┼────────────────┤\n│ !int32     │ !string        │\n├────────────┼────────────────┤\n│          1 │ Afghanistan    │\n│          2 │ Algeria        │\n│          3 │ American Samoa │\n│          4 │ Angola         │\n│          5 │ Anguilla       │\n│          6 │ Argentina      │\n│          7 │ Armenia        │\n│          8 │ Australia      │\n│          9 │ Austria        │\n│         10 │ Azerbaijan     │\n│          … │ …              │\n└────────────┴────────────────┘\n\n\n\n# Arrange \n\nconn.table('rental') \\\n    .order_by(['inventory_id'])\n\n┌───────────┬─────────────────────┬──────────────┬─────────────┬───┐\n│ rental_id │ rental_date         │ inventory_id │ customer_id │ … │\n├───────────┼─────────────────────┼──────────────┼─────────────┼───┤\n│ !int32    │ !timestamp(6)       │ !int32       │ !int16      │ … │\n├───────────┼─────────────────────┼──────────────┼─────────────┼───┤\n│      4863 │ 2005-07-08 19:03:15 │            1 │         431 │ … │\n│     11433 │ 2005-08-02 20:13:10 │            1 │         518 │ … │\n│     14714 │ 2005-08-21 21:27:43 │            1 │         279 │ … │\n│       972 │ 2005-05-30 20:21:07 │            2 │         411 │ … │\n│      2117 │ 2005-06-17 20:24:00 │            2 │         170 │ … │\n│      4187 │ 2005-07-07 10:41:31 │            2 │         161 │ … │\n│      9449 │ 2005-07-30 22:02:34 │            2 │         581 │ … │\n│     15453 │ 2005-08-23 01:01:01 │            2 │         359 │ … │\n│     10126 │ 2005-07-31 21:36:07 │            3 │          39 │ … │\n│     15421 │ 2005-08-22 23:56:37 │            3 │         541 │ … │\n│         … │ …                   │            … │           … │ … │\n└───────────┴─────────────────────┴──────────────┴─────────────┴───┘\n\n\n\n# Group By\nconn.table('payment')[['payment_id', 'customer_id', 'staff_id', 'amount']] \\\n    .execute() \\\n    .groupby('customer_id').agg({'amount': 'mean'}) \\\n    .reset_index() \n\n     customer_id    amount\n0              1  3.823333\n1              2  4.759231\n2              3  5.448333\n3              4  3.717273\n4              5  3.847143\n..           ...       ...\n594          595  3.817586\n595          596  3.353636\n596          597  3.816087\n597          598  3.808182\n598          599  4.378889\n\n[599 rows x 2 columns]\n\n\n\n# To disconnect the database \nconn.disconnect()",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#online-analytical-processing-olap-database-management-system",
    "href": "data_management.html#online-analytical-processing-olap-database-management-system",
    "title": "6  Data Management in Accounting",
    "section": "6.9 Online Analytical Processing (OLAP) Database Management System",
    "text": "6.9 Online Analytical Processing (OLAP) Database Management System",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "data_management.html#conclusion",
    "href": "data_management.html#conclusion",
    "title": "6  Data Management in Accounting",
    "section": "6.10 Conclusion",
    "text": "6.10 Conclusion",
    "crumbs": [
      "Data Exploration and Management",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management in Accounting</span>"
    ]
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "10  Natural Language Processing (NLP)",
    "section": "",
    "text": "Learning Objectives of the Chapter",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#introduction",
    "href": "nlp.html#introduction",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\n\n    In today’s data driven world, a significant amount data is produced each day. For example, Google processes 24 peta bytes of data every day; 10 million photos are uploaded every hour on Facebook; and 400 million tweets are posted on X (formerly Twitter). Of these amount of data, a significant portion consists of text data. Therefore, it it important to gain insights from text data.\n    Natural Language Processing (NLP), according to IBM, is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language. Specifically, NLP involves understanding, interpreting, and extracting insights from human language. Businesses use NLP for many purposes such as processing and analyzing large volume of documents, analyzing customer reviews, and scaling customer services (like developing chatbots or virtual assistants).",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#python-libraries-for-nlp",
    "href": "nlp.html#python-libraries-for-nlp",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.2 Python Libraries for NLP",
    "text": "10.2 Python Libraries for NLP\n\n    Python has built a rich and efficient ecosystem for NLP. Some of the most popular python modules (libraries) for NLP include - nltk (Natural Language Toolkit); SpaCy; gensim; and TextBlob.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#steps-in-nlp",
    "href": "nlp.html#steps-in-nlp",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.3 Steps in NLP",
    "text": "10.3 Steps in NLP\n\n    Like other data science tools or techniques, NLP involves several steps because most the time text data is not readily available or even if they are available, we need to clean the data and make it ready for next step processing. In this section, several important steps, which are called preprocessing, of NLP will be discussed.\n\n\n10.3.1 Preprocessing\n\n    Before applying NLP techniques, it is necessary to preprosess and clean the text data. Therefore, the processes involving cleaning and preparing text data to get them ready for NLP models are called preprocessing. Preprocessing is very important in NLP to get effective and accurate insights from the data. Below we will discuss several important concepts of preprocessing.\n\n\n# An exmaple of a text data \nmy_text = \"\"\"\nAccounting is the systematic process of recording, analyzing, and reporting financial \\\ntransactions. It helps businesses track their income, expenses, and overall financial \\\nhealth. Accountants use various financial statements, such as balance sheets and income \\\nstatements, to summarize a company's financial position. Double-entry bookkeeping is a \\\nfundamental principle in accounting, ensuring that every transaction affects at least two \\\naccounts. Financial accounting focuses on providing information to external stakeholders, \\\nsuch as investors and creditors, while managerial accounting provides information to \\\ninternal stakeholders, like managers, to aid in decision-making. Auditing is an essential \\\naspect of accounting, involving the examination of financial records to ensure accuracy \\\nand compliance. Tax accounting deals with preparing tax returns and planning for \\\nfuture tax obligations. Forensic accounting involves investigating financial discrepancies \\\nand fraud. Accounting software, like QuickBooks and Xero, has revolutionized the way \\\nbusinesses manage their finances, making the process more efficient and accurate. \\\nOverall, accounting plays a crucial role in the financial management and transparency \\\nof businesses and organizations.\n\"\"\"\n\n\n10.3.1.1 Tokenization\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nnltk.download(\"punkt_tab\")\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n\n# sentence tokenize\nmy_text_sent = sent_tokenize(my_text)\nmy_text_sent[0:5]\n\n['\\nAccounting is the systematic process of recording, analyzing, and reporting financial transactions.',\n 'It helps businesses track their income, expenses, and overall financial health.',\n \"Accountants use various financial statements, such as balance sheets and income statements, to summarize a company's financial position.\",\n 'Double-entry bookkeeping is a fundamental principle in accounting, ensuring that every transaction affects at least two accounts.',\n 'Financial accounting focuses on providing information to external stakeholders, such as investors and creditors, while managerial accounting provides information to internal stakeholders, like managers, to aid in decision-making.']\n\n\n\n# word tokenize\nmy_text_word = word_tokenize(my_text)\nmy_text_word[0:5]\n\n['Accounting', 'is', 'the', 'systematic', 'process']\n\n\n\n\n10.3.1.2 Removing Punctuation\n\n    It is evident that in our word tokens, punctuations like comma (,), full stop (.) are also included, but they are unncessary. Therefore, we need to eliminate them from the token list.\n\n\nimport string\nmy_text_nopunc = [x for x in my_text_word if x not in string.punctuation]\nmy_text_nopunc[:11]\n\n['Accounting',\n 'is',\n 'the',\n 'systematic',\n 'process',\n 'of',\n 'recording',\n 'analyzing',\n 'and',\n 'reporting',\n 'financial']\n\n\n\n\n10.3.1.3 Filtering Stop Words\n\n    Stop words are the words that we want to ignore. Words like “in”, “an”, “the” we want to ignore. Therefore, in this step, we want to filter out these kinds of words.\n\n\nnltk.download(\"stopwords\") # to download the stopwords from NLTK repository\nfrom nltk.corpus import stopwords # imports the module \nstop_words = set(stopwords.words(\"english\")) # access the stopwords for english \n# print(stop_words)\n\n\nmy_text_nostopwords = [x for x in my_text_nopunc if x.lower() not in stop_words]\nmy_text_nostopwords[0:11]\n\n['Accounting',\n 'systematic',\n 'process',\n 'recording',\n 'analyzing',\n 'reporting',\n 'financial',\n 'transactions',\n 'helps',\n 'businesses',\n 'track']\n\n\n\n    Still we can see there are some unnessary words in the list. So, we need to eliminate them. For example, “’s” is in the my_text_nostopwords. We need to get rid of it.\n\n\n\"'s\" in my_text_nostopwords\nmy_text_nostopwords = [x for x in my_text_nostopwords if \"'s\" not in x]\n\"'s\" in my_text_nostopwords\n\nFalse\n\n\n\n\n10.3.1.4 Stemming\n\n    Stemming is the process of reducing the words to their base or root form. For example, the token list contains words like recording, reporting, analyzing and so on. The base form of those words are record, report, and analyze respectively. Therefore, we need to reduce those words to base form. Stemming will help to do so. For this purpose, there are several types of stemmers such as Porter stemmer, Lovins stemmer, Dawson stemmer, Krovetz stemmer, and Xerox stemmer.\n\n\nfrom nltk.stem import PorterStemmer,SnowballStemmer, LancasterStemmer\nporter = PorterStemmer()\nsnowball = SnowballStemmer(\"english\")\nlancaster = LancasterStemmer()\n[porter.stem(x) for x in my_text_nostopwords]\n[snowball.stem(x) for x in my_text_nostopwords]\n[lancaster.stem(x) for x in my_text_nostopwords][0:11]\n\n['account',\n 'system',\n 'process',\n 'record',\n 'analys',\n 'report',\n 'fin',\n 'transact',\n 'help',\n 'busy',\n 'track']\n\n\n\n\n10.3.1.5 Lemmatization\n\n    Lemmatization, like stemming, is the process of reducing a word to its base form, but, unlike stemming, it considers the context of the word.\n\n\nfrom nltk.stem import WordNetLemmatizer\nwordnet = WordNetLemmatizer()\nmy_text_lemmatized = [wordnet.lemmatize(x) for x in my_text_nostopwords]\nmy_text_lemmatized[:11]\n\n['Accounting',\n 'systematic',\n 'process',\n 'recording',\n 'analyzing',\n 'reporting',\n 'financial',\n 'transaction',\n 'help',\n 'business',\n 'track']\n\n\n\n\n10.3.1.6 Other Steps in Preprocessing\n\n    In addition to the above preprocessing, we might need to remove many other special characters from the text. These special characters include - hastags, HTML tags, links. For this purpose, knowledge about “regular expression” might be useful. Python built-in package re could be handy for regular expression. To learn more about regular expression - https://www.w3schools.com/python/python_regex.asp.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#visualization-of-words",
    "href": "nlp.html#visualization-of-words",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.4 Visualization of Words",
    "text": "10.4 Visualization of Words\n\n10.4.1 Word Cloud\n\n    Figure 10.1 shows a word cloud of our tokenized text.\n\n\nfrom wordcloud import WordCloud\n# We need a single string; So, it is tranformed below\nmy_text_lemmatizedSstring = ' '.join(my_text_lemmatized)\n# Word Cloud \nword_cloud = WordCloud(collocations = False, background_color = 'white').generate(my_text_lemmatizedSstring)\nimport matplotlib.pyplot as plt\nplt.imshow(word_cloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\nFigure 10.1: Word Cloud of the Words\n\n\n\n\n\n\n\n10.4.2 Bar Diagram of Word Frequency\n\n    Figure 10.2 shows the bar diagram of the words in tokenized list.\n\n\nfrom collections import Counter\n# calculate word frequencies \nword_freq = Counter(my_text_lemmatized)\n# extract word and their frequencies \nwords = list(word_freq.keys())\nfrequencies = list(word_freq.values())\n# create a data frame \nimport pandas as pd\nimport seaborn as sns\nword_df = pd.DataFrame(word_freq.items(), columns = ['Word', \"Frequency\"])\nword_df = word_df.sort_values(by='Frequency', ascending=False)\n# Create the bar diagram \nplt.figure(figsize=(10, 5)) \nsns.barplot(y='Word', x='Frequency', data=word_df[word_df['Frequency']&gt;1], palette='viridis') \nplt.ylabel('Words') \nplt.xlabel('Frequencies') \nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\nFigure 10.2: Bar Diagram of Word Frequency",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#vectorization-of-text",
    "href": "nlp.html#vectorization-of-text",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.5 Vectorization of Text",
    "text": "10.5 Vectorization of Text\n\n    Using word clouds or bar diagrams of most frequent words is easy to use and helps to quickly explore datasets. However, they suffer from some shortcomings. For example, they ignore context and are unable to capture the relations between words within the data. Therefore, sometimes we need convert the texts into numbers.\n    The process of converting text data into numeric vectors is called text vectorization. Text vectorization actually helps us to capture semantic relationships between words, thus allowing us to understand the meaning of text beyond just keywords. Further, it enhances processing of data. Moreover, vectorized text can be used for various NLP tasks such as document classification, sentiment analysis, and topic modeling. There are several techniques of text vectorization, spanning from simple to complex technieques.\n\n10.5.1 Bag of Words (BoW)\n    Bag of Words is the simplest text vectorization technique. BoW counts the frequency of words in text documents. BoW does not consider the order of the words or syntax. “Dog toy” or “toy dog” have equal importance in BoW. In BoW, each document is represented as a vector of word frequencies. Actually, BoW generates document-term matrix, which is a m×n matrix, where m is the document and n is the term (word). So, the cell in the matrix contains raw count of the number of times the \\(j\\)-word appear in the $i%-th document.\n\n\n10.5.2 Term Frequency-Inverse Document Frequency (TF-IDF)\n    TF-IDF is another text vectorization technique. It is basically an extension of BoW model and considers the importance or significance of words in the texts. Term Frequecny (TF) refers to the frequency at which a particular word appears in a document and IDF measures how rare a word is across a collection of documents. IDF assigns more weights to words that are frequent in a document, but rare across all documents1. Since TF-IDF takes into consideration the significance of the words, it is sometimes called Latent Semantic Analysis (LSA). Unlike BoW, the cell in TF-IDF matrix contains tf-idf score, which is calculated in Equation 10.12.\n\\[\nw_{i,j} = tf_{i,j} × log\\frac{N}{df_{i}}\n\\tag{10.1}\\]\n    In Equation 10.1, \\(tf_{i,j}\\) refers to the occurrences of term \\(j\\) in document \\(i\\); \\(N\\) is total number of documents; \\(df_{i}\\) is the number of documents that contain the term \\(j\\); and \\(w_{i,j}\\) is the tf-idf score in the TF-IDF matrix.\n\nsklearngensim\n\n\n\n# using sklearn package \n# preprocessing function \nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Download necessary NLTK data\nnltk.download('stopwords')\nnltk.download('wordnet')\n\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n    \n    return text\n\ndef preprocess_documents(documents): \n    return [preprocess_text(doc) for doc in documents]\n\n\n# Bag of Words (BoW)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntexts = [\"I love Accounting\", \"Accounting is called language of Business\", \"I will graduate with an Accounting degree\"]\ntexts_processed = preprocess_documents(texts)\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(texts_processed)\n# Get the feature names (terms) \nfeature_names = vectorizer.get_feature_names_out()\nfeature_names\n\narray(['accounting', 'business', 'called', 'degree', 'graduate',\n       'language', 'love'], dtype=object)\n\n\n\ntexts_processed\n\n['love accounting',\n 'accounting called language business',\n 'graduate accounting degree']\n\n\n\nprint(X.toarray())\n\n[[1 0 0 0 0 0 1]\n [1 1 1 0 0 1 0]\n [1 0 0 1 1 0 0]]\n\n\n\npd.DataFrame(X.toarray(), columns = feature_names)\n\n\n\n\n\n\n\n\n\naccounting\nbusiness\ncalled\ndegree\ngraduate\nlanguage\nlove\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n1\n1\n1\n1\n0\n0\n1\n0\n\n\n2\n1\n0\n0\n1\n1\n0\n0\n\n\n\n\n\n\n\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(texts_processed)\nprint(X.toarray())\n\n[[0.50854232 0.         0.         0.         0.         0.\n  0.861037  ]\n [0.32274454 0.54645401 0.54645401 0.         0.         0.54645401\n  0.        ]\n [0.38537163 0.         0.         0.65249088 0.65249088 0.\n  0.        ]]\n\n\n\npd.DataFrame(X.toarray(), \ncolumns = vectorizer.get_feature_names_out())\n\n\n\n\n\n\n\n\n\naccounting\nbusiness\ncalled\ndegree\ngraduate\nlanguage\nlove\n\n\n\n\n0\n0.508542\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.861037\n\n\n1\n0.322745\n0.546454\n0.546454\n0.000000\n0.000000\n0.546454\n0.000000\n\n\n2\n0.385372\n0.000000\n0.000000\n0.652491\n0.652491\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n# using gensim package\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom gensim import corpora, models, matutils\nimport pandas as pd\n\n# Download necessary NLTK data\nnltk.download('stopwords')\nnltk.download('wordnet')\n\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n    \n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    \n    # Lemmatization\n    lemmatizer = WordNetLemmatizer()\n    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n    \n    return text\n\ndef preprocess_documents(documents):\n    return [preprocess_text(doc).split() for doc in documents]\n\n# Sample documents\ndocuments = [\"I love Accounting\", \"Accounting is called language of Business\", \"I will graduate with an Accounting degree\"]\n\n# Preprocess the documents\ntexts_processed = preprocess_documents(documents)\n\n# Create a dictionary from the tokenized texts\ndictionary = corpora.Dictionary(texts_processed)\n\n# Create a corpus using the dictionary\ncorpus = [dictionary.doc2bow(text) for text in texts_processed]\n\n\n# Create a TF-IDF model from the corpus\ntfidf_model = models.TfidfModel(corpus)\n\n# Transform the corpus using the TF-IDF model\ncorpus_tfidf = tfidf_model[corpus]\n\n# Convert the TF-IDF weighted corpus to a dense matrix\ntfidf_matrix = matutils.corpus2dense(corpus_tfidf, num_terms=len(dictionary)).T\n\n# Convert the dense matrix to a DataFrame for easier inspection\ndf_tfidf_matrix = pd.DataFrame(tfidf_matrix, columns=[dictionary[i] for i in range(len(dictionary))])\n\ndf_tfidf_matrix\n\n\n\n\n\n\n\n\n\naccounting\nlove\nbusiness\ncalled\nlanguage\ndegree\ngraduate\n\n\n\n\n0\n0.0\n1.0\n0.00000\n0.00000\n0.00000\n0.000000\n0.000000\n\n\n1\n0.0\n0.0\n0.57735\n0.57735\n0.57735\n0.000000\n0.000000\n\n\n2\n0.0\n0.0\n0.00000\n0.00000\n0.00000\n0.707107\n0.707107\n\n\n\n\n\n\n\n\n\n# Convert the corpus to a document-term matrix (DTM)\ndtm_matrix = matutils.corpus2dense(corpus, num_terms=len(dictionary)).T\n# Convert the DTM to a DataFrame for easier inspection\ndf_dtm_matrix = pd.DataFrame(dtm_matrix, columns=[dictionary[i] for i in range(len(dictionary))])\ndf_dtm_matrix\n\n\n\n\n\n\n\n\n\naccounting\nlove\nbusiness\ncalled\nlanguage\ndegree\ngraduate\n\n\n\n\n0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n\n\n2\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n10.5.3 Word Embeddings\n    Word embeddings are dense vector representations of words (not documents) that capture semantic relationships. Popular models include Word2Vec, GloVe, and FastText.\n\nfrom gensim.models import Word2Vec\n\n# Sample tokenized texts\ntexts_processed = [\n    ['love', 'accounting'],\n    ['accounting', 'called', 'language', 'business'],\n    ['graduate', 'accounting', 'degree']\n]\n\n# Create the Word2Vec model\nmodel = Word2Vec(sentences=texts_processed, vector_size=10, window=5, min_count=1, workers=4)\n\n# Get the word vectors for all terms in the vocabulary\nword_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n\n# Create a Data Frame from the word vectors \ndf_word_vectors = pd.DataFrame(word_vectors).T\n# Print the word vectors\ndf_word_vectors\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\naccounting\n-0.005362\n0.002364\n0.051033\n0.090093\n-0.093029\n-0.071168\n0.064589\n0.089730\n-0.050154\n-0.037634\n\n\ndegree\n0.073805\n-0.015335\n-0.045366\n0.065541\n-0.048602\n-0.018160\n0.028766\n0.009919\n-0.082852\n-0.094488\n\n\ngraduate\n0.073118\n0.050703\n0.067577\n0.007629\n0.063509\n-0.034054\n-0.009464\n0.057686\n-0.075216\n-0.039361\n\n\nbusiness\n-0.075116\n-0.009300\n0.095381\n-0.073192\n-0.023338\n-0.019377\n0.080774\n-0.059309\n0.000452\n-0.047537\n\n\nlanguage\n-0.096036\n0.050073\n-0.087596\n-0.043918\n-0.000351\n-0.002962\n-0.076612\n0.096147\n0.049821\n0.092331\n\n\ncalled\n-0.081579\n0.044958\n-0.041371\n0.008245\n0.084986\n-0.044622\n0.045175\n-0.067870\n-0.035485\n0.093985\n\n\nlove\n-0.015777\n0.003214\n-0.041406\n-0.076827\n-0.015080\n0.024698\n-0.008880\n0.055337\n-0.027430\n0.022601\n\n\n\n\n\n\n\n\n\n\n10.5.4 Doc2Vec\n    Doc2Vec is an extension of Word2Vec that generates vector representations for entire documents, capturing the context of the document.\n\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\n\ntexts_processed = [['love', 'accounting'],\n ['accounting', 'called', 'language', 'business'],\n ['graduate', 'accounting', 'degree']]\n# Create tagged documents\ntagged_data = [TaggedDocument(words=text, tags=[str(i)]) for i, text in enumerate(texts)]\n# Create Doc2Vec Model \nmodel = Doc2Vec(tagged_data, vector_size=10, window=5, min_count=1, workers=4)\n# Get the document vectors for all documents\ndoc_vectors = {str(i): model.dv[str(i)] for i in range(len(texts_processed))} \n# Create a DataFrame for the document vectors \ndf_doc_vectors = pd.DataFrame(doc_vectors).T \n# Print the DataFrame \ndf_doc_vectors\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n-0.052996\n-0.059291\n-0.099730\n0.085218\n0.036740\n0.001727\n-0.098661\n-0.050780\n-0.099170\n0.019854\n\n\n1\n0.026782\n0.047362\n-0.044838\n-0.031941\n-0.028554\n-0.089483\n0.022004\n0.094703\n-0.100429\n-0.035253\n\n\n2\n-0.039851\n0.027446\n-0.059339\n0.025528\n0.061253\n-0.084000\n-0.082807\n-0.096986\n0.042936\n-0.091991\n\n\n\n\n\n\n\n\n\n\n10.5.5 BERT (Bidirectional Encoder Representations from Transformers)\n    BERT is a transformer-based model that generates contextualized word embeddings. It captures the context of words in a sentence, making it powerful for various NLP tasks.\n\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n\ntext = \"I love programming\"\ninputs = tokenizer(text, return_tensors='pt')\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\nprint(last_hidden_states)",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#topic-modeling",
    "href": "nlp.html#topic-modeling",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.6 Topic Modeling",
    "text": "10.6 Topic Modeling\n\n    Topic Modeling is a technique in NLP that tries to identify or extract semantic patterns or topic from a collection of documents. In other words, topic modeling helps to idenfity the cluster of words that appear together often and ultimately, they can be grouped into topics. For example, imagine an auditor is reveiwing important contracts of the client to test some management assertions, but before starting collecting evident to test the assertion, the auditor needs to understand the main themes of those contracts. Topic modeling can be used to figure out the themes (topics) based on the words used in the contracts. Figure 10.3 shows the general process of how topic modeling works.\n\n\n\n\n\n\nFigure 10.3: Topic Modeling\n\n\n\n    There are two commonly used methods for topic modeling. They include - Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). LSA assumes that words with similar meaning will appear in similar documents. LSA uses mathmatical techniques to reduce the dimensionality of the data, but LDA is a proabilistic technique, which assumes documents contain topics and topics contain words.\n    In LSA, a term-document matrix is created in which rows represent terms and columns represent documents. The values (cells) of the matrix indicates the frequency of each term in each document. Then Singular Value Decomposition (SVD) is applied on the term-document matrix. The SVD technique converts the term-document matrix into three matrices - U (term-topic matrx), \\(\\sum\\) (diagonal matrix of singualr values), and V (document-topic matrix). This breakdown helps to reduce the dimensionality of the data while retaining the most improtant relationships between the data. Then, we select the top \\(k\\) singualr values and their associated vectors from \\(U\\) and \\(V\\) matrices to form a reduced dimensional representation of the data.\n    In LDA, on the other hand, it is assumed each document is a mixture of topics and that each topic is a mixture of words. Therefore, in LDA documents are mapped to a list of topics by assigning words in the document to different topics. LDA uses Bayesian inferences to find the underlying topics in a corpus of documents. Of the two methods, it is recommended to use LDA because of its probabilistic nature, interpretability and scalability.\n\nimport pandas as pd\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim, spacy, logging, warnings\nfrom gensim import corpora, models, matutils, utils\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nimport pyLDAvis \nimport pyLDAvis.gensim_models as gensimvis\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(\n    ['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come']\n    )\n\n# reading the dataset\ndf= pd.read_csv('DATA/tripadvisor_hotel_reviews.csv')\ndf_500= df[0:500]\ndef sent_to_words(sentences):\n    for sent in sentences:\n        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n        sent = utils.simple_preprocess(str(sent), deacc=True) \n        yield(sent)  \n\n# Convert to list\ndata = df_500.Review.values.tolist()\ndata_words = list(sent_to_words(data))\n\n    In topic modeling, sometimes we use bigrams or trigrams. Specifically, bigrams and trigrams arte sequences of two or three words respectively that appear consecutively in texts. They are also called n-grams, where \\(n\\) represents the number of words in the sequence. In topic modeling, bigrams or trigrams help to capture the context and meaning of texts better than individual words. For example, by considering the triplets such as “machine learning algorithm”, we can identify patterns, relationships, and meanings better. Therefore, bigrams or trigrams enhances the quality and coherence of the topics generated by LDA.\n\n# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) \n# higher threshold fewer phrases\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# Before running the process_words function, please run the following line of code in the terminal - \n# python -m spacy download en_core_web_sm\ndef process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n    texts = [bigram_mod[doc] for doc in texts]\n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = spacy.load(\"en_core_web_sm\")\n    #nlp = spacy.load('en', disable=['parser', 'ner'])\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n    return texts_out\n\ndata_ready = process_words(data_words)  # processed Text Data!\n\n\n# turn our tokenized documents into a term  &lt;-&gt; id dictionary)\ndictionary = corpora.Dictionary(data_ready)\n# convert tokenized documents into a document-term matrix)\ncorpus = [dictionary.doc2bow(text) for text in data_ready]\n# Convert the corpus to a document-term matrix \ndoc_term_matrix = matutils.corpus2csc(corpus).transpose()\n# Convert the document-term matrix to a DataFrame \ndf_doc_term_matrix = pd.DataFrame(doc_term_matrix.toarray(), \ncolumns=[dictionary[i] for i in range(len(dictionary))])\n# Print the DataFrame \ndf_doc_term_matrix\n\n\n\n\n\n\n\n\n\nadvantage\nadvice\nanniversary\narrive\naveda\nbang\nbed\ncheck\nclosing\ncomfortable\n...\ncushy\ngouge\nhumor\nlast\nnavigating\noxygen\nreplacement\nextraordinarily\nranier\nseperate\n\n\n\n\n0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n2.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n2.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n496\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n497\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n498\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n499\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n\n\n\n\n500 rows × 4230 columns\n\n\n\n\n\n# Create a TF-IDF model from the corpus \ntfidf_model = models.TfidfModel(corpus)\n# Transform the corpus using the TF-IDF model \ncorpus_tfidf = tfidf_model[corpus]\n# Convert the TF-IDF weighted corpus to a document-term matrix \ndoc_term_matrix_tfidf = matutils.corpus2csc(corpus_tfidf).transpose()\n# Convert the document-term matrix to a DataFrame \ndf_doc_term_matrix_tfidf = pd.DataFrame(doc_term_matrix_tfidf.toarray(), \ncolumns=[dictionary[i] for i in range(len(dictionary))])\n# Print the DataFrame \ndf_doc_term_matrix_tfidf\n\n\n\n\n\n\n\n\n\nadvantage\nadvice\nanniversary\narrive\naveda\nbang\nbed\ncheck\nclosing\ncomfortable\n...\ncushy\ngouge\nhumor\nlast\nnavigating\noxygen\nreplacement\nextraordinarily\nranier\nseperate\n\n\n\n\n0\n0.160135\n0.173497\n0.179099\n0.085600\n0.23166\n0.23166\n0.038655\n0.060582\n0.23166\n0.056518\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n1\n0.000000\n0.000000\n0.140405\n0.033553\n0.00000\n0.00000\n0.015152\n0.000000\n0.00000\n0.022154\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n2\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.040298\n0.063157\n0.00000\n0.029460\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n3\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.000000\n0.000000\n0.00000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n4\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.000000\n0.026962\n0.00000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.000000\n0.000000\n0.00000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n496\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.065851\n0.000000\n0.00000\n0.048142\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n497\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.029146\n0.000000\n0.00000\n0.042615\n...\n0.196603\n0.196603\n0.196603\n0.196603\n0.196603\n0.196603\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n498\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.075078\n0.000000\n0.00000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.253216\n0.000000\n0.000000\n0.000000\n\n\n499\n0.000000\n0.000000\n0.000000\n0.000000\n0.00000\n0.00000\n0.000000\n0.000000\n0.00000\n0.067615\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.311936\n0.311936\n0.311936\n\n\n\n\n500 rows × 4230 columns\n\n\n\n\n\n# Create an LDA model from the corpus\nlda_model = models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n# Print the topics\nfor idx, topic in lda_model.print_topics(-1):\n    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n\nTopic: 0 \nWords: 0.031*\"room\" + 0.028*\"hotel\" + 0.019*\"stay\" + 0.019*\"great\" + 0.013*\"location\" + 0.013*\"staff\" + 0.011*\"place\" + 0.010*\"night\" + 0.007*\"walk\" + 0.007*\"price\"\n\nTopic: 1 \nWords: 0.039*\"hotel\" + 0.032*\"room\" + 0.020*\"stay\" + 0.009*\"night\" + 0.008*\"bed\" + 0.008*\"great\" + 0.008*\"service\" + 0.007*\"place\" + 0.007*\"location\" + 0.007*\"staff\"\n\nTopic: 2 \nWords: 0.036*\"room\" + 0.025*\"hotel\" + 0.017*\"stay\" + 0.011*\"great\" + 0.010*\"night\" + 0.009*\"bed\" + 0.008*\"desk\" + 0.008*\"staff\" + 0.006*\"time\" + 0.006*\"service\"\n\n\n\n\n# Topic Membership Likelihood \n# Create a DataFrame with topic vectors for each document\ntopic_vectors = []\nfor doc in corpus:\n    topic_vector = lda_model.get_document_topics(doc, minimum_probability=0)\n    topic_vectors.append([prob for _, prob in topic_vector])\ndf_topic_vectors = pd.DataFrame(topic_vectors)\n# Print the DataFrame of the probabilities\ndf_topic_vectors\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.637979\n0.006157\n0.355863\n\n\n1\n0.001946\n0.002036\n0.996018\n\n\n2\n0.002676\n0.002600\n0.994724\n\n\n3\n0.005811\n0.986824\n0.007365\n\n\n4\n0.002571\n0.002569\n0.994860\n\n\n...\n...\n...\n...\n\n\n495\n0.978756\n0.010359\n0.010885\n\n\n496\n0.253207\n0.006250\n0.740543\n\n\n497\n0.391466\n0.004891\n0.603643\n\n\n498\n0.983575\n0.007941\n0.008484\n\n\n499\n0.014031\n0.013798\n0.972171\n\n\n\n\n500 rows × 3 columns\n\n\n\n\n\n# Create the visualization \nvis_data = gensimvis.prepare(lda_model, corpus, dictionary) \npyLDAvis.display(vis_data)\n\n\n\n\n\n\n\n\n\n\n# Dominat topics and its percentage contribution in each document\n\ndef format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # =&gt; dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                # Create a new row as a DataFrame and concatenate it\n                new_row = pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords])\n                sent_topics_df = pd.concat([sent_topics_df, new_row.to_frame().T], ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(5)\n\n\n\n\n\n\n\n\n\nDocument_No\nDominant_Topic\nTopic_Perc_Contrib\nKeywords\nText\n\n\n\n\n0\n0\n0\n0.638\nroom, hotel, stay, great, location, staff, pla...\n[hotel, expensive, parking, deal, stay, hotel,...\n\n\n1\n1\n2\n0.996\nroom, hotel, stay, great, night, bed, desk, st...\n[special, charge, diamond, member, decide, cha...\n\n\n2\n2\n2\n0.9947\nroom, hotel, stay, great, night, bed, desk, st...\n[room, experience, hotel, level, positive, lar...\n\n\n3\n3\n1\n0.9866\nhotel, room, stay, night, bed, great, service,...\n[unique, great, stay, wonderful, time, hotel, ...\n\n\n4\n4\n2\n0.9949\nroom, hotel, stay, great, night, bed, desk, st...\n[great, stay, great, stay, game, awesome, down...\n\n\n\n\n\n\n\n\n    There are two goodness of fit (gof) measures to evaluate topic modeling. They are - coherence score and perplexity. Coherence score measures the coherence between topics. Higher coherence score indicates more meaningful and interpretable topics. Coherent scores range from 0 to 1. The higher coherence scores indicate more interpretable and meaningful topics.\n    Perplexity score evaluates how well the model represents the data. Lower perplexity score indicates a better fit. Perplexity might not always correlate with human interpretability of the topics. Unlike, coherence socres, there is no good or bad perplexity score, but it is useful to compare different models on the same dataset.\n\n# Compute Coherence Score\nfrom gensim.models import CoherenceModel\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready,dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n\n\n\n\nCoherence Score:  0.3579669980731333\n\n\n\n# Compute Perplexity\nprint('\\nPerplexity : ', lda_model.log_perplexity(corpus)) \nlda_model.log_perplexity(corpus)\n\n\nPerplexity :  -7.121542581413699\n\n\n-7.121542407290765\n\n\n    To optimize topic modeling performance, we can tune some hyperparameters (also called hyperparameter tuning). Some of the important hyperparameters of topic modeling include - number of topics, document topic density, topic word density, number of iterations and so on.",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#sentiment-analysis",
    "href": "nlp.html#sentiment-analysis",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.7 Sentiment Analysis",
    "text": "10.7 Sentiment Analysis\n\n    Sentiment analysis involves converting text into sentiments such as positive, neutral, and negative. Texts are widely used to express emotion, feelings, opinion and so on. Therefore, sometimes sentiment analysis is also called “Opinion Mining.” Identifying sentiment from texts could provide valuable insights to make strategic decisions such as improving product features, launching new products, identifying strengths or weaknesses of product or service offerings. Before, we perform the sentiment analysis, we need to do the preprocessing as described in Section 10.3.1 first.\n    Below we use texblob python module for seniment analysis of our text about Accounting. texblob is simple for sentiment analysis because the function accepts text as input and return sentiment score. There are two types of sentiment scores - polarity and subjectivity. Polarity score actually measures the sentiment of the text and it values are between -1 and +1, where -1 indicates high negative sentiment and +1 indicates very positive sentiment. On the other hand, subjectivity score measures whether the text contaiins factual information or personal opinion. Subjectivity scores range from 0 to 1, where 0 indicates factual information and 1 indicates personal opinion.\n\nfrom textblob import TextBlob\n\n\n# Determining Polarity \nTextBlob(my_text).sentiment.polarity\n\n0.028571428571428574\n\n\n\n# Determining Subjectivity \nTextBlob(my_text).sentiment.subjectivity\n\n0.21706349206349207\n\n\n    In the above analysis, we see the polarity score is 0.02857, which is very close to zero. Therefore, we can say our text is neutral. On the other hand, subjectivity score is 0.21706, which is close to 0, indicating that our text is factual information (not personal opinion).",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#readability-index",
    "href": "nlp.html#readability-index",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.8 Readability Index",
    "text": "10.8 Readability Index",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#text-similarity",
    "href": "nlp.html#text-similarity",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.9 Text Similarity",
    "text": "10.9 Text Similarity",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#conclusion",
    "href": "nlp.html#conclusion",
    "title": "10  Natural Language Processing (NLP)",
    "section": "10.10 Conclusion",
    "text": "10.10 Conclusion",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  },
  {
    "objectID": "nlp.html#footnotes",
    "href": "nlp.html#footnotes",
    "title": "10  Natural Language Processing (NLP)",
    "section": "",
    "text": "The collection of documents is called corpus.↩︎\nYou might find some discrepencies between Equation 10.1 tf-idf scores and tf-idf scores generated by different packages because the packages uses a slightly different formula that might include some kinds of smoothing.↩︎",
    "crumbs": [
      "Advanced Techniques and Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing (NLP)</span>"
    ]
  }
]