{"title":"Exploratory Data Analysis (EDA)","markdown":{"yaml":{"title":"Exploratory Data Analysis (EDA)","format":"html"},"headingText":"Learning Objectives of the Chapter","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n::: {style=\"text-align: justify\"}\nAt the End of the Chapter, Students should be Able to -\n\n-   Learn about the purpose of Exploratory Data Analysis (EDA)\n\n-   Understand different techniques of transforming and cleaning data\n\n-   Learn about Different R and Python Packages for EDA\n\n-   Understand how to use six verbs for EDA\n\n-   Perform EDA on some real world data sets\n\n-   Learn about how to interpret results from EDA\n:::\n\n## Introduction\n\n::: {style=\"text-align: justify\"}\n     Exploratory Data Analysis (EDA) is the initial process of examining a dataset to understand its main characteristics before applying formal statistical models or machine learning algorithms. It typically combines summary statistics with visual tools such as histograms, boxplots, and scatterplots to uncover patterns, spot anomalies, test assumptions, and check data quality. \n\n     By revealing relationships among variables, distributions, and potential outliers, EDA helps refine research questions, choose appropriate modeling techniques, and prevent misleading conclusions. In this way, EDA acts as a bridge between raw data and rigorous analysis, ensuring that subsequent steps are grounded in an accurate and transparent understanding of the underlying information.\n:::\n\n## Data Collection & Importing\n\n## Data Cleaning\n\n## Packages for Exploratory Data Analysis (EDA)\n\n::: {style=\"text-align: justify\"}\n     In order to use `pyjanitor`, the data frame must be pandas because `pyjanitor` extends pandas data frame functionality.\n:::\n\n::: panel-tabset\n## dplyr\n\n```{r}\n#| warning: false\n# loading packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n```\n\n```{r}\n#| include: false\nlibrary(reticulate)\nSys.unsetenv(\"RETICULATE_PYTHON\")\nreticulate::use_virtualenv(\"C:/Users/mshar/OneDrive - Southern Illinois University/ANALYTICS_FOR_ACCOUNTING_DATA/accounting_analytics_book\", required = TRUE)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n#py_install(\"pyjanitor\")\n#py_install(\"polars\")\n#Sys.setenv('RETICULATE_PYTHON' = '~/.venv/quarto_book_python/Scripts/python.exe')\n```\n\n## pandas\n\n```{python}\n# loading the package\nimport numpy as np\nimport pandas as pd\n# from pyjanitor package \n# pip install pyjanitor\nimport janitor \nfrom janitor import clean_names, remove_empty\n```\n\n\n\n## polars \n\n```{python}\n# loading the package \nimport polars as pl\n```\n\n\n\n\n:::\n\n## Importing the Dataset\n\n::: panel-tabset\n## dplyr\n\n```{r}\n#| warning: false\n# importing data frame \ndf = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n## pandas\n\n```{python}\n# importing data frame \ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n## polars \n\n```{python}\n# importing data frame \ndf_pl = pl.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n\n:::\n\n## Meta Data\n\n::: {style=\"text-align: justify\"}\n     Meta data is data about the data. Before we put the data into analysis, we need to learn about our dataset. This learning invovles knowing about the number of rows, number of columns, the types of the fields, the appropriateness of those types, the missing values in the dataset and so on.\n:::\n\n::: panel-tabset\n## dplyr\n\n```{r}\nglimpse(df)\n```\n\n```{r}\nmap_df(df, ~sum(is.na(.))) |>\n     glimpse()\n```\n\n```{r}\nncol(df)\nnrow(df)\n```\n\n```{r}\nhead(df)\n```\n\n```{r}\ntail(df)\n```\n\n```{r}\ndplyr::sample_n(df, 10)\n```\n\n## Pandas\n\n```{python}\ndf_pd.info()\n```\n\n```{python}\ndf_pd.shape\n```\n\n```{python}\nprint('The total number of rows and columns of the product data is \\\n {} and {} respectively.'.format(df_pd.shape[0], df_pd.shape[1]))\n```\n\n```{python}\nprint(f'The total number of rows and columns of the product data is \\\n {df_pd.shape[0]} and {df_pd.shape[1]} respectively.')\n```\n\n```{python}\ndf_pd.columns\n```\n\n```{python}\ndf_pd.head()\n```\n\n```{python}\ndf_pd.tail()\n```\n\n```{python}\ndf_pd.isna().sum()\n```\n\n```{python}\ndf_pd.dtypes\n```\n\n```{python}\ndf_pd.sample(n=10)\n```\n\n## polars \n\n```{python}\n# metadata\ndf_pl.shape # rows and columns\ndf_pl.height # rows\ndf_pl.width # columns \ndf_pl.columns\nlen(df_pl.columns)\ndf_pl.schema # column names and their types \n\ndf_pl.dtypes\ndf_pl.head()\ndf_pl.tail()\ndf_pl.sample(n=10)\ndf_pl.describe()\n```\n\n:::\n\n## Cleaning the Dataset\n\n::: panel-tabset\n## dplyr\n\n```{r}\n df |>\n     rename_all(toupper) |>\n     janitor::clean_names() |>\n     rename_all(toupper) |>\n     glimpse()\n```\n\n```{r}\ndf = df |>\n     rename_all(toupper) |>\n     janitor::clean_names() |>\n     rename_all(toupper)\nglimpse(df)\n```\n\n## panads\n\n```{python}\ndf_pd.columns.str.upper().to_list()\n```\n\n```{python}\n(df_pd\n     .pipe(remove_empty)\n     .pipe(lambda x: x.clean_names(case_type = \"upper\"))\n     .pipe(lambda x: x.rename(columns = {'SIZE_US_': 'SIZE_US', 'SIZE_EUROPE_':\"SIZE_EUROPE\", \"SIZE_UK_\":\"SIZE_UK\"}))\n     .pipe(lambda x: x.info())\n     )\n```\n\n```{python}\n# Changing the names of the columns to uppercase\ndf_pd.rename(columns = str.upper, inplace = True)\ndf_pd.columns\n```\n\n```{python}\n#| warning: false\nnew_column = df_pd.columns \\\n .str.replace(\"(\", '').str.replace(\")\", \"\") \\\n .str.replace(' ','_') # Cleaning the names of the variables\nnew_column\n```\n\n```{python}\ndf_pd.columns = new_column\ndf_pd.columns\ndf_pd.rename(columns=str.upper, inplace = True)\ndf_pd.columns \n\n```\n\n## polars \n\n```{python}\n# upper case column name\ndf_pl.rename({col: col.upper() for col in df_pl.columns})\ndf_pl.rename({col: col.strip().upper().replace(' ', '_') for col in df_pl.columns})\n```\n\n\n:::\n\n### Changing the Types of Variables\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    glimpse()\n```\n\n     From the above, it is now evident the the type of the `DATE` variable now is `date`.\n\n```{r}\ndf |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    mutate (PRODUCTID = as.character(PRODUCTID)) |>\n    glimpse()\n```\n\n     From the above, it is now evident the the type of the `DATE` and `PRODUCTID` variable now is date (`date`) and character (`chr`) respectively. We can now incorparte the changes into the data frame.\n\n```{r}\ndf = df |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    mutate (PRODUCTID = as.character(PRODUCTID)) \nglimpse(df)\n```\n\n## pandas\n\n```{python}\n(\n    df_pd\n    .pipe(lambda x: x.assign(DATE = pd.to_datetime(x['DATE'])))\n    .pipe(lambda x: x.info())\n)\n\n```\n\n```{python}\n# converting integer to object\ndf_pd.INVOICENO = df_pd.INVOICENO.astype(str)\ndf_pd[['MONTH', 'PRODUCTID']] = df_pd[['MONTH', 'PRODUCTID']].astype(str)\ndf_pd.info()\n```\n\n## polars \n\n\n```{python}\n# Change the type of columns \ndf_pl = df_pl.with_columns(\n    pl.col(\"Date\").str.strptime(pl.Date, \"%m/%d/%Y\")\n)\ndf_pl.head()\ndf_pl.schema\n```\n\n```{python}\n# change to character variable\ndf_pl = df_pl.with_columns(pl.col(\"InvoiceNo\").cast(pl.Utf8))\ndf_pl.schema\ndf_pl = df_pl.with_columns(pl.col([\"ProductID\", \"Month\"]).cast(pl.Utf8))\ndf_pl.schema\n```\n\n\n\n\n:::\n\n## Some Other Useful Functions\n\n     There are some other useful functions that can be used to explore the dataset for analysis. Some of those useful functions are discussed below.\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf|> count(YEAR)\n```\n\n```{r}\ndf|> count(COUNTRY)\n```\n\n```{r}\ndf|> distinct(COUNTRY)\n```\n\n## pandas\n\n```{python}\ndf_pd['YEAR'].value_counts()\n```\n\n```{python}\ndf_pd['YEAR'].unique()\n```\n\n\n## polars \n\n```{python}\ndf_pl.select(pl.col(\"Year\").value_counts())\ndf_pl.select(pl.col(\"Year\").value_counts()).unnest(\"Year\")\n\ndf_pl.select(pl.col(\"Gender\").value_counts())\ndf_pl.select(pl.col(\"Gender\").value_counts()).unnest(\"Gender\")\n\ndf_pl.group_by(\"Gender\").len()\ndf_pl.group_by(\"Gender\").len().rename({\"len\":\"Total\"})\n```\n\n\n:::\n\n## Six Verbs for EDA\n\n     @tbl-compareDplyrPandas shows the comparable functions in both `dplyr` and `pandas` packages. These functions are very much important to perform exploratory data analysis in both `R` and `Python`. `group_by` (`groupby` in pandas) and `summarize ()`[^eda-1] (`agg ()` in pandas) are often used together; therefore, they are in the same group in @tbl-compareDplyrPandas.\n\n[^eda-1]: You can also use British spelling - `summarise ()`\n\n```{r}\n#| include: false\ntidyverse_pandas = tibble::tribble(\n  ~`Verb Number`,~`tidyverse`, ~ `pandas`, ~ `polars` ,\n  '1','filter ()', 'query () or loc () or iloc ()', 'filter ()',\n  '2','arrange ()', 'sort_values ()', 'sort ()',\n  '3','select ()', 'filter () or loc ()', 'select ()', \n  '4','rename ()', 'rename ()', 'rename ()', \n  '5','mutate ()', 'assign ()', 'with_columns ()', \n  '6','group_by ()', 'groupby ()', 'group_by ()', \n  '6','summarize ()', 'agg ()', 'agg ()'\n)\n\n```\n\n```{r}\n#| label: tbl-compareDplyrPandas\n#| tbl-cap: Tidyverse and Pandas Equivalent Functions \n#| echo: false\n#| warning: false\n# These are R code to prepare Table 2 using KableExtra \nlibrary(kableExtra)\nkbl(tidyverse_pandas, booktabs = TRUE \n    #,caption = \"Tidyverse and Pandas Equivalent Functions\"\n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n### 1st Verb - filter () Function\n\n     Filter functions are used to subset a data frame based on rows, meaning that retaining rows that satisfy given conditions. Filtering rows is also called slicing[^eda-2] becasue we obtain a set of elements by filtering.\n\n[^eda-2]: Indexing involves obtaining individual elements.\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |> filter (YEAR == \"2015\")\n```\n\n```{r}\ndf |> filter (COUNTRY %in% c(\"United States\", \"Canada\"))\n```\n\n```{r}\ndf |> filter (COUNTRY == \"United States\", YEAR == \"2016\")\n```\n\n```{r}\ndf |> filter (COUNTRY == \"United States\", YEAR %in% c(\"2015\",\"2016\"))\n```\n\n```{r}\ndf |> filter (COUNTRY %in% c(\"United States\", \"Canada\"), YEAR == \"2014\")\n```\n\n## pandas\n\n```{python}\ndf_pd.query(\"YEAR == 2015\")\n```\n\n```{python}\ndf_pd.query('COUNTRY== \"United States\" | COUNTRY == \"Canada\"')\n```\n\n```{python}\ndf_pd.query(\"COUNTRY in ['United States', 'Canada']\")\n```\n\n```{python}\ndf_pd.query(\"COUNTRY== 'United States' & YEAR== 2016\")\n```\n\n```{python}\ndf_pd.query(\"COUNTRY== 'United States' & YEAR in [2015,2016]\")\n```\n\n```{python}\ndf_pd[df_pd['COUNTRY'] == \"United States\"]\n```\n\n```{python}\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\")]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY'].isin([\"United States\", \"Canada\"])]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY']\\\n .isin([\"United States\", \"Canada\"]) &(df_pd['YEAR']==2014)]\n```\n\n```{python}\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\") &(df_pd [\"YEAR\"] ==2014)]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY'] == \"United States\", :]\n```\n\n```{python}\ndf_pd.loc[\n    df_pd['COUNTRY']=='United States',\n    ['COUNTRY', \"UNITPRICE\", \"SALEPRICE\"]]\n```\n\n## polars \n\n```{python}\ndf_pl.filter(pl.col(\"Year\") == 2015)\n```\n\n```{python}\ndf_pl.filter(pl.col(\"Country\").is_in([\"United States\", \"Canada\"]))\n```\n\n```{python}\ndf_pl.filter(\n    (pl.col(\"Country\") == \"United States\") & (pl.col(\"Year\") == 2015)\n    )\n```\n\n```{python}\ndf_pl.filter(\n    (pl.col(\"Country\") == \"United States\") & (pl.col(\"Year\").is_in([2015, 2016]) )\n    )\n```\n\n```{python}\ndf_pl.filter(\n    (pl.col(\"Country\").is_in([\"United States\", \"Canada\"])) & (pl.col(\"Year\")== 2014)\n)\n```\n\n\n\n:::\n\n### 2nd Verb - arrange () Function\n\n     In arrange functions, we order the rows of a data frame by the values of given columns. It is like sorting or odering the data.\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    arrange(DATE)     \n```\n\n```{r}\ndf |>\n    arrange(desc(DATE))     \n```\n\n```{r}\ndf |>\n    arrange(MONTH, SALEPRICE)     \n```\n\n## pandas\n\n```{python}\ndf_pd.sort_values(by =['DATE'])   \n```\n\n```{python}\ndf_pd.sort_values(by =['DATE'], ascending = False)   \n```\n\n```{python}\ndf_pd.sort_values(by =['MONTH', 'SALEPRICE'])\n```\n\n## polars \n\n```{python}\ndf_pl.sort(\"Date\") # default asecending \n```\n\n```{python}\ndf_pl.sort(['Month', 'SalePrice'])\ndf_pl.sort(['Month', 'SalePrice'], descending = [True, False])\n```\n\n\n:::\n\n### 3rd Verb - select () Function\n\n     Select functions help to select or obtain columns from the data frame. When there are a lot of columns in our dataset, select functions become very useful.\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |> select(DATE, UNITPRICE, DISCOUNT)   \n```\n\n```{r}\ndf |> select(1:2, 5:8)  \n```\n\n```{r}\ndf |>\n    select(starts_with('SIZE'))\n```\n\n```{r}\ndf |>\n    select(ends_with('PRICE'))\n```\n\n```{r}\ndf |>\n    select(contains(\"_\"))\n```\n\n```{r}\ndf |>\n    select(matches(\"SIZE\"))\n```\n\n```{r}\ndf |>\n    select(matches(\"PRICE$\"))\n```\n\n```{r}\n# starts with letter S\ndf |>\n    select(matches(\"^S\"))\n```\n\n```{r}\ndf |>\n    select(where(is.character))\n```\n\n```{r}\ndf |>\n    select(where(is.numeric))\n```\n\n```{r}\ndf |>\n    select(MONTH, YEAR, everything())\n```\n\n```{r}\n# any_of () vs all_of ()\ndf |>\n    select(any_of(c(\"PRICE\", \"SIZE\")))\n```\n\n```{r}\n# Dropping columns \ndf |>\n    select(-DATE)\n```\n\n## pandas\n\n```{python}\ndf_pd['DATE']   \n```\n\n```{python}\ndf_pd[['DATE', 'UNITPRICE']]   \n```\n\n```{python}\ndf_pd.loc[:,['DATE', 'UNITPRICE']]   \n```\n\n```{python}\ndf_pd.iloc[:,5:8]\n```\n\n```{python}\ndf_pd.iloc[:,[3,5,8]]\n```\n\n```{python}\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n```\n\n```{python}\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n```\n\n```{python}\n #RegularExpression(Regex)\ndf_pd.filter(regex =\"PRICE$\") #Ends with Price\n```\n\n```{python}\ndf_pd.filter(regex =\"ˆSIZE\") #Starts with SIZE\n```\n\n```{python}\ndf_pd.filter(regex =\"PRICE\") #Contains the word Price\n```\n\n```{python}\ndf_pd.select_dtypes('object')\n```\n\n```{python}\ndf_pd.select_dtypes('int')\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.startswith('SIZE')]\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.contains('PRICE')]\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.endswith('PRICE')]\n```\n\n```{python}\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\n```\n\n```{python}\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\\\n    .pipe(lambda x: x.info())\n```\n\n```{python}\n# Rearranging columns \n# Sorting Alphabetically\ndf_pd.reindex(sorted(df_pd.columns), axis =1)\n```\n\n```{python}\n# Rearranging columns \n# Sorting As You Want (ASY)\ncol_first = ['YEAR','MONTH']\ncol_rest = df_pd.columns.difference(col_first, sort=False).to_list()\ndf_pd2 = df_pd [col_first +col_rest]\ndf_pd2.info()\n```\n\n## polars \n```{python}\ndf_pl.select([\"Date\", 'UnitPrice', \"Discount\"])\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if c.startswith(\"Size\")]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if c.endswith(\"Price\")]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if \"_\" in c]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if \"Size\" in c]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if c.startswith(\"S\")]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ndf_pl.select(pl.col(pl.Utf8)) # all character columns\ndf_pl.select(pl.col(pl.Int64)) # all numeric columns \n```\n\n```{python}\ncols = [\"Month\", \"Year\"] + [c for c in df_pl.columns if c not in [\"Month\", \"Year\"]]\ndf_pl.select(cols)\n```\n\n\n\n:::\n\n### 4th Verb - rename () Function\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    rename(INVOICE = INVOICENO,\n    PRODUCT = PRODUCTID) |>\n    glimpse()\n     \n```\n\n## pandas\n\n```{python}\n(df_pd.rename(columns = {\"PRODUCTID\": \"PRODUCT\", \"INVOICENO\": \"INVOICE\"})\n     .pipe(lambda x: x.info())\n)\n```\n\n## polars \n\n\n```{python}\ndf_pl = df_pl.rename({\"ProductID\": \"PRODUCTID\", \"InvoiceNo\":\"INVOICE\"})\ndf_pl.schema\n```\n\n:::\n\n### 5th Verb - mutate () Function\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    mutate(NECOLUMN = 5,\n    SALESPRICE2 = UNITPRICE*(1-DISCOUNT)) |>\n    glimpse()\n     \n```\n\n## pandas\n\n```{python}\ndf_pd['NEWCOLUMN'] = 5\ndf_pd.info()     \n```\n\n```{python}\ndf_pd.drop(columns = ['NEWCOLUMN'], axis = 1, inplace = True)   \ndf_pd.info() \n```\n\n```{python}\ndf_pd['SALEPRICE2']=df_pd['UNITPRICE']*(1-df_pd['DISCOUNT'])\ndf_pd.info()\n```\n\n```{python}\n# Using the assign() function\n(df_pd[['PRODUCTID', 'UNITPRICE', 'DISCOUNT']]\\\n    .assign(SALEPRICE3 =lambda x: x.UNITPRICE*(1-x.DISCOUNT)) \\\n    .head(5)\n)\n```\n\n## polars \n\n```{python}\ndf_pl.with_columns(\n    NEWCOLUMN = 5,\n    SALEPRICE2 = pl.col('UnitPrice')*(1-pl.col('Discount'))\n)\n```\n\n\n\n:::\n\n### 6th Verbs - group_by () and summarize () Functions\n\n     @fig-splitapplycombine presents Split Apply Combine principle in `group_by ()` and `summarize ()` functions.\n\n![Split Apply Combine Principle](images/split-apply-combine.png){#fig-splitapplycombine}\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    group_by(COUNTRY) |>\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE))\n     \n```\n\n```{r}\ndf |>\n    group_by(COUNTRY) |>\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE),\n    AVGSALEPRICE = mean (SALEPRICE, na.rm = TRUE))\n     \n```\n\n```{r}\n# Summary Statistics \ndf %>%\n  select(UNITPRICE, SALEPRICE) %>%\n  summarize(across(where(is.numeric), \n                   .fns = list(N = ~length(.),\n                               Mean = mean, \n                               Std = sd,\n                               Median = median, \n                               P25 = ~quantile(.,0.25), \n                               P75 = ~quantile(., 0.75)\n                               )\n                   )) %>%\n  pivot_longer(everything(), names_sep='_', names_to=c('variable', '.value'))  \n```\n\n## pandas\n\n```{python}\ndf_pd.groupby(['COUNTRY'])['UNITPRICE'].mean()\n     \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY'])[['UNITPRICE', 'SALEPRICE']].mean()\n     \n```\n\n```{python}\n#| warning: false\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(np.mean) \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(\"mean\") \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"))\n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"),\n    TOTALN=(\"SALEPRICE\",\"size\"), # size function for n\n    TOTALOBS=(\"SALEPRICE\",\"count\") # count function for n\n )\n```\n\n```{python}\n# Defining a Function\ndef percentile(n):\n    def percentile_(x):\n        return x.quantile(n)\n    percentile_.__name__ ='percentile_{:02.0f}'.format(n*100)\n    return percentile_\n\n# Some summary statistics \ndf_pd[['UNITPRICE', 'SALEPRICE']] \\\n    .agg(['count', 'mean', 'std', 'median', percentile(0.25), percentile (0.75)]) \\\n    .transpose() \\\n    .reset_index() \\\n    .rename(columns = {\"index\": \"variables\", \"percentile_25\": \"P25\", \"percentile_75\": \"P75\", 'count': \"N\"}) \\\n    .round(3) \n```\n\n```{python}\n#| eval: false\n# Summary Statistics \nagg_dict = {\n    \"N\": \"count\",\n    'Mean':\"mean\",\n    \"Std. Dev\" : \"std\",\n    'P25': lambda x: x.quantile(0.25),\n    'Median': 'median',\n    'p75': lambda x: x.quantile(0.75)\n}\n\ndf_pd[['UNITPRICE', 'SALEPRICE']].agg(agg_dict)\n```\n\n## polars \n```{python}\ndf_pl.group_by(\"Country\").agg(pl.col(\"UnitPrice\").mean().alias(\"AvgPrice\"))\n```\n\n```{python}\ndf_pl.group_by(\"Country\").agg(\n    [pl.col(\"UnitPrice\").mean().alias(\"Avgprice\"),\n     pl.col(\"SalePrice\").mean().alias(\"AvgSalePrice\")]\n)\n```\n\n:::\n\n## Reshaping Data\n\n::: {style=\"text-align: justify\"}\n     Before we discuss about reshaping of the data, we need to discuss about tidy format of the data. Data can come in many shapes, but not all shapes are useful for data analysis. In most cases, tidy format of the data is most useful for analysis. Therefore, if the data is untidy, we need to make it tidy first. There are three interrelated rules which make a dataset tidy [@wickham2023r]. These rules are given below. @fig-tidyprinciple visually represents tidy principle.\n\n1.  Each variable must have its own column\n\n2.  Each observation must have its own row\n\n3.  Each value must have its own cell\n\n![Tidy Principle](images/tidy-1.png){#fig-tidyprinciple}\n\n     For analysis, many times we need to change the format of our dataset and we call it reshaping. Data come primarily in two shapes -*wide* and *long*. Sometimes *wide* format is called \"record\" format and *long* format is called \"stacked\" format. In `wide` format data, there is one row for each subject (units of observation). Data is `long` when there are multiple rows for each subject (units of observations).\n\n     This reshaping can be two types - a) long to wide and 2) wide to long. Long-to-wide means reshaping a long data, which has many rows, into wide format, which has many variables. In wide-to-long format, we do otherwise. For analytical purpose, reshaping data is useful; so, we need to know how to do the reshaping.\n\n     Whether a given dataset (e.g., @tbl-formatofdata) is in wide or long format depends on our research questions (on what variables we are interested in and how we conceive of our data). If we are interested in variable `Temp` and `Month` variable is the unit of obsevation, then the dataset in @tbl-formatofdata is in `long` format because `Month` is repeated in mutiple rows.\n:::\n\n```{r}\n#| warning: false\nairquality = airquality\n\nexamp = airquality |>\n    slice(1:10)\n```\n\n```{r}\n#| label: tbl-formatofdata\n#| tbl-cap: Which Format - Long or Wide \n#| echo: false\n#| warning: false\n\nkbl(examp, booktabs = TRUE \n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n### Long-to-Wide Format\n\n::: {style=\"text-align: justify\"}\n     To make a long dataset to wide, we can use `pivot_wider()` function from `tidyr` package in R and `pivot()` function from `pandas` in python.\n:::\n\n::: panel-tabset\n## tidyr\n\n```{r}\ntidyr::us_rent_income\n```\n\n```{r}\ntidyr::us_rent_income |>\n    pivot_wider(\n        names_from = variable, \n        values_from = c(estimate, moe)\n    )\n\n```\n\n## pandas\n\n```{python}\n#| warning: false\n# install palmerpenguins package\n# pip install palmerpenguins\nimport palmerpenguins\npenguins = palmerpenguins.load_penguins()\n```\n\n```{python}\npenguins[[\"island\", \"bill_length_mm\"]] \\\n    .pivot(columns = \"island\", values = \"bill_length_mm\") \\\n    .fillna(0)\n```\n\n## polars \n\n\n```{python}\nimport palmerpenguins\n\npenguins = palmerpenguins.load_penguins()\npenguins.head()\ndf_penguins = pl.DataFrame(penguins)\n```\n\n```{python}\n# Long to Wide (pivot)\ndf_penguins.with_columns(\n    pl.arange(0,df_penguins.height).alias(\"row_num\")\n).select(pl.col([\"row_num\", \"island\",\"bill_length_mm\"])\n).pivot(values = \"bill_length_mm\", on = \"island\", index = \"row_num\") \\\n    .fill_null (0)\n```\n\n\n:::\n\n### Wide-to-Long Format\n\n::: {style=\"text-align: justify\"}\n     To make a wide dataset to long, we can use `pivot_longer()` function from `tidyr` package in R and `melt()` function from `pandas` in python.\n:::\n\n::: panel-tabset\n## tidyr\n\n```{r}\nrelig_income\n```\n\n```{r}\nrelig_income |>\n    pivot_longer(\n        cols = !c(religion),\n        names_to = \"income\",\n        values_to = \"count\"\n    )\n```\n\n## pandas\n\n```{python}\npenguins.melt(value_vars=[\"bill_length_mm\", \"bill_depth_mm\",\"flipper_length_mm\", \"body_mass_g\"],\n              id_vars = ['species', 'island', 'sex', 'year']\n              )\n```\n\n## polars \n\n\n```{python}\n# Wide to Long (melt() but now polar calls it unpivot() )\ndf_penguins.with_columns(\n    pl.arange(0,df_penguins.height).alias(\"row_num\")\n).select(pl.col([\"row_num\", \"island\",\"bill_length_mm\"])\n).pivot(values = \"bill_length_mm\", on = \"island\", index = \"row_num\") \\\n    .unpivot(index = \"row_num\", value_name = \"bill_length_mm\")\n```\n\n\n\n\n:::\n\n## Merging Datasets\n\n::: {style=\"text-align: justify\"}\n     Many times, for analysis purposes, we need to join two datasets. This process is also called merging[^eda-3]. There are different types of joining. So, it is important to learn about those joining techniques. In @fig-joindatasets shows the joining technique and functions using `dplyr` in R. Below all of these joining functions are explained.\n\n-   `left_join()`: The merged dataset contains **all** observations fromthe **first** (or left) dataset and only **matched** observations from the **second** (or right) dataset\n\n-   `right_join()`: The merged dataset contains only **matched** observations from the **first** (or left) dataset and **all** observations from the **second** (or right) dataset\n\n-   `inner_join()`: The merged dataset contains *only* **matched** observations from both datasets\n\n-   `semi_join()`: The merged dataset contains **matched** observations from the **first** (or left) dataset. Please note that `semi_join()` differs from `inner_join()` in that `inner_join()` will return one row of first dataset (x) for each matching row of second dataset (y), whereas `semi_join()` will never duplicate rows of x.\n\n-   `full_join()`: The merged dataset contains **all** observations from both datasets\n\n-   `anti_join()`: The merged dataset contains only **not matched** observations from the **first** (or left) dataset and contains only the variable from the **left** dataset\n\n![Joining Datasets](images/join_diagram.png){#fig-joindatasets width=\"40%\"}\n\n```{r}\n#| include: false\ndf_join <- tribble(\n  ~dplyr, ~pandas, ~Description,\n  \"left_join()\", \"pd.merge(df1, df2, on='key', how='left')\", \"Join matching rows from df2 to df1, keeping all rows from df1.\",\n  \"right_join()\", \"pd.merge(df1, df2, on='key', how='right')\", \"Join matching rows from df1 to df2, keeping all rows from df2.\",\n  \"inner_join()\", \"pd.merge(df1, df2, on='key', how='inner')\", \"Join matching rows from both dataframes (default behavior of merge()).\",\n  \"full_join()\", \"pd.merge(df1, df2, on='key', how='outer')\", \"Join all rows from both dataframes, filling missing values with NaN.\",\n  \"semi_join()\", \"No direct equivalent, but can be achieved using filtering\", \"Keep rows in df1 where a match exists in df2.\",\n  \"anti_join()\", \"No direct equivalent, but can be achieved using filtering\", \"Keep rows in df1 where no match exists in df2.\"\n)\n```\n\n     @tbl-joinRPython compares the `dplyr` joining functions with equivalent joining functions from `pandas`.\n\n```{r}\n#| label: tbl-joinRPython\n#| tbl-cap: Joining Functions - dplyr vs pandas  \n#| echo: false\n#| warning: false\n\nkbl(df_join, booktabs = TRUE \n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n:::\n\n[^eda-3]: In database context, it is \"merging\", but commonly it is called \"joining\".\n\n::: panel-tabset\n## dplyr\n\n```{r}\n#| warning: false\ndata1 = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\nglimpse(data1)\n```\n\n```{r}\n#| warning: false\ndata2 = read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\")\nglimpse(data2)\n```\n\n```{r}\n# left_join\nleft_join(data1, data2, by = c(\"ticker\", \"year\"))\n```\n\n```{r}\n# left_join\nleft_join(data1 |> distinct(ticker, year, .keep_all = TRUE), \n          data2 |> distinct(ticker, year, .keep_all = TRUE), \n          by = c(\"ticker\", \"year\")\n          )\n```\n\n## pandas\n\n```{python}\n#| warning: false\ndataset1 = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\ndataset2 = pd.read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\",encoding=\"latin-1\")\n```\n\n```{python}\npd.merge(dataset1, dataset2, on=['ticker', 'year'], how='left')\n```\n\n```{python}\n#| warning: false\ndataset1_drop = dataset1.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\ndataset2_drop = dataset2.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\npd.merge(dataset1_drop, dataset2_drop, on=['ticker', 'year'], how='left')\n```\n\n```{python}\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})\ndf2 = pd.DataFrame({'id': [2, 3, 4], 'other_value': ['X', 'Y', 'Z']})\n\n# Left join\nleft_join = pd.merge(df1, df2, on='id', how='left')\n\n# Inner join\ninner_join = pd.merge(df1, df2, on='id')\n\n# Semi join\nsemi_join = df1[df1['id'].isin(df2['id'])]\n\n# Anti join\nanti_join = df1[~df1['id'].isin(df2['id'])]\n```\n\n## polars \n\n```{python}\ndataset1 = pl.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\ndataset2 = pl.read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\",encoding=\"latin-1\")\n\n```\n\n```{python}\ndataset1.join(dataset2, on = [\"ticker\", 'year'], how = \"left\")\n```\n\n```{python}\ndataset1_drop = dataset1.unique(subset=['ticker', 'year'])\ndataset2_drop = dataset2.unique(subset=['ticker', 'year'])\n```\n\n```{python}\ndataset1_drop.join(dataset2_drop, on = [\"ticker\", 'year'], how = \"left\")\n```\n\n```{python}\ndataset1_drop.join(dataset2_drop, on = [\"ticker\", 'year'], how = \"left\").shape\n```\n\n```{python}\ndf1 = pl.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})\ndf2 = pl.DataFrame({'id': [2, 3, 4], 'other_value': ['X', 'Y', 'Z']})\n```\n\n```{python}\n# Left join\nleft_join = df1.join(df2, on='id', how='left')\n```\n\n```{python}\n# Inner join\ninner_join = df1.join(df2, on='id')\n```\n\n```{python}\n# Semi join\nsemi_join = df1.join(df2, on=\"id\", how=\"semi\")\n```\n\n```{python}\n# Anti join\nanti_join = df1.join(df2, on=\"id\", how=\"anti\")\n```\n\n:::\n\n## Conclusions\n\n::: {style=\"text-align: justify\"}\n     Exploratory Data Analysis (EDA) is the crucial first phase of any data-focused project, turning raw data into meaningful insights that inform all later modeling and decision choices. By systematically applying descriptive statistics and visual techniques, EDA uncovers a dataset’s structure, trends, relationships, and irregularities, so that subsequent analyses rely on sound assumptions instead of guesswork. It enables analysts to spot errors and outliers, clarify what variables represent, and pinpoint the most important features, thereby strengthening model performance, interpretability, and the overall value of the conclusions drawn. \n\n     Rather than a single step, EDA is an iterative, reflective process that refines research questions, tests the suitability of methods, and ensures that technical work stays aligned with practical goals. Ultimately, EDA is essential: skipping it leaves statistical analysis and machine learning vulnerable to being elaborate but misguided, while using it turns them into dependable, transparent, and practically useful tools.\n\n:::","srcMarkdownNoYaml":"\n\n### Learning Objectives of the Chapter {.unnumbered}\n\n::: {style=\"text-align: justify\"}\nAt the End of the Chapter, Students should be Able to -\n\n-   Learn about the purpose of Exploratory Data Analysis (EDA)\n\n-   Understand different techniques of transforming and cleaning data\n\n-   Learn about Different R and Python Packages for EDA\n\n-   Understand how to use six verbs for EDA\n\n-   Perform EDA on some real world data sets\n\n-   Learn about how to interpret results from EDA\n:::\n\n## Introduction\n\n::: {style=\"text-align: justify\"}\n     Exploratory Data Analysis (EDA) is the initial process of examining a dataset to understand its main characteristics before applying formal statistical models or machine learning algorithms. It typically combines summary statistics with visual tools such as histograms, boxplots, and scatterplots to uncover patterns, spot anomalies, test assumptions, and check data quality. \n\n     By revealing relationships among variables, distributions, and potential outliers, EDA helps refine research questions, choose appropriate modeling techniques, and prevent misleading conclusions. In this way, EDA acts as a bridge between raw data and rigorous analysis, ensuring that subsequent steps are grounded in an accurate and transparent understanding of the underlying information.\n:::\n\n## Data Collection & Importing\n\n## Data Cleaning\n\n## Packages for Exploratory Data Analysis (EDA)\n\n::: {style=\"text-align: justify\"}\n     In order to use `pyjanitor`, the data frame must be pandas because `pyjanitor` extends pandas data frame functionality.\n:::\n\n::: panel-tabset\n## dplyr\n\n```{r}\n#| warning: false\n# loading packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n```\n\n```{r}\n#| include: false\nlibrary(reticulate)\nSys.unsetenv(\"RETICULATE_PYTHON\")\nreticulate::use_virtualenv(\"C:/Users/mshar/OneDrive - Southern Illinois University/ANALYTICS_FOR_ACCOUNTING_DATA/accounting_analytics_book\", required = TRUE)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n#py_install(\"pyjanitor\")\n#py_install(\"polars\")\n#Sys.setenv('RETICULATE_PYTHON' = '~/.venv/quarto_book_python/Scripts/python.exe')\n```\n\n## pandas\n\n```{python}\n# loading the package\nimport numpy as np\nimport pandas as pd\n# from pyjanitor package \n# pip install pyjanitor\nimport janitor \nfrom janitor import clean_names, remove_empty\n```\n\n\n\n## polars \n\n```{python}\n# loading the package \nimport polars as pl\n```\n\n\n\n\n:::\n\n## Importing the Dataset\n\n::: panel-tabset\n## dplyr\n\n```{r}\n#| warning: false\n# importing data frame \ndf = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n## pandas\n\n```{python}\n# importing data frame \ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n## polars \n\n```{python}\n# importing data frame \ndf_pl = pl.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n\n:::\n\n## Meta Data\n\n::: {style=\"text-align: justify\"}\n     Meta data is data about the data. Before we put the data into analysis, we need to learn about our dataset. This learning invovles knowing about the number of rows, number of columns, the types of the fields, the appropriateness of those types, the missing values in the dataset and so on.\n:::\n\n::: panel-tabset\n## dplyr\n\n```{r}\nglimpse(df)\n```\n\n```{r}\nmap_df(df, ~sum(is.na(.))) |>\n     glimpse()\n```\n\n```{r}\nncol(df)\nnrow(df)\n```\n\n```{r}\nhead(df)\n```\n\n```{r}\ntail(df)\n```\n\n```{r}\ndplyr::sample_n(df, 10)\n```\n\n## Pandas\n\n```{python}\ndf_pd.info()\n```\n\n```{python}\ndf_pd.shape\n```\n\n```{python}\nprint('The total number of rows and columns of the product data is \\\n {} and {} respectively.'.format(df_pd.shape[0], df_pd.shape[1]))\n```\n\n```{python}\nprint(f'The total number of rows and columns of the product data is \\\n {df_pd.shape[0]} and {df_pd.shape[1]} respectively.')\n```\n\n```{python}\ndf_pd.columns\n```\n\n```{python}\ndf_pd.head()\n```\n\n```{python}\ndf_pd.tail()\n```\n\n```{python}\ndf_pd.isna().sum()\n```\n\n```{python}\ndf_pd.dtypes\n```\n\n```{python}\ndf_pd.sample(n=10)\n```\n\n## polars \n\n```{python}\n# metadata\ndf_pl.shape # rows and columns\ndf_pl.height # rows\ndf_pl.width # columns \ndf_pl.columns\nlen(df_pl.columns)\ndf_pl.schema # column names and their types \n\ndf_pl.dtypes\ndf_pl.head()\ndf_pl.tail()\ndf_pl.sample(n=10)\ndf_pl.describe()\n```\n\n:::\n\n## Cleaning the Dataset\n\n::: panel-tabset\n## dplyr\n\n```{r}\n df |>\n     rename_all(toupper) |>\n     janitor::clean_names() |>\n     rename_all(toupper) |>\n     glimpse()\n```\n\n```{r}\ndf = df |>\n     rename_all(toupper) |>\n     janitor::clean_names() |>\n     rename_all(toupper)\nglimpse(df)\n```\n\n## panads\n\n```{python}\ndf_pd.columns.str.upper().to_list()\n```\n\n```{python}\n(df_pd\n     .pipe(remove_empty)\n     .pipe(lambda x: x.clean_names(case_type = \"upper\"))\n     .pipe(lambda x: x.rename(columns = {'SIZE_US_': 'SIZE_US', 'SIZE_EUROPE_':\"SIZE_EUROPE\", \"SIZE_UK_\":\"SIZE_UK\"}))\n     .pipe(lambda x: x.info())\n     )\n```\n\n```{python}\n# Changing the names of the columns to uppercase\ndf_pd.rename(columns = str.upper, inplace = True)\ndf_pd.columns\n```\n\n```{python}\n#| warning: false\nnew_column = df_pd.columns \\\n .str.replace(\"(\", '').str.replace(\")\", \"\") \\\n .str.replace(' ','_') # Cleaning the names of the variables\nnew_column\n```\n\n```{python}\ndf_pd.columns = new_column\ndf_pd.columns\ndf_pd.rename(columns=str.upper, inplace = True)\ndf_pd.columns \n\n```\n\n## polars \n\n```{python}\n# upper case column name\ndf_pl.rename({col: col.upper() for col in df_pl.columns})\ndf_pl.rename({col: col.strip().upper().replace(' ', '_') for col in df_pl.columns})\n```\n\n\n:::\n\n### Changing the Types of Variables\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    glimpse()\n```\n\n     From the above, it is now evident the the type of the `DATE` variable now is `date`.\n\n```{r}\ndf |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    mutate (PRODUCTID = as.character(PRODUCTID)) |>\n    glimpse()\n```\n\n     From the above, it is now evident the the type of the `DATE` and `PRODUCTID` variable now is date (`date`) and character (`chr`) respectively. We can now incorparte the changes into the data frame.\n\n```{r}\ndf = df |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    mutate (PRODUCTID = as.character(PRODUCTID)) \nglimpse(df)\n```\n\n## pandas\n\n```{python}\n(\n    df_pd\n    .pipe(lambda x: x.assign(DATE = pd.to_datetime(x['DATE'])))\n    .pipe(lambda x: x.info())\n)\n\n```\n\n```{python}\n# converting integer to object\ndf_pd.INVOICENO = df_pd.INVOICENO.astype(str)\ndf_pd[['MONTH', 'PRODUCTID']] = df_pd[['MONTH', 'PRODUCTID']].astype(str)\ndf_pd.info()\n```\n\n## polars \n\n\n```{python}\n# Change the type of columns \ndf_pl = df_pl.with_columns(\n    pl.col(\"Date\").str.strptime(pl.Date, \"%m/%d/%Y\")\n)\ndf_pl.head()\ndf_pl.schema\n```\n\n```{python}\n# change to character variable\ndf_pl = df_pl.with_columns(pl.col(\"InvoiceNo\").cast(pl.Utf8))\ndf_pl.schema\ndf_pl = df_pl.with_columns(pl.col([\"ProductID\", \"Month\"]).cast(pl.Utf8))\ndf_pl.schema\n```\n\n\n\n\n:::\n\n## Some Other Useful Functions\n\n     There are some other useful functions that can be used to explore the dataset for analysis. Some of those useful functions are discussed below.\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf|> count(YEAR)\n```\n\n```{r}\ndf|> count(COUNTRY)\n```\n\n```{r}\ndf|> distinct(COUNTRY)\n```\n\n## pandas\n\n```{python}\ndf_pd['YEAR'].value_counts()\n```\n\n```{python}\ndf_pd['YEAR'].unique()\n```\n\n\n## polars \n\n```{python}\ndf_pl.select(pl.col(\"Year\").value_counts())\ndf_pl.select(pl.col(\"Year\").value_counts()).unnest(\"Year\")\n\ndf_pl.select(pl.col(\"Gender\").value_counts())\ndf_pl.select(pl.col(\"Gender\").value_counts()).unnest(\"Gender\")\n\ndf_pl.group_by(\"Gender\").len()\ndf_pl.group_by(\"Gender\").len().rename({\"len\":\"Total\"})\n```\n\n\n:::\n\n## Six Verbs for EDA\n\n     @tbl-compareDplyrPandas shows the comparable functions in both `dplyr` and `pandas` packages. These functions are very much important to perform exploratory data analysis in both `R` and `Python`. `group_by` (`groupby` in pandas) and `summarize ()`[^eda-1] (`agg ()` in pandas) are often used together; therefore, they are in the same group in @tbl-compareDplyrPandas.\n\n[^eda-1]: You can also use British spelling - `summarise ()`\n\n```{r}\n#| include: false\ntidyverse_pandas = tibble::tribble(\n  ~`Verb Number`,~`tidyverse`, ~ `pandas`, ~ `polars` ,\n  '1','filter ()', 'query () or loc () or iloc ()', 'filter ()',\n  '2','arrange ()', 'sort_values ()', 'sort ()',\n  '3','select ()', 'filter () or loc ()', 'select ()', \n  '4','rename ()', 'rename ()', 'rename ()', \n  '5','mutate ()', 'assign ()', 'with_columns ()', \n  '6','group_by ()', 'groupby ()', 'group_by ()', \n  '6','summarize ()', 'agg ()', 'agg ()'\n)\n\n```\n\n```{r}\n#| label: tbl-compareDplyrPandas\n#| tbl-cap: Tidyverse and Pandas Equivalent Functions \n#| echo: false\n#| warning: false\n# These are R code to prepare Table 2 using KableExtra \nlibrary(kableExtra)\nkbl(tidyverse_pandas, booktabs = TRUE \n    #,caption = \"Tidyverse and Pandas Equivalent Functions\"\n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n### 1st Verb - filter () Function\n\n     Filter functions are used to subset a data frame based on rows, meaning that retaining rows that satisfy given conditions. Filtering rows is also called slicing[^eda-2] becasue we obtain a set of elements by filtering.\n\n[^eda-2]: Indexing involves obtaining individual elements.\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |> filter (YEAR == \"2015\")\n```\n\n```{r}\ndf |> filter (COUNTRY %in% c(\"United States\", \"Canada\"))\n```\n\n```{r}\ndf |> filter (COUNTRY == \"United States\", YEAR == \"2016\")\n```\n\n```{r}\ndf |> filter (COUNTRY == \"United States\", YEAR %in% c(\"2015\",\"2016\"))\n```\n\n```{r}\ndf |> filter (COUNTRY %in% c(\"United States\", \"Canada\"), YEAR == \"2014\")\n```\n\n## pandas\n\n```{python}\ndf_pd.query(\"YEAR == 2015\")\n```\n\n```{python}\ndf_pd.query('COUNTRY== \"United States\" | COUNTRY == \"Canada\"')\n```\n\n```{python}\ndf_pd.query(\"COUNTRY in ['United States', 'Canada']\")\n```\n\n```{python}\ndf_pd.query(\"COUNTRY== 'United States' & YEAR== 2016\")\n```\n\n```{python}\ndf_pd.query(\"COUNTRY== 'United States' & YEAR in [2015,2016]\")\n```\n\n```{python}\ndf_pd[df_pd['COUNTRY'] == \"United States\"]\n```\n\n```{python}\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\")]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY'].isin([\"United States\", \"Canada\"])]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY']\\\n .isin([\"United States\", \"Canada\"]) &(df_pd['YEAR']==2014)]\n```\n\n```{python}\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\") &(df_pd [\"YEAR\"] ==2014)]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY'] == \"United States\", :]\n```\n\n```{python}\ndf_pd.loc[\n    df_pd['COUNTRY']=='United States',\n    ['COUNTRY', \"UNITPRICE\", \"SALEPRICE\"]]\n```\n\n## polars \n\n```{python}\ndf_pl.filter(pl.col(\"Year\") == 2015)\n```\n\n```{python}\ndf_pl.filter(pl.col(\"Country\").is_in([\"United States\", \"Canada\"]))\n```\n\n```{python}\ndf_pl.filter(\n    (pl.col(\"Country\") == \"United States\") & (pl.col(\"Year\") == 2015)\n    )\n```\n\n```{python}\ndf_pl.filter(\n    (pl.col(\"Country\") == \"United States\") & (pl.col(\"Year\").is_in([2015, 2016]) )\n    )\n```\n\n```{python}\ndf_pl.filter(\n    (pl.col(\"Country\").is_in([\"United States\", \"Canada\"])) & (pl.col(\"Year\")== 2014)\n)\n```\n\n\n\n:::\n\n### 2nd Verb - arrange () Function\n\n     In arrange functions, we order the rows of a data frame by the values of given columns. It is like sorting or odering the data.\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    arrange(DATE)     \n```\n\n```{r}\ndf |>\n    arrange(desc(DATE))     \n```\n\n```{r}\ndf |>\n    arrange(MONTH, SALEPRICE)     \n```\n\n## pandas\n\n```{python}\ndf_pd.sort_values(by =['DATE'])   \n```\n\n```{python}\ndf_pd.sort_values(by =['DATE'], ascending = False)   \n```\n\n```{python}\ndf_pd.sort_values(by =['MONTH', 'SALEPRICE'])\n```\n\n## polars \n\n```{python}\ndf_pl.sort(\"Date\") # default asecending \n```\n\n```{python}\ndf_pl.sort(['Month', 'SalePrice'])\ndf_pl.sort(['Month', 'SalePrice'], descending = [True, False])\n```\n\n\n:::\n\n### 3rd Verb - select () Function\n\n     Select functions help to select or obtain columns from the data frame. When there are a lot of columns in our dataset, select functions become very useful.\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |> select(DATE, UNITPRICE, DISCOUNT)   \n```\n\n```{r}\ndf |> select(1:2, 5:8)  \n```\n\n```{r}\ndf |>\n    select(starts_with('SIZE'))\n```\n\n```{r}\ndf |>\n    select(ends_with('PRICE'))\n```\n\n```{r}\ndf |>\n    select(contains(\"_\"))\n```\n\n```{r}\ndf |>\n    select(matches(\"SIZE\"))\n```\n\n```{r}\ndf |>\n    select(matches(\"PRICE$\"))\n```\n\n```{r}\n# starts with letter S\ndf |>\n    select(matches(\"^S\"))\n```\n\n```{r}\ndf |>\n    select(where(is.character))\n```\n\n```{r}\ndf |>\n    select(where(is.numeric))\n```\n\n```{r}\ndf |>\n    select(MONTH, YEAR, everything())\n```\n\n```{r}\n# any_of () vs all_of ()\ndf |>\n    select(any_of(c(\"PRICE\", \"SIZE\")))\n```\n\n```{r}\n# Dropping columns \ndf |>\n    select(-DATE)\n```\n\n## pandas\n\n```{python}\ndf_pd['DATE']   \n```\n\n```{python}\ndf_pd[['DATE', 'UNITPRICE']]   \n```\n\n```{python}\ndf_pd.loc[:,['DATE', 'UNITPRICE']]   \n```\n\n```{python}\ndf_pd.iloc[:,5:8]\n```\n\n```{python}\ndf_pd.iloc[:,[3,5,8]]\n```\n\n```{python}\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n```\n\n```{python}\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n```\n\n```{python}\n #RegularExpression(Regex)\ndf_pd.filter(regex =\"PRICE$\") #Ends with Price\n```\n\n```{python}\ndf_pd.filter(regex =\"ˆSIZE\") #Starts with SIZE\n```\n\n```{python}\ndf_pd.filter(regex =\"PRICE\") #Contains the word Price\n```\n\n```{python}\ndf_pd.select_dtypes('object')\n```\n\n```{python}\ndf_pd.select_dtypes('int')\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.startswith('SIZE')]\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.contains('PRICE')]\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.endswith('PRICE')]\n```\n\n```{python}\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\n```\n\n```{python}\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\\\n    .pipe(lambda x: x.info())\n```\n\n```{python}\n# Rearranging columns \n# Sorting Alphabetically\ndf_pd.reindex(sorted(df_pd.columns), axis =1)\n```\n\n```{python}\n# Rearranging columns \n# Sorting As You Want (ASY)\ncol_first = ['YEAR','MONTH']\ncol_rest = df_pd.columns.difference(col_first, sort=False).to_list()\ndf_pd2 = df_pd [col_first +col_rest]\ndf_pd2.info()\n```\n\n## polars \n```{python}\ndf_pl.select([\"Date\", 'UnitPrice', \"Discount\"])\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if c.startswith(\"Size\")]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if c.endswith(\"Price\")]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if \"_\" in c]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if \"Size\" in c]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ncol_to_select = [c for c in df_pl.columns if c.startswith(\"S\")]\ndf_pl.select(pl.col(col_to_select))\n```\n\n```{python}\ndf_pl.select(pl.col(pl.Utf8)) # all character columns\ndf_pl.select(pl.col(pl.Int64)) # all numeric columns \n```\n\n```{python}\ncols = [\"Month\", \"Year\"] + [c for c in df_pl.columns if c not in [\"Month\", \"Year\"]]\ndf_pl.select(cols)\n```\n\n\n\n:::\n\n### 4th Verb - rename () Function\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    rename(INVOICE = INVOICENO,\n    PRODUCT = PRODUCTID) |>\n    glimpse()\n     \n```\n\n## pandas\n\n```{python}\n(df_pd.rename(columns = {\"PRODUCTID\": \"PRODUCT\", \"INVOICENO\": \"INVOICE\"})\n     .pipe(lambda x: x.info())\n)\n```\n\n## polars \n\n\n```{python}\ndf_pl = df_pl.rename({\"ProductID\": \"PRODUCTID\", \"InvoiceNo\":\"INVOICE\"})\ndf_pl.schema\n```\n\n:::\n\n### 5th Verb - mutate () Function\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    mutate(NECOLUMN = 5,\n    SALESPRICE2 = UNITPRICE*(1-DISCOUNT)) |>\n    glimpse()\n     \n```\n\n## pandas\n\n```{python}\ndf_pd['NEWCOLUMN'] = 5\ndf_pd.info()     \n```\n\n```{python}\ndf_pd.drop(columns = ['NEWCOLUMN'], axis = 1, inplace = True)   \ndf_pd.info() \n```\n\n```{python}\ndf_pd['SALEPRICE2']=df_pd['UNITPRICE']*(1-df_pd['DISCOUNT'])\ndf_pd.info()\n```\n\n```{python}\n# Using the assign() function\n(df_pd[['PRODUCTID', 'UNITPRICE', 'DISCOUNT']]\\\n    .assign(SALEPRICE3 =lambda x: x.UNITPRICE*(1-x.DISCOUNT)) \\\n    .head(5)\n)\n```\n\n## polars \n\n```{python}\ndf_pl.with_columns(\n    NEWCOLUMN = 5,\n    SALEPRICE2 = pl.col('UnitPrice')*(1-pl.col('Discount'))\n)\n```\n\n\n\n:::\n\n### 6th Verbs - group_by () and summarize () Functions\n\n     @fig-splitapplycombine presents Split Apply Combine principle in `group_by ()` and `summarize ()` functions.\n\n![Split Apply Combine Principle](images/split-apply-combine.png){#fig-splitapplycombine}\n\n::: panel-tabset\n## dplyr\n\n```{r}\ndf |>\n    group_by(COUNTRY) |>\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE))\n     \n```\n\n```{r}\ndf |>\n    group_by(COUNTRY) |>\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE),\n    AVGSALEPRICE = mean (SALEPRICE, na.rm = TRUE))\n     \n```\n\n```{r}\n# Summary Statistics \ndf %>%\n  select(UNITPRICE, SALEPRICE) %>%\n  summarize(across(where(is.numeric), \n                   .fns = list(N = ~length(.),\n                               Mean = mean, \n                               Std = sd,\n                               Median = median, \n                               P25 = ~quantile(.,0.25), \n                               P75 = ~quantile(., 0.75)\n                               )\n                   )) %>%\n  pivot_longer(everything(), names_sep='_', names_to=c('variable', '.value'))  \n```\n\n## pandas\n\n```{python}\ndf_pd.groupby(['COUNTRY'])['UNITPRICE'].mean()\n     \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY'])[['UNITPRICE', 'SALEPRICE']].mean()\n     \n```\n\n```{python}\n#| warning: false\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(np.mean) \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(\"mean\") \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"))\n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"),\n    TOTALN=(\"SALEPRICE\",\"size\"), # size function for n\n    TOTALOBS=(\"SALEPRICE\",\"count\") # count function for n\n )\n```\n\n```{python}\n# Defining a Function\ndef percentile(n):\n    def percentile_(x):\n        return x.quantile(n)\n    percentile_.__name__ ='percentile_{:02.0f}'.format(n*100)\n    return percentile_\n\n# Some summary statistics \ndf_pd[['UNITPRICE', 'SALEPRICE']] \\\n    .agg(['count', 'mean', 'std', 'median', percentile(0.25), percentile (0.75)]) \\\n    .transpose() \\\n    .reset_index() \\\n    .rename(columns = {\"index\": \"variables\", \"percentile_25\": \"P25\", \"percentile_75\": \"P75\", 'count': \"N\"}) \\\n    .round(3) \n```\n\n```{python}\n#| eval: false\n# Summary Statistics \nagg_dict = {\n    \"N\": \"count\",\n    'Mean':\"mean\",\n    \"Std. Dev\" : \"std\",\n    'P25': lambda x: x.quantile(0.25),\n    'Median': 'median',\n    'p75': lambda x: x.quantile(0.75)\n}\n\ndf_pd[['UNITPRICE', 'SALEPRICE']].agg(agg_dict)\n```\n\n## polars \n```{python}\ndf_pl.group_by(\"Country\").agg(pl.col(\"UnitPrice\").mean().alias(\"AvgPrice\"))\n```\n\n```{python}\ndf_pl.group_by(\"Country\").agg(\n    [pl.col(\"UnitPrice\").mean().alias(\"Avgprice\"),\n     pl.col(\"SalePrice\").mean().alias(\"AvgSalePrice\")]\n)\n```\n\n:::\n\n## Reshaping Data\n\n::: {style=\"text-align: justify\"}\n     Before we discuss about reshaping of the data, we need to discuss about tidy format of the data. Data can come in many shapes, but not all shapes are useful for data analysis. In most cases, tidy format of the data is most useful for analysis. Therefore, if the data is untidy, we need to make it tidy first. There are three interrelated rules which make a dataset tidy [@wickham2023r]. These rules are given below. @fig-tidyprinciple visually represents tidy principle.\n\n1.  Each variable must have its own column\n\n2.  Each observation must have its own row\n\n3.  Each value must have its own cell\n\n![Tidy Principle](images/tidy-1.png){#fig-tidyprinciple}\n\n     For analysis, many times we need to change the format of our dataset and we call it reshaping. Data come primarily in two shapes -*wide* and *long*. Sometimes *wide* format is called \"record\" format and *long* format is called \"stacked\" format. In `wide` format data, there is one row for each subject (units of observation). Data is `long` when there are multiple rows for each subject (units of observations).\n\n     This reshaping can be two types - a) long to wide and 2) wide to long. Long-to-wide means reshaping a long data, which has many rows, into wide format, which has many variables. In wide-to-long format, we do otherwise. For analytical purpose, reshaping data is useful; so, we need to know how to do the reshaping.\n\n     Whether a given dataset (e.g., @tbl-formatofdata) is in wide or long format depends on our research questions (on what variables we are interested in and how we conceive of our data). If we are interested in variable `Temp` and `Month` variable is the unit of obsevation, then the dataset in @tbl-formatofdata is in `long` format because `Month` is repeated in mutiple rows.\n:::\n\n```{r}\n#| warning: false\nairquality = airquality\n\nexamp = airquality |>\n    slice(1:10)\n```\n\n```{r}\n#| label: tbl-formatofdata\n#| tbl-cap: Which Format - Long or Wide \n#| echo: false\n#| warning: false\n\nkbl(examp, booktabs = TRUE \n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n### Long-to-Wide Format\n\n::: {style=\"text-align: justify\"}\n     To make a long dataset to wide, we can use `pivot_wider()` function from `tidyr` package in R and `pivot()` function from `pandas` in python.\n:::\n\n::: panel-tabset\n## tidyr\n\n```{r}\ntidyr::us_rent_income\n```\n\n```{r}\ntidyr::us_rent_income |>\n    pivot_wider(\n        names_from = variable, \n        values_from = c(estimate, moe)\n    )\n\n```\n\n## pandas\n\n```{python}\n#| warning: false\n# install palmerpenguins package\n# pip install palmerpenguins\nimport palmerpenguins\npenguins = palmerpenguins.load_penguins()\n```\n\n```{python}\npenguins[[\"island\", \"bill_length_mm\"]] \\\n    .pivot(columns = \"island\", values = \"bill_length_mm\") \\\n    .fillna(0)\n```\n\n## polars \n\n\n```{python}\nimport palmerpenguins\n\npenguins = palmerpenguins.load_penguins()\npenguins.head()\ndf_penguins = pl.DataFrame(penguins)\n```\n\n```{python}\n# Long to Wide (pivot)\ndf_penguins.with_columns(\n    pl.arange(0,df_penguins.height).alias(\"row_num\")\n).select(pl.col([\"row_num\", \"island\",\"bill_length_mm\"])\n).pivot(values = \"bill_length_mm\", on = \"island\", index = \"row_num\") \\\n    .fill_null (0)\n```\n\n\n:::\n\n### Wide-to-Long Format\n\n::: {style=\"text-align: justify\"}\n     To make a wide dataset to long, we can use `pivot_longer()` function from `tidyr` package in R and `melt()` function from `pandas` in python.\n:::\n\n::: panel-tabset\n## tidyr\n\n```{r}\nrelig_income\n```\n\n```{r}\nrelig_income |>\n    pivot_longer(\n        cols = !c(religion),\n        names_to = \"income\",\n        values_to = \"count\"\n    )\n```\n\n## pandas\n\n```{python}\npenguins.melt(value_vars=[\"bill_length_mm\", \"bill_depth_mm\",\"flipper_length_mm\", \"body_mass_g\"],\n              id_vars = ['species', 'island', 'sex', 'year']\n              )\n```\n\n## polars \n\n\n```{python}\n# Wide to Long (melt() but now polar calls it unpivot() )\ndf_penguins.with_columns(\n    pl.arange(0,df_penguins.height).alias(\"row_num\")\n).select(pl.col([\"row_num\", \"island\",\"bill_length_mm\"])\n).pivot(values = \"bill_length_mm\", on = \"island\", index = \"row_num\") \\\n    .unpivot(index = \"row_num\", value_name = \"bill_length_mm\")\n```\n\n\n\n\n:::\n\n## Merging Datasets\n\n::: {style=\"text-align: justify\"}\n     Many times, for analysis purposes, we need to join two datasets. This process is also called merging[^eda-3]. There are different types of joining. So, it is important to learn about those joining techniques. In @fig-joindatasets shows the joining technique and functions using `dplyr` in R. Below all of these joining functions are explained.\n\n-   `left_join()`: The merged dataset contains **all** observations fromthe **first** (or left) dataset and only **matched** observations from the **second** (or right) dataset\n\n-   `right_join()`: The merged dataset contains only **matched** observations from the **first** (or left) dataset and **all** observations from the **second** (or right) dataset\n\n-   `inner_join()`: The merged dataset contains *only* **matched** observations from both datasets\n\n-   `semi_join()`: The merged dataset contains **matched** observations from the **first** (or left) dataset. Please note that `semi_join()` differs from `inner_join()` in that `inner_join()` will return one row of first dataset (x) for each matching row of second dataset (y), whereas `semi_join()` will never duplicate rows of x.\n\n-   `full_join()`: The merged dataset contains **all** observations from both datasets\n\n-   `anti_join()`: The merged dataset contains only **not matched** observations from the **first** (or left) dataset and contains only the variable from the **left** dataset\n\n![Joining Datasets](images/join_diagram.png){#fig-joindatasets width=\"40%\"}\n\n```{r}\n#| include: false\ndf_join <- tribble(\n  ~dplyr, ~pandas, ~Description,\n  \"left_join()\", \"pd.merge(df1, df2, on='key', how='left')\", \"Join matching rows from df2 to df1, keeping all rows from df1.\",\n  \"right_join()\", \"pd.merge(df1, df2, on='key', how='right')\", \"Join matching rows from df1 to df2, keeping all rows from df2.\",\n  \"inner_join()\", \"pd.merge(df1, df2, on='key', how='inner')\", \"Join matching rows from both dataframes (default behavior of merge()).\",\n  \"full_join()\", \"pd.merge(df1, df2, on='key', how='outer')\", \"Join all rows from both dataframes, filling missing values with NaN.\",\n  \"semi_join()\", \"No direct equivalent, but can be achieved using filtering\", \"Keep rows in df1 where a match exists in df2.\",\n  \"anti_join()\", \"No direct equivalent, but can be achieved using filtering\", \"Keep rows in df1 where no match exists in df2.\"\n)\n```\n\n     @tbl-joinRPython compares the `dplyr` joining functions with equivalent joining functions from `pandas`.\n\n```{r}\n#| label: tbl-joinRPython\n#| tbl-cap: Joining Functions - dplyr vs pandas  \n#| echo: false\n#| warning: false\n\nkbl(df_join, booktabs = TRUE \n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n:::\n\n[^eda-3]: In database context, it is \"merging\", but commonly it is called \"joining\".\n\n::: panel-tabset\n## dplyr\n\n```{r}\n#| warning: false\ndata1 = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\nglimpse(data1)\n```\n\n```{r}\n#| warning: false\ndata2 = read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\")\nglimpse(data2)\n```\n\n```{r}\n# left_join\nleft_join(data1, data2, by = c(\"ticker\", \"year\"))\n```\n\n```{r}\n# left_join\nleft_join(data1 |> distinct(ticker, year, .keep_all = TRUE), \n          data2 |> distinct(ticker, year, .keep_all = TRUE), \n          by = c(\"ticker\", \"year\")\n          )\n```\n\n## pandas\n\n```{python}\n#| warning: false\ndataset1 = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\ndataset2 = pd.read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\",encoding=\"latin-1\")\n```\n\n```{python}\npd.merge(dataset1, dataset2, on=['ticker', 'year'], how='left')\n```\n\n```{python}\n#| warning: false\ndataset1_drop = dataset1.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\ndataset2_drop = dataset2.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\npd.merge(dataset1_drop, dataset2_drop, on=['ticker', 'year'], how='left')\n```\n\n```{python}\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})\ndf2 = pd.DataFrame({'id': [2, 3, 4], 'other_value': ['X', 'Y', 'Z']})\n\n# Left join\nleft_join = pd.merge(df1, df2, on='id', how='left')\n\n# Inner join\ninner_join = pd.merge(df1, df2, on='id')\n\n# Semi join\nsemi_join = df1[df1['id'].isin(df2['id'])]\n\n# Anti join\nanti_join = df1[~df1['id'].isin(df2['id'])]\n```\n\n## polars \n\n```{python}\ndataset1 = pl.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\ndataset2 = pl.read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\",encoding=\"latin-1\")\n\n```\n\n```{python}\ndataset1.join(dataset2, on = [\"ticker\", 'year'], how = \"left\")\n```\n\n```{python}\ndataset1_drop = dataset1.unique(subset=['ticker', 'year'])\ndataset2_drop = dataset2.unique(subset=['ticker', 'year'])\n```\n\n```{python}\ndataset1_drop.join(dataset2_drop, on = [\"ticker\", 'year'], how = \"left\")\n```\n\n```{python}\ndataset1_drop.join(dataset2_drop, on = [\"ticker\", 'year'], how = \"left\").shape\n```\n\n```{python}\ndf1 = pl.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})\ndf2 = pl.DataFrame({'id': [2, 3, 4], 'other_value': ['X', 'Y', 'Z']})\n```\n\n```{python}\n# Left join\nleft_join = df1.join(df2, on='id', how='left')\n```\n\n```{python}\n# Inner join\ninner_join = df1.join(df2, on='id')\n```\n\n```{python}\n# Semi join\nsemi_join = df1.join(df2, on=\"id\", how=\"semi\")\n```\n\n```{python}\n# Anti join\nanti_join = df1.join(df2, on=\"id\", how=\"anti\")\n```\n\n:::\n\n## Conclusions\n\n::: {style=\"text-align: justify\"}\n     Exploratory Data Analysis (EDA) is the crucial first phase of any data-focused project, turning raw data into meaningful insights that inform all later modeling and decision choices. By systematically applying descriptive statistics and visual techniques, EDA uncovers a dataset’s structure, trends, relationships, and irregularities, so that subsequent analyses rely on sound assumptions instead of guesswork. It enables analysts to spot errors and outliers, clarify what variables represent, and pinpoint the most important features, thereby strengthening model performance, interpretability, and the overall value of the conclusions drawn. \n\n     Rather than a single step, EDA is an iterative, reflective process that refines research questions, tests the suitability of methods, and ensures that technical work stays aligned with practical goals. Ultimately, EDA is essential: skipping it leaves statistical analysis and machine learning vulnerable to being elaborate but misguided, while using it turns them into dependable, transparent, and practically useful tools.\n\n:::"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc-depth":5,"output-file":"eda.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","jupyter":"python3","bibliography":["references.bib"],"editor":"visual","number-depth":5,"theme":"flatly","mainfont":"Georgia, serif","fig-cap-location":"bottom","callout-appearance":"default","cover-image":"images/titlepage_1_alt.webp","title":"Exploratory Data Analysis (EDA)"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"pdf-book"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"jupyter":"python3","bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt","mainfont":"Times New Roman","sansfont":"Arial","monofont":"Courier New","colorlinks":true,"title":"Exploratory Data Analysis (EDA)"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}