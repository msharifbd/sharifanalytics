{"title":"Exploratory Data Analysis (EDA)","markdown":{"yaml":{"title":"Exploratory Data Analysis (EDA)","format":"html"},"headingText":"Learning Objectives of the Chapter","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n::: {style=\"text-align: justify\"}\nAt the End of the Chapter, Students should be Able to -\n\n-   Learn about the purpose of Exploratory Data Analysis (EDA)\n\n-   Understand different techniques of transforming and cleaning data\n\n-   Learn about Different R and Python Packages for EDA\n\n-   Understand how to use six verbs for EDA\n\n-   Perform EDA on some real world data sets. \n\n-   Learn about how to interpret results from EDA\n:::\n\n\n## Introduction \n\n::: {style=\"text-align: justify\"}\n     In descriptive statistics, we summarize the data using different metrics such as mean, median, standard deviation, minimum value, maximum value, and percentile. Descriptive statisics is also called summary statistics.\n:::\n\n## Data Collection & Importing \n\n\n## Data Cleaning \n\n\n## Packages for Exploratory Data Analysis (EDA)\n\n::: {style=\"text-align: justify\"}\n     In order to use `pyjanitor`, the data frame must be pandas because `pyjanitor` extends pandas data frame functionality. \n:::\n\n\n::: {.panel-tabset}\n\n## dplyr\n\n```{r}\n#| warning: false\n# loading packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n```\n\n```{r}\n#| include: false\nlibrary(reticulate)\nSys.unsetenv(\"RETICULATE_PYTHON\")\nreticulate::use_virtualenv(\"C:/Users/mshar/OneDrive - Southern Illinois University/ANALYTICS_FOR_ACCOUNTING_DATA/accounting_analytics_book\", required = TRUE)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n#py_install(\"pyjanitor\")\n#py_install(\"polars\")\n#Sys.setenv('RETICULATE_PYTHON' = '~/.venv/quarto_book_python/Scripts/python.exe')\n```\n\n\n## pandas \n```{python}\n# loading the package\nimport numpy as np\nimport pandas as pd\n# from pyjanitor package \n# pip install pyjanitor\nimport janitor \nfrom janitor import clean_names, remove_empty\n```\n\n:::\n\n## Importing the Dataset \n\n::: {.panel-tabset}\n\n## dplyr\n```{r}\n#| warning: false\n# importing data frame \ndf = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n## pandas \n```{python}\n# importing data frame \ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n\n\n:::\n\n## Meta Data\n\n::: {style=\"text-align: justify\"}\n     Meta data is data about the data. Before we put the data into analysis, we need to learn about our dataset. This learning invovles knowing about the number of rows, number of columns, the types of the fields, the appropriateness of those types, the missing values in the dataset and so on. \n:::\n\n::: {.panel-tabset}\n\n## dplyr \n\n```{r}\nglimpse(df)\n```\n\n\n```{r}\nmap_df(df, ~sum(is.na(.))) |>\n     glimpse()\n```\n\n```{r}\nncol(df)\nnrow(df)\n```\n\n```{r}\nhead(df)\n```\n\n```{r}\ntail(df)\n```\n\n```{r}\ndplyr::sample_n(df, 10)\n```\n\n\n## Pandas \n\n```{python}\ndf_pd.info()\n```\n\n```{python}\ndf_pd.shape\n```\n\n```{python}\nprint('The total number of rows and columns of the product data is \\\n {} and {} respectively.'.format(df_pd.shape[0], df_pd.shape[1]))\n```\n\n\n```{python}\nprint(f'The total number of rows and columns of the product data is \\\n {df_pd.shape[0]} and {df_pd.shape[1]} respectively.')\n```\n\n```{python}\ndf_pd.columns\n```\n\n```{python}\ndf_pd.head()\n```\n\n```{python}\ndf_pd.tail()\n```\n\n```{python}\ndf_pd.isna().sum()\n```\n\n\n```{python}\ndf_pd.dtypes\n```\n\n```{python}\ndf_pd.sample(n=10)\n```\n\n\n:::\n\n## Cleaning the Dataset \n\n::: {.panel-tabset}\n\n## dplyr\n\n```{r}\n df |>\n     rename_all(toupper) |>\n     janitor::clean_names() |>\n     rename_all(toupper) |>\n     glimpse()\n```\n\n```{r}\ndf = df |>\n     rename_all(toupper) |>\n     janitor::clean_names() |>\n     rename_all(toupper)\nglimpse(df)\n```\n\n\n## panads \n\n```{python}\ndf_pd.columns.str.upper().to_list()\n```\n\n```{python}\n(df_pd\n     .pipe(remove_empty)\n     .pipe(lambda x: x.clean_names(case_type = \"upper\"))\n     .pipe(lambda x: x.rename(columns = {'SIZE_US_': 'SIZE_US', 'SIZE_EUROPE_':\"SIZE_EUROPE\", \"SIZE_UK_\":\"SIZE_UK\"}))\n     .pipe(lambda x: x.info())\n     )\n```\n\n```{python}\n# Changing the names of the columns to uppercase\ndf_pd.rename(columns = str.upper, inplace = True)\ndf_pd.columns\n```\n\n\n```{python}\n#| warning: false\nnew_column = df_pd.columns \\\n .str.replace(\"(\", '').str.replace(\")\", \"\") \\\n .str.replace(' ','_') # Cleaning the names of the variables\nnew_column\n```\n\n\n```{python}\ndf_pd.columns = new_column\ndf_pd.columns\ndf_pd.rename(columns=str.upper, inplace = True)\ndf_pd.columns \n\n```\n\n\n::: \n\n\n### Changing the Types of Variables \n\n::: {.panel-tabset}\n\n## dplyr\n\n```{r}\ndf |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    glimpse()\n```\n\n     From the above, it is now evident the the type of the `DATE` variable now is `date`. \n\n```{r}\ndf |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    mutate (PRODUCTID = as.character(PRODUCTID)) |>\n    glimpse()\n```\n\n     From the above, it is now evident the the type of the `DATE` and `PRODUCTID` variable now is date (`date`) and character (`chr`) respectively. We can now incorparte the changes into the data frame. \n\n```{r}\ndf = df |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    mutate (PRODUCTID = as.character(PRODUCTID)) \nglimpse(df)\n```\n\n\n## pandas \n\n```{python}\n(\n    df_pd\n    .pipe(lambda x: x.assign(DATE = pd.to_datetime(x['DATE'])))\n    .pipe(lambda x: x.info())\n)\n\n```\n\n\n```{python}\n# converting integer to object\ndf_pd.INVOICENO = df_pd.INVOICENO.astype(str)\ndf_pd[['MONTH', 'PRODUCTID']] = df_pd[['MONTH', 'PRODUCTID']].astype(str)\ndf_pd.info()\n```\n\n\n\n:::\n\n## Some Other Useful Functions \n     There are some other useful functions that can be used to explore the dataset for analysis. Some of those useful functions are discussed below. \n\n:::{.panel-tabset}\n\n\n## dplyr\n\n```{r}\ndf|> count(YEAR)\n```\n\n```{r}\ndf|> count(COUNTRY)\n```\n\n```{r}\ndf|> distinct(COUNTRY)\n```\n\n## pandas \n```{python}\ndf_pd['YEAR'].value_counts()\n```\n\n```{python}\ndf_pd['YEAR'].unique()\n```\n\n\n:::\n\n\n## Six Verbs for EDA \n\n     @tbl-compareDplyrPandas shows the comparable functions in both `dplyr` and `pandas` packages. These functions are very much important to perform exploratory data analysis in both `R` and `Python`. `group_by` (`groupby` in pandas) and `summarize ()`^[You can also use British spelling - `summarise ()`] (`agg ()` in pandas) are often used together; therefore, they are in the same group in @tbl-compareDplyrPandas. \n\n```{r}\n#| include: false\ntidyverse_pandas = tibble::tribble(\n  ~`Verb Number`,~`tidyverse`, ~ `pandas`, \n  '1','filter ()', 'query () or loc () or iloc ()',\n  '2','arrange ()', 'sort_values ()',\n  '3','select ()', 'filter () or loc ()',\n  '4','rename ()', 'rename ()',\n  '5','mutate ()', 'assign ()',\n  '6','group_by ()', 'groupby ()',\n  '6','summarize ()', 'agg ()'\n)\n\n```\n\n```{r}\n#| label: tbl-compareDplyrPandas\n#| tbl-cap: Tidyverse and Pandas Equivalent Functions \n#| echo: false\n#| warning: false\n# These are R code to prepare Table 2 using KableExtra \nlibrary(kableExtra)\nkbl(tidyverse_pandas, booktabs = TRUE \n    #,caption = \"Tidyverse and Pandas Equivalent Functions\"\n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n### 1st Verb - filter () Function \n\n     Filter functions are used to subset a data frame based on rows, meaning that retaining rows that satisfy given conditions. Filtering rows is also called slicing^[Indexing involves obtaining individual elements.] becasue we obtain a set of elements by filtering.  \n\n:::{.panel-tabset}\n\n## dplyr \n\n```{r}\ndf |> filter (YEAR == \"2015\")\n```\n\n```{r}\ndf |> filter (COUNTRY %in% c(\"United States\", \"Canada\"))\n```\n\n```{r}\ndf |> filter (COUNTRY == \"United States\", YEAR == \"2016\")\n```\n\n```{r}\ndf |> filter (COUNTRY == \"United States\", YEAR %in% c(\"2015\",\"2016\"))\n```\n\n```{r}\ndf |> filter (COUNTRY %in% c(\"United States\", \"Canada\"), YEAR == \"2014\")\n```\n\n\n## pandas \n\n```{python}\ndf_pd.query(\"YEAR == 2015\")\n```\n\n```{python}\ndf_pd.query('COUNTRY== \"United States\" | COUNTRY == \"Canada\"')\n```\n\n```{python}\ndf_pd.query(\"COUNTRY in ['United States', 'Canada']\")\n```\n\n```{python}\ndf_pd.query(\"COUNTRY== 'United States' & YEAR== 2016\")\n```\n\n```{python}\ndf_pd.query(\"COUNTRY== 'United States' & YEAR in [2015,2016]\")\n```\n\n```{python}\ndf_pd[df_pd['COUNTRY'] == \"United States\"]\n```\n\n```{python}\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\")]\n```\n\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY'].isin([\"United States\", \"Canada\"])]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY']\\\n .isin([\"United States\", \"Canada\"]) &(df_pd['YEAR']==2014)]\n```\n\n```{python}\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\") &(df_pd [\"YEAR\"] ==2014)]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY'] == \"United States\", :]\n```\n\n```{python}\ndf_pd.loc[\n    df_pd['COUNTRY']=='United States',\n    ['COUNTRY', \"UNITPRICE\", \"SALEPRICE\"]]\n```\n\n:::\n\n### 2nd Verb - arrange () Function \n\n     In arrange functions, we order the rows of a data frame by the values of given columns. It is like sorting or odering the data. \n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |>\n    arrange(DATE)     \n```\n\n```{r}\ndf |>\n    arrange(desc(DATE))     \n```\n\n\n```{r}\ndf |>\n    arrange(MONTH, SALEPRICE)     \n```\n\n\n\n## pandas \n```{python}\ndf_pd.sort_values(by =['DATE'])   \n```\n\n```{python}\ndf_pd.sort_values(by =['DATE'], ascending = False)   \n```\n\n```{python}\ndf_pd.sort_values(by =['MONTH', 'SALEPRICE'])\n```\n\n\n:::\n\n\n### 3rd Verb - select () Function \n     Select functions help to select or obtain columns from the data frame. When there are a lot of columns in our dataset, select functions become very useful. \n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |> select(DATE, UNITPRICE, DISCOUNT)   \n```\n\n\n```{r}\ndf |> select(1:2, 5:8)  \n```\n\n```{r}\ndf |>\n    select(starts_with('SIZE'))\n```\n\n```{r}\ndf |>\n    select(ends_with('PRICE'))\n```\n\n\n```{r}\ndf |>\n    select(contains(\"_\"))\n```\n\n```{r}\ndf |>\n    select(matches(\"SIZE\"))\n```\n\n```{r}\ndf |>\n    select(matches(\"PRICE$\"))\n```\n\n```{r}\n# starts with letter S\ndf |>\n    select(matches(\"^S\"))\n```\n\n\n```{r}\ndf |>\n    select(where(is.character))\n```\n\n```{r}\ndf |>\n    select(where(is.numeric))\n```\n\n```{r}\ndf |>\n    select(MONTH, YEAR, everything())\n```\n\n```{r}\n# any_of () vs all_of ()\ndf |>\n    select(any_of(c(\"PRICE\", \"SIZE\")))\n```\n\n\n```{r}\n# Dropping columns \ndf |>\n    select(-DATE)\n```\n\n\n\n## pandas \n```{python}\ndf_pd['DATE']   \n```\n\n```{python}\ndf_pd[['DATE', 'UNITPRICE']]   \n```\n\n```{python}\ndf_pd.loc[:,['DATE', 'UNITPRICE']]   \n```\n\n\n```{python}\ndf_pd.iloc[:,5:8]\n```\n\n```{python}\ndf_pd.iloc[:,[3,5,8]]\n```\n\n```{python}\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n```\n\n```{python}\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n```\n\n```{python}\n #RegularExpression(Regex)\ndf_pd.filter(regex =\"PRICE$\") #Ends with Price\n```\n\n```{python}\ndf_pd.filter(regex =\"ˆSIZE\") #Starts with SIZE\n```\n\n\n```{python}\ndf_pd.filter(regex =\"PRICE\") #Contains the word Price\n```\n\n```{python}\ndf_pd.select_dtypes('object')\n```\n\n```{python}\ndf_pd.select_dtypes('int')\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.startswith('SIZE')]\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.contains('PRICE')]\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.endswith('PRICE')]\n```\n\n\n```{python}\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\n```\n\n```{python}\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\\\n    .pipe(lambda x: x.info())\n```\n\n```{python}\n# Rearranging columns \n# Sorting Alphabetically\ndf_pd.reindex(sorted(df_pd.columns), axis =1)\n```\n\n```{python}\n# Rearranging columns \n# Sorting As You Want (ASY)\ncol_first = ['YEAR','MONTH']\ncol_rest = df_pd.columns.difference(col_first, sort=False).to_list()\ndf_pd2 = df_pd [col_first +col_rest]\ndf_pd2.info()\n```\n\n\n:::\n\n\n### 4th Verb - rename () Function \n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |>\n    rename(INVOICE = INVOICENO,\n    PRODUCT = PRODUCTID) |>\n    glimpse()\n     \n```\n\n## pandas \n```{python}\n(df_pd.rename(columns = {\"PRODUCTID\": \"PRODUCT\", \"INVOICENO\": \"INVOICE\"})\n     .pipe(lambda x: x.info())\n)\n```\n\n\n:::\n\n### 5th Verb - mutate () Function \n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |>\n    mutate(NECOLUMN = 5,\n    SALESPRICE2 = UNITPRICE*(1-DISCOUNT)) |>\n    glimpse()\n     \n```\n\n## pandas \n```{python}\ndf_pd['NEWCOLUMN'] = 5\ndf_pd.info()     \n```\n\n```{python}\ndf_pd.drop(columns = ['NEWCOLUMN'], axis = 1, inplace = True)   \ndf_pd.info() \n```\n\n```{python}\ndf_pd['SALEPRICE2']=df_pd['UNITPRICE']*(1-df_pd['DISCOUNT'])\ndf_pd.info()\n```\n\n\n```{python}\n# Using the assign() function\n(df_pd[['PRODUCTID', 'UNITPRICE', 'DISCOUNT']]\\\n    .assign(SALEPRICE3 =lambda x: x.UNITPRICE*(1-x.DISCOUNT)) \\\n    .head(5)\n)\n```\n\n\n:::\n\n### 6th Verbs - group_by () and summarize () Functions \n\n     @fig-splitapplycombine presents Split Apply Combine principle in `group_by ()` and `summarize ()` functions. \n\n![Split Apply Combine Principle](images/split-apply-combine.png){#fig-splitapplycombine}\n\n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |>\n    group_by(COUNTRY) |>\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE))\n     \n```\n\n```{r}\ndf |>\n    group_by(COUNTRY) |>\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE),\n    AVGSALEPRICE = mean (SALEPRICE, na.rm = TRUE))\n     \n```\n\n\n```{r}\n# Summary Statistics \ndf %>%\n  select(UNITPRICE, SALEPRICE) %>%\n  summarize(across(where(is.numeric), \n                   .fns = list(N = ~length(.),\n                               Mean = mean, \n                               Std = sd,\n                               Median = median, \n                               P25 = ~quantile(.,0.25), \n                               P75 = ~quantile(., 0.75)\n                               )\n                   )) %>%\n  pivot_longer(everything(), names_sep='_', names_to=c('variable', '.value'))  \n```\n\n\n\n## pandas \n```{python}\ndf_pd.groupby(['COUNTRY'])['UNITPRICE'].mean()\n     \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY'])[['UNITPRICE', 'SALEPRICE']].mean()\n     \n```\n\n\n```{python}\n#| warning: false\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(np.mean) \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(\"mean\") \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"))\n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"),\n    TOTALN=(\"SALEPRICE\",\"size\"), # size function for n\n    TOTALOBS=(\"SALEPRICE\",\"count\") # count function for n\n )\n```\n\n```{python}\n# Defining a Function\ndef percentile(n):\n    def percentile_(x):\n        return x.quantile(n)\n    percentile_.__name__ ='percentile_{:02.0f}'.format(n*100)\n    return percentile_\n\n# Some summary statistics \ndf_pd[['UNITPRICE', 'SALEPRICE']] \\\n    .agg(['count', 'mean', 'std', 'median', percentile(0.25), percentile (0.75)]) \\\n    .transpose() \\\n    .reset_index() \\\n    .rename(columns = {\"index\": \"variables\", \"percentile_25\": \"P25\", \"percentile_75\": \"P75\", 'count': \"N\"}) \\\n    .round(3) \n```\n\n```{python}\n#| eval: false\n# Summary Statistics \nagg_dict = {\n    \"N\": \"count\",\n    'Mean':\"mean\",\n    \"Std. Dev\" : \"std\",\n    'P25': lambda x: x.quantile(0.25),\n    'Median': 'median',\n    'p75': lambda x: x.quantile(0.75)\n}\n\ndf_pd[['UNITPRICE', 'SALEPRICE']].agg(agg_dict)\n```\n\n\n:::\n\n## Reshaping Data \n\n::: {style=\"text-align: justify\"}\n     Before we discuss about reshaping of the data, we need to discuss about tidy format of the data. Data can come in many shapes, but not all shapes are useful for data analysis. In most cases, tidy format of the data is most useful for analysis. Therefore, if the data is untidy, we need to make it tidy first. There are three interrelated rules which make a dataset tidy [@wickham2023r]. These rules are given below. @fig-tidyprinciple visually represents tidy principle. \n\n1. Each variable must have its own column\n\n2. Each observation must have its own row\n\n3. Each value must have its own cell \n\n![Tidy Principle](images/tidy-1.png){#fig-tidyprinciple}\n\n     For analysis, many times we need to change the format of our dataset and we call it reshaping. Data come primarily in two shapes -*wide* and *long*. Sometimes *wide* format is called \"record\" format and *long* format is called \"stacked\" format. In `wide` format data, there is one row for each subject (units of observation). Data is `long` when there are multiple rows for each subject (units of observations). \n\n     This reshaping can be two types - a) long to wide and 2) wide to long. Long-to-wide means reshaping a long data, which has many rows, into wide format, which has many variables. In wide-to-long format, we do otherwise. For analytical purpose, reshaping data is useful; so, we need to know how to do the reshaping. \n\n     Whether a given dataset (e.g., @tbl-formatofdata) is in wide or long format depends on our research questions (on what variables we are interested in and how we conceive of our data). If we are interested in variable `Temp` and `Month` variable is the unit of obsevation, then the dataset in @tbl-formatofdata is in `long` format because `Month` is repeated in mutiple rows. \n\n:::\n\n```{r}\n#| warning: false\nairquality = airquality\n\nexamp = airquality |>\n    slice(1:10)\n```\n\n\n```{r}\n#| label: tbl-formatofdata\n#| tbl-cap: Which Format - Long or Wide \n#| echo: false\n#| warning: false\n\nkbl(examp, booktabs = TRUE \n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n\n### Long-to-Wide Format  \n\n::: {style=\"text-align: justify\"}\n     To make a long dataset to wide, we can use `pivot_wider()` function from `tidyr` package in R and `pivot()` function from `pandas` in python. \n:::\n\n\n:::{.panel-tabset}\n\n## tidyr\n```{r}\ntidyr::us_rent_income\n```\n\n```{r}\ntidyr::us_rent_income |>\n    pivot_wider(\n        names_from = variable, \n        values_from = c(estimate, moe)\n    )\n\n```\n\n\n## pandas \n\n```{python}\n#| warning: false\n# install palmerpenguins package\n# pip install palmerpenguins\nimport palmerpenguins\npenguins = palmerpenguins.load_penguins()\n```\n\n```{python}\npenguins[[\"island\", \"bill_length_mm\"]] \\\n    .pivot(columns = \"island\", values = \"bill_length_mm\") \\\n    .fillna(0)\n```\n\n\n:::\n\n\n### Wide-to-Long Format  \n\n::: {style=\"text-align: justify\"}\n     To make a wide dataset to long, we can use `pivot_longer()` function from `tidyr` package in R and `melt()` function from `pandas` in python.\n:::\n\n\n:::{.panel-tabset}\n\n## tidyr\n```{r}\nrelig_income\n```\n\n```{r}\nrelig_income |>\n    pivot_longer(\n        cols = !c(religion),\n        names_to = \"income\",\n        values_to = \"count\"\n    )\n```\n\n## pandas \n\n```{python}\npenguins.melt(value_vars=[\"bill_length_mm\", \"bill_depth_mm\",\"flipper_length_mm\", \"body_mass_g\"],\n              id_vars = ['species', 'island', 'sex', 'year']\n              )\n```\n\n:::\n\n\n## Merging Datasets \n\n::: {style=\"text-align: justify\"}\n     Many times, for analysis purposes, we need to join two datasets. This process is also called merging^[In database context, it is \"merging\", but commonly it is called \"joining\".]. There are different types of joining. So, it is important to learn about those joining techniques. In @fig-joindatasets shows the joining technique and functions using `dplyr` in R. Below all of these joining functions are explained. \n\n* `left_join()`: The merged dataset contains **all** observations fromthe **first** (or left) dataset and only **matched** observations from the **second** (or right) dataset\n\n* `right_join()`: The merged dataset contains only **matched** observations from the **first** (or left) dataset and **all** observations from the **second** (or right) dataset\n\n* `inner_join()`: The merged dataset contains *only* **matched** observations from both datasets\n\n* `semi_join()`: The merged dataset contains **matched** observations from the **first** (or left) dataset. Please note that `semi_join()` differs from `inner_join()` in that `inner_join()` will return one row of first dataset (x) for each matching row of second dataset (y), whereas `semi_join()` will never duplicate rows of x.   \n\n* `full_join()`: The merged dataset contains **all** observations from both datasets\n\n* `anti_join()`: The merged dataset contains only **not matched** observations from the **first** (or left) dataset and contains only the variable from the **left** dataset\n\n![Joining Datasets](images/join_diagram.png){#fig-joindatasets width=\"40%\"}\n\n```{r}\n#| include: false\ndf_join <- tribble(\n  ~dplyr, ~pandas, ~Description,\n  \"left_join()\", \"pd.merge(df1, df2, on='key', how='left')\", \"Join matching rows from df2 to df1, keeping all rows from df1.\",\n  \"right_join()\", \"pd.merge(df1, df2, on='key', how='right')\", \"Join matching rows from df1 to df2, keeping all rows from df2.\",\n  \"inner_join()\", \"pd.merge(df1, df2, on='key', how='inner')\", \"Join matching rows from both dataframes (default behavior of merge()).\",\n  \"full_join()\", \"pd.merge(df1, df2, on='key', how='outer')\", \"Join all rows from both dataframes, filling missing values with NaN.\",\n  \"semi_join()\", \"No direct equivalent, but can be achieved using filtering\", \"Keep rows in df1 where a match exists in df2.\",\n  \"anti_join()\", \"No direct equivalent, but can be achieved using filtering\", \"Keep rows in df1 where no match exists in df2.\"\n)\n```\n\n     @tbl-joinRPython compares the `dplyr` joining functions with equivalent joining functions from `pandas`.\n\n```{r}\n#| label: tbl-joinRPython\n#| tbl-cap: Joining Functions - dplyr vs pandas  \n#| echo: false\n#| warning: false\n\nkbl(df_join, booktabs = TRUE \n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n\n:::\n\n:::{.panel-tabset}\n\n## dplyr\n\n```{r}\n#| warning: false\ndata1 = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\nglimpse(data1)\n```\n\n```{r}\n#| warning: false\ndata2 = read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\")\nglimpse(data2)\n```\n\n```{r}\n# left_join\nleft_join(data1, data2, by = c(\"ticker\", \"year\"))\n```\n\n```{r}\n# left_join\nleft_join(data1 |> distinct(ticker, year, .keep_all = TRUE), \n          data2 |> distinct(ticker, year, .keep_all = TRUE), \n          by = c(\"ticker\", \"year\")\n          )\n```\n\n\n## pandas \n\n```{python}\n#| warning: false\ndataset1 = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\ndataset2 = pd.read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\",encoding=\"latin-1\")\n```\n\n```{python}\npd.merge(dataset1, dataset2, on=['ticker', 'year'], how='left')\n```\n\n```{python}\n#| warning: false\ndataset1_drop = dataset1.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\ndataset2_drop = dataset2.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\npd.merge(dataset1_drop, dataset2_drop, on=['ticker', 'year'], how='left')\n```\n\n```{python}\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})\ndf2 = pd.DataFrame({'id': [2, 3, 4], 'other_value': ['X', 'Y', 'Z']})\n\n# Left join\nleft_join = pd.merge(df1, df2, on='id', how='left')\n\n# Inner join\ninner_join = pd.merge(df1, df2, on='id')\n\n# Semi join\nsemi_join = df1[df1['id'].isin(df2['id'])]\n\n# Anti join\nanti_join = df1[~df1['id'].isin(df2['id'])]\n```\n\n:::\n\n## Conclusions \n","srcMarkdownNoYaml":"\n\n### Learning Objectives of the Chapter {.unnumbered}\n\n::: {style=\"text-align: justify\"}\nAt the End of the Chapter, Students should be Able to -\n\n-   Learn about the purpose of Exploratory Data Analysis (EDA)\n\n-   Understand different techniques of transforming and cleaning data\n\n-   Learn about Different R and Python Packages for EDA\n\n-   Understand how to use six verbs for EDA\n\n-   Perform EDA on some real world data sets. \n\n-   Learn about how to interpret results from EDA\n:::\n\n\n## Introduction \n\n::: {style=\"text-align: justify\"}\n     In descriptive statistics, we summarize the data using different metrics such as mean, median, standard deviation, minimum value, maximum value, and percentile. Descriptive statisics is also called summary statistics.\n:::\n\n## Data Collection & Importing \n\n\n## Data Cleaning \n\n\n## Packages for Exploratory Data Analysis (EDA)\n\n::: {style=\"text-align: justify\"}\n     In order to use `pyjanitor`, the data frame must be pandas because `pyjanitor` extends pandas data frame functionality. \n:::\n\n\n::: {.panel-tabset}\n\n## dplyr\n\n```{r}\n#| warning: false\n# loading packages\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n```\n\n```{r}\n#| include: false\nlibrary(reticulate)\nSys.unsetenv(\"RETICULATE_PYTHON\")\nreticulate::use_virtualenv(\"C:/Users/mshar/OneDrive - Southern Illinois University/ANALYTICS_FOR_ACCOUNTING_DATA/accounting_analytics_book\", required = TRUE)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n#py_install(\"pyjanitor\")\n#py_install(\"polars\")\n#Sys.setenv('RETICULATE_PYTHON' = '~/.venv/quarto_book_python/Scripts/python.exe')\n```\n\n\n## pandas \n```{python}\n# loading the package\nimport numpy as np\nimport pandas as pd\n# from pyjanitor package \n# pip install pyjanitor\nimport janitor \nfrom janitor import clean_names, remove_empty\n```\n\n:::\n\n## Importing the Dataset \n\n::: {.panel-tabset}\n\n## dplyr\n```{r}\n#| warning: false\n# importing data frame \ndf = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n## pandas \n```{python}\n# importing data frame \ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/main/Al-Bundy_raw-data.csv\")\n```\n\n\n\n:::\n\n## Meta Data\n\n::: {style=\"text-align: justify\"}\n     Meta data is data about the data. Before we put the data into analysis, we need to learn about our dataset. This learning invovles knowing about the number of rows, number of columns, the types of the fields, the appropriateness of those types, the missing values in the dataset and so on. \n:::\n\n::: {.panel-tabset}\n\n## dplyr \n\n```{r}\nglimpse(df)\n```\n\n\n```{r}\nmap_df(df, ~sum(is.na(.))) |>\n     glimpse()\n```\n\n```{r}\nncol(df)\nnrow(df)\n```\n\n```{r}\nhead(df)\n```\n\n```{r}\ntail(df)\n```\n\n```{r}\ndplyr::sample_n(df, 10)\n```\n\n\n## Pandas \n\n```{python}\ndf_pd.info()\n```\n\n```{python}\ndf_pd.shape\n```\n\n```{python}\nprint('The total number of rows and columns of the product data is \\\n {} and {} respectively.'.format(df_pd.shape[0], df_pd.shape[1]))\n```\n\n\n```{python}\nprint(f'The total number of rows and columns of the product data is \\\n {df_pd.shape[0]} and {df_pd.shape[1]} respectively.')\n```\n\n```{python}\ndf_pd.columns\n```\n\n```{python}\ndf_pd.head()\n```\n\n```{python}\ndf_pd.tail()\n```\n\n```{python}\ndf_pd.isna().sum()\n```\n\n\n```{python}\ndf_pd.dtypes\n```\n\n```{python}\ndf_pd.sample(n=10)\n```\n\n\n:::\n\n## Cleaning the Dataset \n\n::: {.panel-tabset}\n\n## dplyr\n\n```{r}\n df |>\n     rename_all(toupper) |>\n     janitor::clean_names() |>\n     rename_all(toupper) |>\n     glimpse()\n```\n\n```{r}\ndf = df |>\n     rename_all(toupper) |>\n     janitor::clean_names() |>\n     rename_all(toupper)\nglimpse(df)\n```\n\n\n## panads \n\n```{python}\ndf_pd.columns.str.upper().to_list()\n```\n\n```{python}\n(df_pd\n     .pipe(remove_empty)\n     .pipe(lambda x: x.clean_names(case_type = \"upper\"))\n     .pipe(lambda x: x.rename(columns = {'SIZE_US_': 'SIZE_US', 'SIZE_EUROPE_':\"SIZE_EUROPE\", \"SIZE_UK_\":\"SIZE_UK\"}))\n     .pipe(lambda x: x.info())\n     )\n```\n\n```{python}\n# Changing the names of the columns to uppercase\ndf_pd.rename(columns = str.upper, inplace = True)\ndf_pd.columns\n```\n\n\n```{python}\n#| warning: false\nnew_column = df_pd.columns \\\n .str.replace(\"(\", '').str.replace(\")\", \"\") \\\n .str.replace(' ','_') # Cleaning the names of the variables\nnew_column\n```\n\n\n```{python}\ndf_pd.columns = new_column\ndf_pd.columns\ndf_pd.rename(columns=str.upper, inplace = True)\ndf_pd.columns \n\n```\n\n\n::: \n\n\n### Changing the Types of Variables \n\n::: {.panel-tabset}\n\n## dplyr\n\n```{r}\ndf |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    glimpse()\n```\n\n     From the above, it is now evident the the type of the `DATE` variable now is `date`. \n\n```{r}\ndf |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    mutate (PRODUCTID = as.character(PRODUCTID)) |>\n    glimpse()\n```\n\n     From the above, it is now evident the the type of the `DATE` and `PRODUCTID` variable now is date (`date`) and character (`chr`) respectively. We can now incorparte the changes into the data frame. \n\n```{r}\ndf = df |>\n    mutate (DATE = lubridate::mdy(DATE)) |>\n    mutate (PRODUCTID = as.character(PRODUCTID)) \nglimpse(df)\n```\n\n\n## pandas \n\n```{python}\n(\n    df_pd\n    .pipe(lambda x: x.assign(DATE = pd.to_datetime(x['DATE'])))\n    .pipe(lambda x: x.info())\n)\n\n```\n\n\n```{python}\n# converting integer to object\ndf_pd.INVOICENO = df_pd.INVOICENO.astype(str)\ndf_pd[['MONTH', 'PRODUCTID']] = df_pd[['MONTH', 'PRODUCTID']].astype(str)\ndf_pd.info()\n```\n\n\n\n:::\n\n## Some Other Useful Functions \n     There are some other useful functions that can be used to explore the dataset for analysis. Some of those useful functions are discussed below. \n\n:::{.panel-tabset}\n\n\n## dplyr\n\n```{r}\ndf|> count(YEAR)\n```\n\n```{r}\ndf|> count(COUNTRY)\n```\n\n```{r}\ndf|> distinct(COUNTRY)\n```\n\n## pandas \n```{python}\ndf_pd['YEAR'].value_counts()\n```\n\n```{python}\ndf_pd['YEAR'].unique()\n```\n\n\n:::\n\n\n## Six Verbs for EDA \n\n     @tbl-compareDplyrPandas shows the comparable functions in both `dplyr` and `pandas` packages. These functions are very much important to perform exploratory data analysis in both `R` and `Python`. `group_by` (`groupby` in pandas) and `summarize ()`^[You can also use British spelling - `summarise ()`] (`agg ()` in pandas) are often used together; therefore, they are in the same group in @tbl-compareDplyrPandas. \n\n```{r}\n#| include: false\ntidyverse_pandas = tibble::tribble(\n  ~`Verb Number`,~`tidyverse`, ~ `pandas`, \n  '1','filter ()', 'query () or loc () or iloc ()',\n  '2','arrange ()', 'sort_values ()',\n  '3','select ()', 'filter () or loc ()',\n  '4','rename ()', 'rename ()',\n  '5','mutate ()', 'assign ()',\n  '6','group_by ()', 'groupby ()',\n  '6','summarize ()', 'agg ()'\n)\n\n```\n\n```{r}\n#| label: tbl-compareDplyrPandas\n#| tbl-cap: Tidyverse and Pandas Equivalent Functions \n#| echo: false\n#| warning: false\n# These are R code to prepare Table 2 using KableExtra \nlibrary(kableExtra)\nkbl(tidyverse_pandas, booktabs = TRUE \n    #,caption = \"Tidyverse and Pandas Equivalent Functions\"\n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n### 1st Verb - filter () Function \n\n     Filter functions are used to subset a data frame based on rows, meaning that retaining rows that satisfy given conditions. Filtering rows is also called slicing^[Indexing involves obtaining individual elements.] becasue we obtain a set of elements by filtering.  \n\n:::{.panel-tabset}\n\n## dplyr \n\n```{r}\ndf |> filter (YEAR == \"2015\")\n```\n\n```{r}\ndf |> filter (COUNTRY %in% c(\"United States\", \"Canada\"))\n```\n\n```{r}\ndf |> filter (COUNTRY == \"United States\", YEAR == \"2016\")\n```\n\n```{r}\ndf |> filter (COUNTRY == \"United States\", YEAR %in% c(\"2015\",\"2016\"))\n```\n\n```{r}\ndf |> filter (COUNTRY %in% c(\"United States\", \"Canada\"), YEAR == \"2014\")\n```\n\n\n## pandas \n\n```{python}\ndf_pd.query(\"YEAR == 2015\")\n```\n\n```{python}\ndf_pd.query('COUNTRY== \"United States\" | COUNTRY == \"Canada\"')\n```\n\n```{python}\ndf_pd.query(\"COUNTRY in ['United States', 'Canada']\")\n```\n\n```{python}\ndf_pd.query(\"COUNTRY== 'United States' & YEAR== 2016\")\n```\n\n```{python}\ndf_pd.query(\"COUNTRY== 'United States' & YEAR in [2015,2016]\")\n```\n\n```{python}\ndf_pd[df_pd['COUNTRY'] == \"United States\"]\n```\n\n```{python}\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\")]\n```\n\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY'].isin([\"United States\", \"Canada\"])]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY']\\\n .isin([\"United States\", \"Canada\"]) &(df_pd['YEAR']==2014)]\n```\n\n```{python}\ndf_pd.loc[(df_pd['COUNTRY']==\"United States\") &(df_pd [\"YEAR\"] ==2014)]\n```\n\n```{python}\ndf_pd.loc[df_pd['COUNTRY'] == \"United States\", :]\n```\n\n```{python}\ndf_pd.loc[\n    df_pd['COUNTRY']=='United States',\n    ['COUNTRY', \"UNITPRICE\", \"SALEPRICE\"]]\n```\n\n:::\n\n### 2nd Verb - arrange () Function \n\n     In arrange functions, we order the rows of a data frame by the values of given columns. It is like sorting or odering the data. \n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |>\n    arrange(DATE)     \n```\n\n```{r}\ndf |>\n    arrange(desc(DATE))     \n```\n\n\n```{r}\ndf |>\n    arrange(MONTH, SALEPRICE)     \n```\n\n\n\n## pandas \n```{python}\ndf_pd.sort_values(by =['DATE'])   \n```\n\n```{python}\ndf_pd.sort_values(by =['DATE'], ascending = False)   \n```\n\n```{python}\ndf_pd.sort_values(by =['MONTH', 'SALEPRICE'])\n```\n\n\n:::\n\n\n### 3rd Verb - select () Function \n     Select functions help to select or obtain columns from the data frame. When there are a lot of columns in our dataset, select functions become very useful. \n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |> select(DATE, UNITPRICE, DISCOUNT)   \n```\n\n\n```{r}\ndf |> select(1:2, 5:8)  \n```\n\n```{r}\ndf |>\n    select(starts_with('SIZE'))\n```\n\n```{r}\ndf |>\n    select(ends_with('PRICE'))\n```\n\n\n```{r}\ndf |>\n    select(contains(\"_\"))\n```\n\n```{r}\ndf |>\n    select(matches(\"SIZE\"))\n```\n\n```{r}\ndf |>\n    select(matches(\"PRICE$\"))\n```\n\n```{r}\n# starts with letter S\ndf |>\n    select(matches(\"^S\"))\n```\n\n\n```{r}\ndf |>\n    select(where(is.character))\n```\n\n```{r}\ndf |>\n    select(where(is.numeric))\n```\n\n```{r}\ndf |>\n    select(MONTH, YEAR, everything())\n```\n\n```{r}\n# any_of () vs all_of ()\ndf |>\n    select(any_of(c(\"PRICE\", \"SIZE\")))\n```\n\n\n```{r}\n# Dropping columns \ndf |>\n    select(-DATE)\n```\n\n\n\n## pandas \n```{python}\ndf_pd['DATE']   \n```\n\n```{python}\ndf_pd[['DATE', 'UNITPRICE']]   \n```\n\n```{python}\ndf_pd.loc[:,['DATE', 'UNITPRICE']]   \n```\n\n\n```{python}\ndf_pd.iloc[:,5:8]\n```\n\n```{python}\ndf_pd.iloc[:,[3,5,8]]\n```\n\n```{python}\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n```\n\n```{python}\ndf_pd.filter(['YEAR','SALEPRICE', 'DISCOUNT', 'UNITPRICE'])\n```\n\n```{python}\n #RegularExpression(Regex)\ndf_pd.filter(regex =\"PRICE$\") #Ends with Price\n```\n\n```{python}\ndf_pd.filter(regex =\"ˆSIZE\") #Starts with SIZE\n```\n\n\n```{python}\ndf_pd.filter(regex =\"PRICE\") #Contains the word Price\n```\n\n```{python}\ndf_pd.select_dtypes('object')\n```\n\n```{python}\ndf_pd.select_dtypes('int')\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.startswith('SIZE')]\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.contains('PRICE')]\n```\n\n```{python}\ndf_pd.loc[:,df_pd.columns.str.endswith('PRICE')]\n```\n\n\n```{python}\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\n```\n\n```{python}\n# Dropping columns \ndf_pd.drop(columns =['SIZE_EUROPE', 'SIZE_UK'], axis=1)\\\n    .pipe(lambda x: x.info())\n```\n\n```{python}\n# Rearranging columns \n# Sorting Alphabetically\ndf_pd.reindex(sorted(df_pd.columns), axis =1)\n```\n\n```{python}\n# Rearranging columns \n# Sorting As You Want (ASY)\ncol_first = ['YEAR','MONTH']\ncol_rest = df_pd.columns.difference(col_first, sort=False).to_list()\ndf_pd2 = df_pd [col_first +col_rest]\ndf_pd2.info()\n```\n\n\n:::\n\n\n### 4th Verb - rename () Function \n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |>\n    rename(INVOICE = INVOICENO,\n    PRODUCT = PRODUCTID) |>\n    glimpse()\n     \n```\n\n## pandas \n```{python}\n(df_pd.rename(columns = {\"PRODUCTID\": \"PRODUCT\", \"INVOICENO\": \"INVOICE\"})\n     .pipe(lambda x: x.info())\n)\n```\n\n\n:::\n\n### 5th Verb - mutate () Function \n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |>\n    mutate(NECOLUMN = 5,\n    SALESPRICE2 = UNITPRICE*(1-DISCOUNT)) |>\n    glimpse()\n     \n```\n\n## pandas \n```{python}\ndf_pd['NEWCOLUMN'] = 5\ndf_pd.info()     \n```\n\n```{python}\ndf_pd.drop(columns = ['NEWCOLUMN'], axis = 1, inplace = True)   \ndf_pd.info() \n```\n\n```{python}\ndf_pd['SALEPRICE2']=df_pd['UNITPRICE']*(1-df_pd['DISCOUNT'])\ndf_pd.info()\n```\n\n\n```{python}\n# Using the assign() function\n(df_pd[['PRODUCTID', 'UNITPRICE', 'DISCOUNT']]\\\n    .assign(SALEPRICE3 =lambda x: x.UNITPRICE*(1-x.DISCOUNT)) \\\n    .head(5)\n)\n```\n\n\n:::\n\n### 6th Verbs - group_by () and summarize () Functions \n\n     @fig-splitapplycombine presents Split Apply Combine principle in `group_by ()` and `summarize ()` functions. \n\n![Split Apply Combine Principle](images/split-apply-combine.png){#fig-splitapplycombine}\n\n\n:::{.panel-tabset}\n\n## dplyr \n```{r}\ndf |>\n    group_by(COUNTRY) |>\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE))\n     \n```\n\n```{r}\ndf |>\n    group_by(COUNTRY) |>\n    summarize (AVGPRICE = mean(UNITPRICE, na.rm = TRUE),\n    AVGSALEPRICE = mean (SALEPRICE, na.rm = TRUE))\n     \n```\n\n\n```{r}\n# Summary Statistics \ndf %>%\n  select(UNITPRICE, SALEPRICE) %>%\n  summarize(across(where(is.numeric), \n                   .fns = list(N = ~length(.),\n                               Mean = mean, \n                               Std = sd,\n                               Median = median, \n                               P25 = ~quantile(.,0.25), \n                               P75 = ~quantile(., 0.75)\n                               )\n                   )) %>%\n  pivot_longer(everything(), names_sep='_', names_to=c('variable', '.value'))  \n```\n\n\n\n## pandas \n```{python}\ndf_pd.groupby(['COUNTRY'])['UNITPRICE'].mean()\n     \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY'])[['UNITPRICE', 'SALEPRICE']].mean()\n     \n```\n\n\n```{python}\n#| warning: false\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(np.mean) \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(\"mean\") \n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"))\n```\n\n```{python}\ndf_pd.groupby(['COUNTRY']) [['UNITPRICE', 'SALEPRICE']] \\\n    .agg(AVG_UNITPRICE =(\"UNITPRICE\",\"mean\"),\n    AVG_LISTPRICE =(\"SALEPRICE\",\"mean\"),\n    TOTALN=(\"SALEPRICE\",\"size\"), # size function for n\n    TOTALOBS=(\"SALEPRICE\",\"count\") # count function for n\n )\n```\n\n```{python}\n# Defining a Function\ndef percentile(n):\n    def percentile_(x):\n        return x.quantile(n)\n    percentile_.__name__ ='percentile_{:02.0f}'.format(n*100)\n    return percentile_\n\n# Some summary statistics \ndf_pd[['UNITPRICE', 'SALEPRICE']] \\\n    .agg(['count', 'mean', 'std', 'median', percentile(0.25), percentile (0.75)]) \\\n    .transpose() \\\n    .reset_index() \\\n    .rename(columns = {\"index\": \"variables\", \"percentile_25\": \"P25\", \"percentile_75\": \"P75\", 'count': \"N\"}) \\\n    .round(3) \n```\n\n```{python}\n#| eval: false\n# Summary Statistics \nagg_dict = {\n    \"N\": \"count\",\n    'Mean':\"mean\",\n    \"Std. Dev\" : \"std\",\n    'P25': lambda x: x.quantile(0.25),\n    'Median': 'median',\n    'p75': lambda x: x.quantile(0.75)\n}\n\ndf_pd[['UNITPRICE', 'SALEPRICE']].agg(agg_dict)\n```\n\n\n:::\n\n## Reshaping Data \n\n::: {style=\"text-align: justify\"}\n     Before we discuss about reshaping of the data, we need to discuss about tidy format of the data. Data can come in many shapes, but not all shapes are useful for data analysis. In most cases, tidy format of the data is most useful for analysis. Therefore, if the data is untidy, we need to make it tidy first. There are three interrelated rules which make a dataset tidy [@wickham2023r]. These rules are given below. @fig-tidyprinciple visually represents tidy principle. \n\n1. Each variable must have its own column\n\n2. Each observation must have its own row\n\n3. Each value must have its own cell \n\n![Tidy Principle](images/tidy-1.png){#fig-tidyprinciple}\n\n     For analysis, many times we need to change the format of our dataset and we call it reshaping. Data come primarily in two shapes -*wide* and *long*. Sometimes *wide* format is called \"record\" format and *long* format is called \"stacked\" format. In `wide` format data, there is one row for each subject (units of observation). Data is `long` when there are multiple rows for each subject (units of observations). \n\n     This reshaping can be two types - a) long to wide and 2) wide to long. Long-to-wide means reshaping a long data, which has many rows, into wide format, which has many variables. In wide-to-long format, we do otherwise. For analytical purpose, reshaping data is useful; so, we need to know how to do the reshaping. \n\n     Whether a given dataset (e.g., @tbl-formatofdata) is in wide or long format depends on our research questions (on what variables we are interested in and how we conceive of our data). If we are interested in variable `Temp` and `Month` variable is the unit of obsevation, then the dataset in @tbl-formatofdata is in `long` format because `Month` is repeated in mutiple rows. \n\n:::\n\n```{r}\n#| warning: false\nairquality = airquality\n\nexamp = airquality |>\n    slice(1:10)\n```\n\n\n```{r}\n#| label: tbl-formatofdata\n#| tbl-cap: Which Format - Long or Wide \n#| echo: false\n#| warning: false\n\nkbl(examp, booktabs = TRUE \n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n\n### Long-to-Wide Format  \n\n::: {style=\"text-align: justify\"}\n     To make a long dataset to wide, we can use `pivot_wider()` function from `tidyr` package in R and `pivot()` function from `pandas` in python. \n:::\n\n\n:::{.panel-tabset}\n\n## tidyr\n```{r}\ntidyr::us_rent_income\n```\n\n```{r}\ntidyr::us_rent_income |>\n    pivot_wider(\n        names_from = variable, \n        values_from = c(estimate, moe)\n    )\n\n```\n\n\n## pandas \n\n```{python}\n#| warning: false\n# install palmerpenguins package\n# pip install palmerpenguins\nimport palmerpenguins\npenguins = palmerpenguins.load_penguins()\n```\n\n```{python}\npenguins[[\"island\", \"bill_length_mm\"]] \\\n    .pivot(columns = \"island\", values = \"bill_length_mm\") \\\n    .fillna(0)\n```\n\n\n:::\n\n\n### Wide-to-Long Format  \n\n::: {style=\"text-align: justify\"}\n     To make a wide dataset to long, we can use `pivot_longer()` function from `tidyr` package in R and `melt()` function from `pandas` in python.\n:::\n\n\n:::{.panel-tabset}\n\n## tidyr\n```{r}\nrelig_income\n```\n\n```{r}\nrelig_income |>\n    pivot_longer(\n        cols = !c(religion),\n        names_to = \"income\",\n        values_to = \"count\"\n    )\n```\n\n## pandas \n\n```{python}\npenguins.melt(value_vars=[\"bill_length_mm\", \"bill_depth_mm\",\"flipper_length_mm\", \"body_mass_g\"],\n              id_vars = ['species', 'island', 'sex', 'year']\n              )\n```\n\n:::\n\n\n## Merging Datasets \n\n::: {style=\"text-align: justify\"}\n     Many times, for analysis purposes, we need to join two datasets. This process is also called merging^[In database context, it is \"merging\", but commonly it is called \"joining\".]. There are different types of joining. So, it is important to learn about those joining techniques. In @fig-joindatasets shows the joining technique and functions using `dplyr` in R. Below all of these joining functions are explained. \n\n* `left_join()`: The merged dataset contains **all** observations fromthe **first** (or left) dataset and only **matched** observations from the **second** (or right) dataset\n\n* `right_join()`: The merged dataset contains only **matched** observations from the **first** (or left) dataset and **all** observations from the **second** (or right) dataset\n\n* `inner_join()`: The merged dataset contains *only* **matched** observations from both datasets\n\n* `semi_join()`: The merged dataset contains **matched** observations from the **first** (or left) dataset. Please note that `semi_join()` differs from `inner_join()` in that `inner_join()` will return one row of first dataset (x) for each matching row of second dataset (y), whereas `semi_join()` will never duplicate rows of x.   \n\n* `full_join()`: The merged dataset contains **all** observations from both datasets\n\n* `anti_join()`: The merged dataset contains only **not matched** observations from the **first** (or left) dataset and contains only the variable from the **left** dataset\n\n![Joining Datasets](images/join_diagram.png){#fig-joindatasets width=\"40%\"}\n\n```{r}\n#| include: false\ndf_join <- tribble(\n  ~dplyr, ~pandas, ~Description,\n  \"left_join()\", \"pd.merge(df1, df2, on='key', how='left')\", \"Join matching rows from df2 to df1, keeping all rows from df1.\",\n  \"right_join()\", \"pd.merge(df1, df2, on='key', how='right')\", \"Join matching rows from df1 to df2, keeping all rows from df2.\",\n  \"inner_join()\", \"pd.merge(df1, df2, on='key', how='inner')\", \"Join matching rows from both dataframes (default behavior of merge()).\",\n  \"full_join()\", \"pd.merge(df1, df2, on='key', how='outer')\", \"Join all rows from both dataframes, filling missing values with NaN.\",\n  \"semi_join()\", \"No direct equivalent, but can be achieved using filtering\", \"Keep rows in df1 where a match exists in df2.\",\n  \"anti_join()\", \"No direct equivalent, but can be achieved using filtering\", \"Keep rows in df1 where no match exists in df2.\"\n)\n```\n\n     @tbl-joinRPython compares the `dplyr` joining functions with equivalent joining functions from `pandas`.\n\n```{r}\n#| label: tbl-joinRPython\n#| tbl-cap: Joining Functions - dplyr vs pandas  \n#| echo: false\n#| warning: false\n\nkbl(df_join, booktabs = TRUE \n    ) %>% \n  kable_styling(latex_options = c ('striped', 'hold_positions'))\n```\n\n\n:::\n\n:::{.panel-tabset}\n\n## dplyr\n\n```{r}\n#| warning: false\ndata1 = read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\nglimpse(data1)\n```\n\n```{r}\n#| warning: false\ndata2 = read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\")\nglimpse(data2)\n```\n\n```{r}\n# left_join\nleft_join(data1, data2, by = c(\"ticker\", \"year\"))\n```\n\n```{r}\n# left_join\nleft_join(data1 |> distinct(ticker, year, .keep_all = TRUE), \n          data2 |> distinct(ticker, year, .keep_all = TRUE), \n          by = c(\"ticker\", \"year\")\n          )\n```\n\n\n## pandas \n\n```{python}\n#| warning: false\ndataset1 = pd.read_csv(\"https://raw.githubusercontent.com/msharifbd/DATA/refs/heads/main/DATA.csv\")\ndataset2 = pd.read_csv(\"https://github.com/msharifbd/DATA/raw/refs/heads/main/DATA2.csv\",encoding=\"latin-1\")\n```\n\n```{python}\npd.merge(dataset1, dataset2, on=['ticker', 'year'], how='left')\n```\n\n```{python}\n#| warning: false\ndataset1_drop = dataset1.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\ndataset2_drop = dataset2.drop_duplicates(subset=['ticker', 'year'], ignore_index = True)\npd.merge(dataset1_drop, dataset2_drop, on=['ticker', 'year'], how='left')\n```\n\n```{python}\ndf1 = pd.DataFrame({'id': [1, 2, 3], 'value': ['A', 'B', 'C']})\ndf2 = pd.DataFrame({'id': [2, 3, 4], 'other_value': ['X', 'Y', 'Z']})\n\n# Left join\nleft_join = pd.merge(df1, df2, on='id', how='left')\n\n# Inner join\ninner_join = pd.merge(df1, df2, on='id')\n\n# Semi join\nsemi_join = df1[df1['id'].isin(df2['id'])]\n\n# Anti join\nanti_join = df1[~df1['id'].isin(df2['id'])]\n```\n\n:::\n\n## Conclusions \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"eda.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","jupyter":"python3","number-depth":5,"site-url":"https://analyticsforaccountingdata.netlify.app/","repo-url":"https://github.com/msharifbd/sharifanalytics","repo-actions":["edit","issue"],"repo-branch":"main","sharing":["twitter","facebook","linkedin"],"downloads":["pdf","epub"],"bibliography":["references.bib"],"editor":"visual","theme":"flatly","mainfont":"Georgia, serif","fig-cap-location":"bottom","callout-appearance":"default","cover-image":"images/titlepage_1_alt.webp","title":"Exploratory Data Analysis (EDA)"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"pdf-book"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"jupyter":"python3","number-depth":5,"site-url":"https://analyticsforaccountingdata.netlify.app/","repo-url":"https://github.com/msharifbd/sharifanalytics","repo-actions":["edit","issue"],"repo-branch":"main","sharing":["twitter","facebook","linkedin"],"downloads":["pdf","epub"],"bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt","mainfont":"Times New Roman","sansfont":"Arial","monofont":"Courier New","colorlinks":true,"title":"Exploratory Data Analysis (EDA)"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}