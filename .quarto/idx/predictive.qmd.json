{"title":"Predictive Modeling - Linear Regression","markdown":{"yaml":{"title":"Predictive Modeling - Linear Regression","format":"html"},"headingText":"Learning Objectives of the Chapter","headingAttr":{"id":"","classes":["unnumbered"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\n::: {style=\"text-align: justify\"}\nAt the End of the Chapter, Students should be Able to -\n\n-   Learn about Linear Regression\n\n-   Identify Linear Relation between Variables\n\n-   Build Linear Regression Model between Variables\n\n-   Learn about How to Evaluate the Fitness of Linear Models\n\n-   Evaluate the Assumptions of Linear Regressions\n\n-   Build Multiple Regression Models and Evaluate them\n:::\n\n::: {style=\"text-align: justify\"}\n## Introduction\n\n     Discussion on predictive analytics.\n:::\n\n::: {style=\"text-align: justify\"}\n## Regression Analysis\n\n### Simple Linear Regression\n\n     Regression analysis is one of very useful predictive modeling techniques that identify the relationship between two or more variables. The objective of linear regression is to identify a linear line of best fit that can predict the outcome variable (target variable/dependent variable/response variable) for one or more independent variables(predictors). For example, we can draw a scatter diagram to see the relation between horsepower and miles per gallon (MPG). @fig-r-hrspower-mpg and @fig-py-hrspower-mpg show the relationship between horsepower and MPG in R and Python respectively. It is clear that there exists a negative relationship between horsepower and MPG and it makes sense because more power means higher fuel consumption. Similarly, there is negative relationship between vehicle weight and MPG because heavier vehicles need more energy to move (@fig-r-weight-mpg and @fig-py-weight-mpg). The equation of a simple linear regression is - $$y= mX + C$$\n\n     where $y$ = Target variable (Dependent variable); $m$ = slope or rate of change; $X$ = predictor or independent variable; and $C$ = intercept or constant.\n:::\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false\n# loading necessary packages \nlibrary(tidyverse)\n```\n\n```{r}\n#| warning: false\n# loading the dataset \nr_df = read_csv('http://web.pdx.edu/~gerbing/data/cars.csv')\nglimpse (r_df)\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\nSys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n#Sys.setenv('RETICULATE_PYTHON' = '~/.venv/quarto_book_python/Scripts/python.exe')\n```\n\n```{python}\n# loading necessary modules \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n```\n\n```{python}\n# loading the dataset \npy_df = pd.read_csv(\"http://web.pdx.edu/~gerbing/data/cars.csv\")\npy_df.info()\n```\n:::\n\n::: panel-tabset\n## R\n\n```{r}\n#| fig-cap: \"Relationship between Horsepower and MPG\"\n#| label: fig-r-hrspower-mpg\nggplot(r_df, aes(x = Horsepower, y = MPG))+\n    geom_point()\n    \n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n```\n\n```{python}\n#| fig-cap: \"Relationship between Horsepower and MPG\"\n#| label: fig-py-hrspower-mpg\nsns.set_theme(style=\"darkgrid\")\nsns.scatterplot(x = \"Horsepower\", y = \"MPG\", data = py_df)\n\n```\n:::\n\n::: panel-tabset\n## R\n\n```{r}\n#| fig-cap: \"Relationship between Weight and MPG\"\n#| label: fig-r-weight-mpg\nggplot(r_df, aes(x = Weight, y = MPG))+\n    geom_point()\n    \n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n```\n\n```{python}\n#| fig-cap: \"Relationship between Weight and MPG\"\n#| label: fig-py-weight-mpg\nsns.set_theme(style=\"darkgrid\")\nsns.scatterplot(x = \"Weight\", y = \"MPG\", data = py_df)\n\n```\n:::\n\n::: {style=\"text-align: justify\"}\n     Now we can quantify these relationships using linear regression in which we will try to draw a line that will help us to idenfity the relation between the variables. For example, in @fig-r-reg-equation, we can see the linear line that shows the relation between horsepower and MPG. The regression equation we find is `y = 40-0.16x`, where $y$ is MPG, $x$ is Horsepower, $40$ is intercept, and $1.6$ is slope, which means one unit change of Horsepower results in 1.6 units reduction of MPG and the value of MPG is 40 when horsepower is 0. @fig-py-reg-equation shows the same results.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false\n#| fig-cap: Regression of Horsepower and MPG\n#| label: fig-r-reg-equation\nlibrary(ggpubr)\nggplot(r_df, aes (x = Horsepower, y = MPG))+\n    geom_point()+\n    geom_smooth(method = \"lm\")+\n    stat_regline_equation(label.x = 125, label.y = 40)  # to add reg equation\n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n```\n\n```{python}\n#| label: fig-py-reg-equation\n#| fig-cap: Regression of Horsepower and MPG\n#| warning: false\n\nimport plotnine as p9\nfrom scipy import stats\n#calculate best fit line\nslope, intercept, r_value, p_value, std_err = stats.linregress(py_df['Horsepower'],py_df['MPG'])\npy_df['fit']=py_df.Horsepower*slope+intercept\n#format text \ntxt= 'y = {:.4f} x + {:.4f}'.format(round(slope,2), round(intercept,2))\n#create plot.\nplot=(p9.ggplot(data=py_df, mapping= p9.aes('Horsepower','MPG'))\n    + p9.geom_point(p9.aes())\n    + p9.xlab('Horsepower')+ p9.ylab(r'MPG')\n    + p9.geom_line(p9.aes(x='Horsepower', y='fit'), color='blue')\n    + p9.annotate('text', x= 150, y = 40, label = txt))\n# print the plot\nprint(plot)\n```\n:::\n:::\n\n#### Relationship between Stock Returns and a Market Index\n\n::: {style=\"text-align: justify\"}\n     Now we will try to build a simple linear model between the return of a stock and a market index. Using `tidyquant` package from `R` or `yfinance` module from `python`, we will get share prices of different stocks and a market index. For example, we will get the share price data of Coca-Cola (NYSE:KO) and its competitor PepsiCO (NYSE:PEP). Moreover, we will collect data on US Dollar index (ICE:DX) and SPDR S&P500 ETF (NYSERCA:SPY).\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: fig-r-relation-KO-SPY\n#| fig-cap: Linear Model between SPY and KO\n#| warning: false\nlibrary(tidyquant)\n\ntickers = c(\"KO\", \"SPY\", \"PEP\",\"DX-Y.NYB\")\nprices <- tq_get(tickers,\n                 from = \"2019-01-01\",\n                 to = \"2023-12-31\"\n                 #,get = \"stock.prices\"\n                 )\n\ndf_stockR = prices |>\n    select(symbol, date, adjusted) |>\n    drop_na(adjusted) |>\n    pivot_wider(id_cols = date,names_from = symbol, \n    values_from = adjusted)\n\n\ndf_stockR = as_tibble(na.omit(CalculateReturns(df_stockR, method = \"log\")))\n\n# Linear Regression \nlibrary(fixest)\n# feols(KO ~ SPY, data = df_stockR)\nsummary(lm(KO ~ SPY, data = df_stockR))\n\nmodel = lm(KO ~ SPY, data = df_stockR)\n# adding fitted value to the dataset \ndf_stockR$fitted = fitted(model)\n# adding residual to the dataset \ndf_stockR$residual = resid(model)\n\n# We can use broom package to simplify all of these processes \nlibrary(broom)\ndf_stockR2 = df_stockR  |>\n    select(KO:DX.Y.NYB)\n\nmodel2 = lm(KO ~ SPY, data = df_stockR2) \n\ntidy(model2) # summary of model components \nglance(model2) # information about model fitness \naugment(model2, data = df_stockR2) # adds info to the dataset \n\n# Visualization \nlibrary(ggpubr)\nggplot(df_stockR, aes (x = SPY, y = KO))+\n    geom_point()+\n    geom_smooth(method = \"lm\")+\n    labs(x = \"SPY Returns\", y = \"KO Returns\") + \n    stat_regline_equation(label.x = -0.05, label.y = 0.05)  # to add reg equation\n\n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| label: fig-py-relation-KO-SPY\n#| fig-cap: Linear Model between SPY and KO\nimport yfinance as yf\nimport datetime\nimport numpy as np\n\nimport plotnine as p9\nfrom scipy import stats\n## To use statsmodels for linear regression\nimport statsmodels.api as sm \nimport statsmodels.formula.api as smf\n## To use sklearn for linear regression\nfrom sklearn.linear_model import LinearRegression\n\n#########################################################\n# Importing stock data \n## 5-year daily data for Coca-Cola, SPY, Pepsi, and USD index\n#########################################################\nend = datetime.date(2023, 12, 31)\nstart = end - pd.Timedelta(days = 365 * 5) \n# Getting the data \nko_df = yf.download(\"KO\", start = start, end = end, progress = False) # CocaCola \nspy_df = yf.download(\"SPY\", start = start, end = end, progress = False) # SPDR S&P 500\npep_df = yf.download(\"PEP\", start = start, end = end, progress = False) # PepsiCo\nusdx_df = yf.download(\"DX-Y.NYB\", start = start, end = end, progress = False) # US Dollar Index\n\n## Calculate log returns for the period based on Adj Close prices\n\nko_df['ko'] = np.log(ko_df['Adj Close'] / ko_df['Adj Close'].shift(1))\nspy_df['spy'] = np.log(spy_df['Adj Close'] / spy_df['Adj Close'].shift(1))\npep_df['pep'] = np.log(pep_df['Adj Close'] / pep_df['Adj Close'].shift(1))\nusdx_df['usdx'] = np.log(usdx_df['Adj Close'] / usdx_df['Adj Close'].shift(1))\n\n## Create a dataframe with X's (spy, pep, usdx) and Y (ko)\n\ndf_stock = pd.concat([spy_df['spy'], ko_df['ko'], \n                pep_df['pep'], usdx_df['usdx']], axis = 1).dropna()\n\n####################################################\n## 2a. Fit a simple linear regression model to the data using statsmodels \n\n### Create an instance of the class OLS\nslr_sm_model = smf.ols('ko ~ spy', data=df_stock)\n\n### Fit the model (statsmodels calculates beta_0 and beta_1 here)\nslr_sm_model_ko = slr_sm_model.fit()\n\n### Summarize the model\n\nprint(slr_sm_model_ko.summary()) \n\n## Adding the fitted values to the dataframe \ndf_stock['fitted'] = slr_sm_model_ko.fittedvalues\n\n## Adding the residuals to the dataframe \ndf_stock['residual'] = slr_sm_model_ko.resid\n\n\nparam_slr = slr_sm_model_ko.params\n\n## Linear regression plot of X (spy) and Y (ko)\nplt.figure(figsize = (10, 6))\nplt.rcParams.update({'font.size': 14})\nplt.xlabel(\"SPY Returns\")\nplt.ylabel(\"KO Returns\")\n#plt.title(\"Simple linear regression model\")\nplt.scatter(df_stock['spy'],df_stock['ko'])\nplt.plot(df_stock['spy'], param_slr.Intercept+param_slr.spy * df_stock['spy'],\n         label='Y={:.4f}+{:.4f}X'.format(param_slr.Intercept, param_slr.spy), \n         color='red')\nplt.legend()\nplt.show()\n\n```\n:::\n:::\n\n### Evaluating Linear Regression Models - Assessing the Fitness\n\n::: {style=\"text-align: justify\"}\n     When we build a linear regression model, we need to check the accuracy of the model by evaluating different parameters of the model. Some of the parameters that need to be evaluated are discussed below -\n\n#### F-statistic and Overall P-value\n\n     The F-statistic of the model tests whether the linear model provides a better fit of the data than the model that contains no independent variables. The null hypothesis is that the model as a whole does not explain a significant amount of variance of the data.\n\n#### Coefficient of Determination ($R^2$) and Adjusted $R^2$\n\n     Coefficient of determination is a statistical metric that measures the proportion of the variance of the target variable explained by the independent variables. The coefficient of determination is represented by $R^2$. The value of $R^2$ ranges between $0$ and $1$, where $0$ indicates poor fit and $1$ indicates perfect fit. There is a problem with $R^2$; it increases as the number of independent variable increases. Therefore, when we compare models with different numbers of independent variables, adjuted $R^2$ is used to evaluate the model because it penalizes models with a large number of predictors.\n\n#### P-value of Parameter Estimates\n\n     In addition to overall p-value associated with the model, linear regression models generate p-value for each predicators (parameter estimates). This p-value tests the null hypothesis that the coefficient is zero (or it does not have any effect on outcome variable). A low p-value (\\<0.05) indicates that we can reject the null hypothesis, meaning that a predictor with a low p-value should be included in the model because it has statistically significant effect on the target variable.\n\n#### Residual Standard Error (RSE)\n\n     Residual Standard Error (RSE) measures how far away an observation is from the prediction (regression line). In another words, it is mean (average) distance between the actual outcome and regression line. For example, @fig-r-relation-KO-SPY regression equation's RSE is 0.01034, which means the regression model predicts CocaCola's return with an avearagr error 0.01034.\n:::\n\n::: {style=\"text-align: justify\"}\n### Assumptions of Linear Regression\n\n     Linear regression follows some assumptions. After fitting a regression model, we should check the assumptions of the model.\n\n1.  **Linearity (Linear Relationship)**:\n\n     There must be a linear relationship between the outcome variable (y) and predictors (x). @fig-assump1 shows what we want to see and @fig-r-assump1 and @fig-py-assump1 show what we see from one of our models.\n\n![Assumption 1- What We Want to See](images/Linear_Reg_Assump1.webp){#fig-assump1 fig-align=\"center\"}\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: fig-r-assump1\n#| fig-cap: Testing Assumption 1\nplot(lm(KO ~ SPY, data = df_stockR), 1)\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| label: fig-py-assump1\n#| fig-cap: Testing Assumption 1\nsns.residplot(x = 'fitted', y = 'ko', data=df_stock, \nlowess=True,\nline_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\n# Diagnostic Plots in Python similar to R - \n# https://towardsdatascience.com/going-from-r-to-python-linear-regression-diagnostic-plots-144d1c4aa5a\n# https://robert-alvarez.github.io/2018-06-04-diagnostic_plots/\n# https://www.kirenz.com/blog/posts/2021-11-14-linear-regression-diagnostics-in-python/\n```\n:::\n\n2.  **Independence**:\n\n     Indpendence means that each observation (data point) is independent of the others, meaning that the error terms in the model are not correlated with each other and the occurrence of one observation does not influence the probability of another observation occurring; essentially, each data point should be considered a separate, unrelated event from the others. We can check this assumption by Durbin-Watson test. The null hypothesis (H~0~) is there is no correlation among the residuals. Basically, the test detects autocorrelation in the residuals of a linear regression model. The test statistic is a value between 0 and 4, with the following interpretations: 2: No autocorrelation; \\< 2: Positive autocorrelation; \\> 2: Negative autocorrelation. To solve the independence issue, it is suggested that for positive serial correlation, consider adding lags of the dependent and/or independent variable to the model. For negative serial correlation, check to make sure that none of your variables are overdifferenced, and For seasonal correlation, consider adding seasonal dummy variables to the model. However, as a rule of thumb, test statistic values between the range of 1.5 and 2.5 are considered normal. One limitation of the DW test is that it can only test for first-order serial correlation.\n\n     Alternatively, we can can visually examine a scatterplot of the residuals (errors) against the fitted values (@fig-r-assump1 and @fig-py-assump1); if there is no clear pattern, it suggests that the independence assumption is likely satisfied. Independence assmption is important because violating it can lead to unreliable hypothesis tests and confidence intervals.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false \nlibrary(car)\ndurbinWatsonTest(lm(KO ~ SPY, data = df_stockR))\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\nfrom statsmodels.stats.stattools import durbin_watson\ndurbin_watson(slr_sm_model_ko.resid)\n```\n:::\n\n3.  **Residual Error**:\n\n     The errors have a mean (expected) value zero ($0$), constant varianace (Homoscedasticity), and are independent from each other (otherwise auto-correlation problem) and predictor variables. We can check the assumption - expected value of errors are equal to 0 - by looking at @fig-r-assump1 and @fig-py-assump1. If there is a pattern in the plot, the assumption is violated.\n\n     The assumption homoscedasticity is tested using the Scale-Location plot, in which fitted values are compared with square root of standardized residuals. Theoretically, we want to see @fig-assump3. @fig-r-assump3 and @fig-py-assump3 show Scale-Location plot (also called spread-location plot) of our model. From @fig-r-assump3 and @fig-py-assump3, it is clear that residual plots have a trend (are not all equally spread out). Thus, the assumption is violated. One solution to the problem is that using log or square root transformation of outcome variable. Moreover, we can use Non-Constant Error Variance (NVC) test to test the assumption. If p-value of the test is less than 0.05, then null hypothesis is rejected, meaning that homoscedasticity is violated. Moreover, Breusch-Pagan test or White test can also be used to test homoscedasticity.\n\n     Finally, testing the assumption - errors are independent from each other and predictors - requires the knowledge of study design and data collection to establish the validity of this assumption. Violation of this assumption is also called endogeneity problem of the model, which is the same as assumption 6 - No Endogeneity below.\n\n![Assumption 3- What We Want to See](images/Linear_Reg_Assump3.webp){#fig-assump3 fig-align=\"center\"}\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false \n#| label: fig-r-assump3\n#| fig-cap: Testing Assumption 3 \n# Scale-Location Plot \nplot(lm(KO ~ SPY, data = df_stockR),3)\n# Non-Constant Error Variance Test \nncvTest (lm(KO ~ SPY, data = df_stockR))\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| warning: false \n#| label: fig-py-assump3\n#| fig-cap: Testing Assumption 3 \n\n# Scale-Location Plot \nsns.regplot(\n    x = slr_sm_model_ko.fittedvalues, \n    y = np.sqrt(np.abs(slr_sm_model_ko.get_influence().resid_studentized_internal)),\n    scatter=True, \n    ci= False, \n    lowess=True,\n    line_kws={'color': 'blue', 'lw': 1, 'alpha': 0.8} \n)\nplt.title('Scale-Location', fontsize=10)\nplt.xlabel('Fitted Values', fontsize=15)\nplt.ylabel('$\\sqrt{|Standardized Residuals|}$', fontsize=15)\n\n# Non-Constant Error Variance Test \nimport statsmodels.stats.api as sms\n## Breusch-Pagan test \nbp_test = sms.het_white(slr_sm_model_ko.resid, slr_sm_model_ko.model.exog)\nlabels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']\nfor stat, value in zip(labels,bp_test):\n    print('{}: {:.3f}'.format(stat, value))\n\n## White Test \nwhite_test = sms.het_white(slr_sm_model_ko.resid, slr_sm_model_ko.model.exog)\nfor stat, value in zip(labels,white_test):\n    print('{}: {:.3f}'.format(stat, value))\n```\n:::\n\n4.  **Normality**:\n\n     The residuals are normally distributed. We can test this assumption by Q-Q plots (also called Quantile-Quantile plot) of the residuals. @fig-r-qqplot and @fig-py-qqplot show a QQ plot. The dots do not lie perfectly along the straight line. Moreover, Kolmogorov-Smirnov (KS) test can be used to check the normality assumption. KS tests whether a sample comes from a certain distribution. From the KS result below, we can see test statistic is 0.484 and p-value is 2.2e-16, thus rejecting null hypothesis and indicating that sample data does not come from a normal distribution.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false \n#| label: fig-r-qqplot\n#| fig-cap: Testing Assumption 4\n\n#######################################\n# Drawing QQ Plot\n########################################\n## qqnorm () function \n#qqnorm((df_stockR$residual))\n## qqline () function \n#qqline(df_stockR$residual)\n# qqPlot () function from car package \nqqPlot(df_stockR$residual) \n\n#########################################\n# Kolmogorov-Smirnov (KS) test\n##########################################\nlibrary(stats)\nks.test(df_stockR$residual, \"pnorm\")\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| warning: false\n#| label: fig-py-qqplot\n#| fig-cap: Testing Assumption 4\n#######################################\n# Drawing QQ Plot\n########################################\nsm.qqplot(slr_sm_model_ko.resid, fit=True, line=\"45\")\n\n#########################################\n# Kolmogorov-Smirnov (KS) test\n##########################################\nfrom scipy.stats import kstest\nks_test = kstest(slr_sm_model_ko.resid, \"norm\")\nlabel_ks = [\"KS Statistic\", \"P-value\"]\n\nfor stat, value in zip(label_ks, ks_test[:2]):\n    print('{}: {:.5f}'.format(stat, value))\n```\n:::\n\n5.  **Multicollinearity**:\n\n     There is little or no correlation between the predictor variables. It makes difficult to interpret the coefficients of the model. Variance Inflation Factor (VIF) can be used to test the multicollinearity. A value equal to or greater than 10 of VIF indicates multicollinearity in the data. We see the VIF of SPY and PEP is 1.84 and 1.84 respectively. Moreover, correlation matrix can also be used to identify the predictors that are highly correlated. @fig-r-corplot and @fig-py-corplot show a correlation plot of our stock returns data.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false\n#| label: fig-r-corplot\n#| fig-cap: Correlation Plot \n\n## Variance Inflation Factor (VIF)\nlibrary(car)\nvif(lm(KO ~ SPY+PEP, data = df_stockR))\n\n## Correlation Matrix and Correlation Plot \n# to get the function \nsource(\"http://www.sthda.com/upload/rquery_cormat.r\")\nrquery.cormat(df_stockR |> select(KO:DX.Y.NYB), type = c(\"lower\"), graph = TRUE)$r\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| warning: false\n#| label: fig-py-corplot\n#| fig-cap: Correlation Plot \n\n## Correlation Matrix and Correlation Plot \n### Correlation Matrix\npd.DataFrame(np.tril(df_stock[['usdx', 'spy', 'ko','pep']].corr()),\ncolumns = ['usdx', 'spy', 'ko','pep'], index = ['usdx', 'spy', 'ko','pep']).replace(0,'')\n### Correlation Plot\nmask = np.triu(np.ones_like(df_stock[['usdx', 'spy', 'ko','pep']].corr()))\nsns.heatmap(df_stock[['usdx', 'spy', 'ko','pep']].corr(), \nannot=True, cmap=\"YlGnBu\", \nmask=mask)\n\n## Variance Inflation Factor (VIF)\n#orrelaiton_coeff = np.corrcoef(df_stock[['ko','spy']], rowvar=False)\n#VIF = np.linalg.inv(correlaiton_coeff)\n#VIF.diagonal()\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(df_stock[['ko','spy']])\n\npd.Series([variance_inflation_factor(X.values, i) \n               for i in range(1,X.shape[1])], \n              index=X.columns[1:])\n\n```\n:::\n\n6.  **No Endogeneity**:\n\n     There is no relation between the errors and the independent variables.\n:::\n\n::: {style=\"text-align: justify\"}\n### Multiple Linear Regression\n\n      Multiple regression analysis estimates the relationship between an outcome variable and two or more independent variables. More specifically, multiple regression analysis helps to understand how the value of the dependent variable changes when one of the independent variables varies, while the other independent variables remain constant.\n\n     We use Grunfeld dataset for multiple regressions. The dataset contains investment data for 11 US firms. The variables include - `invest`, which is Gross investment in 1947 dollars; `value`, which is market value as of Dec 31 in 1947 dollars; `capital`, which is stock of plant and equipment in 1947 dollars; `firm`, which include 11 US firms (General Motors, US Steel, General Electric, Chrysler, Atlantic Refining, IBM, Union Oil, Westinghouse, Goodyear, Diamond Match, American Steel); and `year`, which is 1935-1954. Our multiple regeression model is @eq-multiple -\n\n$$\ninvest_{it} = \\beta_{0} + \\beta_{1}value_{i} + \\beta_{3}capital_{it} + \\alpha_{it} + \\delta_{t} + \\epsilon_{it}\n$$ {#eq-multiple}\n\nWhere\n\n$invest_{it}$ is the gross investment of firm $i$ in year $t$\n\n$value_{it}$ is the market value of assets of firm $i$ in year $t$\n\n$capital_{it}$ is the stock value of plant and equipment of firm $i$ in year $t$\n\n$alpha_{i}$ is the fixed effect for firm $i$ (capturing unobserved firm-specific factors that don’t vary over time)\n\n$delta_{t}$ is the fixed effect for year $t$ (capturing unobserved year-specific factors that are common to all firms in that year)\n\n$epsilon_{it}$ is the error term, which includes all other unobserved factors that affect investment but are not accounted for by the independent variables or the fixed effects.\n\n     In @tbl-r-multiple and @tbl-py-multiple, we generate some multiple regression models.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false\n#| label: tbl-r-multiple\n#| tbl-cap: Regression of Investment on Market Value and PPE\n\n#############################################################\n## Grunfeld Data from AER Package\n#############################################################\n# Grunfeld data contains investment data for 11 US firms. The variables \n# include - invest = \nlibrary(AER)\nlibrary(modelsummary)\nlibrary(kableExtra)\nlibrary(fixest)\ndata(\"Grunfeld\", package = \"AER\")\n\nmodels_r = list()\n\n# Model (1)\nmodels_r [['OLS_FYfe']] = feols(invest ~ value + capital | firm + year, data = Grunfeld)\n\n# Model (2)\nmodels_r [['OLS_FYfeC']] = feols(invest ~ value + capital | firm + year,\ncluster = ~firm,  \ndata = Grunfeld)\n\n# Model (3)\nmodels_r [['OLS_FYfeCtwo']] = feols(invest ~ value + capital | firm + year,\ncluster = ~firm+year,  \ndata = Grunfeld)\n\nrows = tribble(\n    ~term, ~'OLS_FYfe',~'OLS_FYfeC', ~'OLS_FYfeCtwo',\n    'Firm Fixed Effects', \"YES\", \"YES\", \"YES\",\n    \"Year Fixed Effects\", \"YES\", \"YES\", \"YES\"\n)\nattr(rows, 'position') = c (5,6)\n\n\n\nmodelsummary(models_r, fmt = 2,\nestimate = \"{estimate}{stars}\",\nstatistic = 'statistic'\n#,vcov = \"robust\" # robust clustering \n#,vcov = ~firm # clusting by firm\n#,vcov = vcovHAC\n,stars = c (\"*\" = 0.10, \"**\" = 0.05, \"***\" =  0.01)\n,coef_rename = c(\n    \"value\" = \"VALUE\",\n    \"capital\" = \"CAPITAL\"\n)\n#,gof_omit = 'DF|Deviance|AIC|BIC'\n,gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\")\n,add_rows = rows\n#,notes = list(\n#'Note: In all models both firm and year fixed effects are controlled and in models 2 and 3 starndard errors', \n#'are clustered by firm (one-way cluster) and firm and year (two-way cluster) respectively. The numbers in parentheses indicate t values.')\n#,title = \"Title of the Table\"\n,output = \"kableExtra\"\n) |>\n# The line below is for styling the table, not necessary for regression table\n#kableExtra::kbl() |>\nkable_styling(full_width = TRUE) |>\nrow_spec(c(6,9), extra_css = \"border-bottom: 1.25px solid\") |>\nfootnote('In all models both firm and year fixed effects are controlled and in models 2 and 3 starndard errors are clustered by firm (one-way cluster) and firm and year (two-way cluster) respectively. The numbers in parentheses indicate t values.')\n\n\n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n\n```\n\n```{python}\n#| warning: false\n#| label: tbl-py-multiple\n#| tbl-cap: Regression of Investment on Market Value and PPE\n\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.datasets import grunfeld\ndata = grunfeld.load_pandas().data\ndata.year = data.year.astype(np.int64)\n\n# Link for pyfixest - \n# https://py-econometrics.github.io/pyfixest/quickstart.html\nimport pyfixest as pf\nfrom pyfixest.estimation import feols\n\n# Model (1)\nfe_model1 = feols(\n  \"invest ~ value + capital | firm + year\", \n  data = data)\n#fe_model1.summary()\n#pf.etable(fe_model1)\n#fe_model1.tidy()\n#fe_model1.coefplot()\n\n\n# Model (2)\nfe_model2 = feols(\n  \"invest ~ value + capital | firm + year\",\n  vcov= {\"CRV1\": \"firm\"}, \n  data = data)\n\n# Model (3)\nfe_model3 = feols(\n  \"invest ~ value + capital | firm + year\",\n  vcov= {\"CRV1\": \"firm+year\"}, \n  data = data)\n\npf.etable([fe_model1, fe_model2, fe_model3]\n#, type = \"gt\"\n,coef_fmt= 'b \\n (t)'\n,signif_code= [0.01,0.05,0.10]\n)\n```\n:::\n:::\n\n## Time Series Analysis\n\n## Exercises\n\n1.  Calculate a linear model between PepsiCo stock returns and S&P 500 Market index (the Ticker of S&P 500 Index is `^GSPC`)","srcMarkdownNoYaml":"\n\n### Learning Objectives of the Chapter {.unnumbered}\n\n::: {style=\"text-align: justify\"}\nAt the End of the Chapter, Students should be Able to -\n\n-   Learn about Linear Regression\n\n-   Identify Linear Relation between Variables\n\n-   Build Linear Regression Model between Variables\n\n-   Learn about How to Evaluate the Fitness of Linear Models\n\n-   Evaluate the Assumptions of Linear Regressions\n\n-   Build Multiple Regression Models and Evaluate them\n:::\n\n::: {style=\"text-align: justify\"}\n## Introduction\n\n     Discussion on predictive analytics.\n:::\n\n::: {style=\"text-align: justify\"}\n## Regression Analysis\n\n### Simple Linear Regression\n\n     Regression analysis is one of very useful predictive modeling techniques that identify the relationship between two or more variables. The objective of linear regression is to identify a linear line of best fit that can predict the outcome variable (target variable/dependent variable/response variable) for one or more independent variables(predictors). For example, we can draw a scatter diagram to see the relation between horsepower and miles per gallon (MPG). @fig-r-hrspower-mpg and @fig-py-hrspower-mpg show the relationship between horsepower and MPG in R and Python respectively. It is clear that there exists a negative relationship between horsepower and MPG and it makes sense because more power means higher fuel consumption. Similarly, there is negative relationship between vehicle weight and MPG because heavier vehicles need more energy to move (@fig-r-weight-mpg and @fig-py-weight-mpg). The equation of a simple linear regression is - $$y= mX + C$$\n\n     where $y$ = Target variable (Dependent variable); $m$ = slope or rate of change; $X$ = predictor or independent variable; and $C$ = intercept or constant.\n:::\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false\n# loading necessary packages \nlibrary(tidyverse)\n```\n\n```{r}\n#| warning: false\n# loading the dataset \nr_df = read_csv('http://web.pdx.edu/~gerbing/data/cars.csv')\nglimpse (r_df)\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\nSys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n#Sys.setenv('RETICULATE_PYTHON' = '~/.venv/quarto_book_python/Scripts/python.exe')\n```\n\n```{python}\n# loading necessary modules \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n```\n\n```{python}\n# loading the dataset \npy_df = pd.read_csv(\"http://web.pdx.edu/~gerbing/data/cars.csv\")\npy_df.info()\n```\n:::\n\n::: panel-tabset\n## R\n\n```{r}\n#| fig-cap: \"Relationship between Horsepower and MPG\"\n#| label: fig-r-hrspower-mpg\nggplot(r_df, aes(x = Horsepower, y = MPG))+\n    geom_point()\n    \n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n```\n\n```{python}\n#| fig-cap: \"Relationship between Horsepower and MPG\"\n#| label: fig-py-hrspower-mpg\nsns.set_theme(style=\"darkgrid\")\nsns.scatterplot(x = \"Horsepower\", y = \"MPG\", data = py_df)\n\n```\n:::\n\n::: panel-tabset\n## R\n\n```{r}\n#| fig-cap: \"Relationship between Weight and MPG\"\n#| label: fig-r-weight-mpg\nggplot(r_df, aes(x = Weight, y = MPG))+\n    geom_point()\n    \n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n```\n\n```{python}\n#| fig-cap: \"Relationship between Weight and MPG\"\n#| label: fig-py-weight-mpg\nsns.set_theme(style=\"darkgrid\")\nsns.scatterplot(x = \"Weight\", y = \"MPG\", data = py_df)\n\n```\n:::\n\n::: {style=\"text-align: justify\"}\n     Now we can quantify these relationships using linear regression in which we will try to draw a line that will help us to idenfity the relation between the variables. For example, in @fig-r-reg-equation, we can see the linear line that shows the relation between horsepower and MPG. The regression equation we find is `y = 40-0.16x`, where $y$ is MPG, $x$ is Horsepower, $40$ is intercept, and $1.6$ is slope, which means one unit change of Horsepower results in 1.6 units reduction of MPG and the value of MPG is 40 when horsepower is 0. @fig-py-reg-equation shows the same results.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false\n#| fig-cap: Regression of Horsepower and MPG\n#| label: fig-r-reg-equation\nlibrary(ggpubr)\nggplot(r_df, aes (x = Horsepower, y = MPG))+\n    geom_point()+\n    geom_smooth(method = \"lm\")+\n    stat_regline_equation(label.x = 125, label.y = 40)  # to add reg equation\n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n```\n\n```{python}\n#| label: fig-py-reg-equation\n#| fig-cap: Regression of Horsepower and MPG\n#| warning: false\n\nimport plotnine as p9\nfrom scipy import stats\n#calculate best fit line\nslope, intercept, r_value, p_value, std_err = stats.linregress(py_df['Horsepower'],py_df['MPG'])\npy_df['fit']=py_df.Horsepower*slope+intercept\n#format text \ntxt= 'y = {:.4f} x + {:.4f}'.format(round(slope,2), round(intercept,2))\n#create plot.\nplot=(p9.ggplot(data=py_df, mapping= p9.aes('Horsepower','MPG'))\n    + p9.geom_point(p9.aes())\n    + p9.xlab('Horsepower')+ p9.ylab(r'MPG')\n    + p9.geom_line(p9.aes(x='Horsepower', y='fit'), color='blue')\n    + p9.annotate('text', x= 150, y = 40, label = txt))\n# print the plot\nprint(plot)\n```\n:::\n:::\n\n#### Relationship between Stock Returns and a Market Index\n\n::: {style=\"text-align: justify\"}\n     Now we will try to build a simple linear model between the return of a stock and a market index. Using `tidyquant` package from `R` or `yfinance` module from `python`, we will get share prices of different stocks and a market index. For example, we will get the share price data of Coca-Cola (NYSE:KO) and its competitor PepsiCO (NYSE:PEP). Moreover, we will collect data on US Dollar index (ICE:DX) and SPDR S&P500 ETF (NYSERCA:SPY).\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: fig-r-relation-KO-SPY\n#| fig-cap: Linear Model between SPY and KO\n#| warning: false\nlibrary(tidyquant)\n\ntickers = c(\"KO\", \"SPY\", \"PEP\",\"DX-Y.NYB\")\nprices <- tq_get(tickers,\n                 from = \"2019-01-01\",\n                 to = \"2023-12-31\"\n                 #,get = \"stock.prices\"\n                 )\n\ndf_stockR = prices |>\n    select(symbol, date, adjusted) |>\n    drop_na(adjusted) |>\n    pivot_wider(id_cols = date,names_from = symbol, \n    values_from = adjusted)\n\n\ndf_stockR = as_tibble(na.omit(CalculateReturns(df_stockR, method = \"log\")))\n\n# Linear Regression \nlibrary(fixest)\n# feols(KO ~ SPY, data = df_stockR)\nsummary(lm(KO ~ SPY, data = df_stockR))\n\nmodel = lm(KO ~ SPY, data = df_stockR)\n# adding fitted value to the dataset \ndf_stockR$fitted = fitted(model)\n# adding residual to the dataset \ndf_stockR$residual = resid(model)\n\n# We can use broom package to simplify all of these processes \nlibrary(broom)\ndf_stockR2 = df_stockR  |>\n    select(KO:DX.Y.NYB)\n\nmodel2 = lm(KO ~ SPY, data = df_stockR2) \n\ntidy(model2) # summary of model components \nglance(model2) # information about model fitness \naugment(model2, data = df_stockR2) # adds info to the dataset \n\n# Visualization \nlibrary(ggpubr)\nggplot(df_stockR, aes (x = SPY, y = KO))+\n    geom_point()+\n    geom_smooth(method = \"lm\")+\n    labs(x = \"SPY Returns\", y = \"KO Returns\") + \n    stat_regline_equation(label.x = -0.05, label.y = 0.05)  # to add reg equation\n\n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| label: fig-py-relation-KO-SPY\n#| fig-cap: Linear Model between SPY and KO\nimport yfinance as yf\nimport datetime\nimport numpy as np\n\nimport plotnine as p9\nfrom scipy import stats\n## To use statsmodels for linear regression\nimport statsmodels.api as sm \nimport statsmodels.formula.api as smf\n## To use sklearn for linear regression\nfrom sklearn.linear_model import LinearRegression\n\n#########################################################\n# Importing stock data \n## 5-year daily data for Coca-Cola, SPY, Pepsi, and USD index\n#########################################################\nend = datetime.date(2023, 12, 31)\nstart = end - pd.Timedelta(days = 365 * 5) \n# Getting the data \nko_df = yf.download(\"KO\", start = start, end = end, progress = False) # CocaCola \nspy_df = yf.download(\"SPY\", start = start, end = end, progress = False) # SPDR S&P 500\npep_df = yf.download(\"PEP\", start = start, end = end, progress = False) # PepsiCo\nusdx_df = yf.download(\"DX-Y.NYB\", start = start, end = end, progress = False) # US Dollar Index\n\n## Calculate log returns for the period based on Adj Close prices\n\nko_df['ko'] = np.log(ko_df['Adj Close'] / ko_df['Adj Close'].shift(1))\nspy_df['spy'] = np.log(spy_df['Adj Close'] / spy_df['Adj Close'].shift(1))\npep_df['pep'] = np.log(pep_df['Adj Close'] / pep_df['Adj Close'].shift(1))\nusdx_df['usdx'] = np.log(usdx_df['Adj Close'] / usdx_df['Adj Close'].shift(1))\n\n## Create a dataframe with X's (spy, pep, usdx) and Y (ko)\n\ndf_stock = pd.concat([spy_df['spy'], ko_df['ko'], \n                pep_df['pep'], usdx_df['usdx']], axis = 1).dropna()\n\n####################################################\n## 2a. Fit a simple linear regression model to the data using statsmodels \n\n### Create an instance of the class OLS\nslr_sm_model = smf.ols('ko ~ spy', data=df_stock)\n\n### Fit the model (statsmodels calculates beta_0 and beta_1 here)\nslr_sm_model_ko = slr_sm_model.fit()\n\n### Summarize the model\n\nprint(slr_sm_model_ko.summary()) \n\n## Adding the fitted values to the dataframe \ndf_stock['fitted'] = slr_sm_model_ko.fittedvalues\n\n## Adding the residuals to the dataframe \ndf_stock['residual'] = slr_sm_model_ko.resid\n\n\nparam_slr = slr_sm_model_ko.params\n\n## Linear regression plot of X (spy) and Y (ko)\nplt.figure(figsize = (10, 6))\nplt.rcParams.update({'font.size': 14})\nplt.xlabel(\"SPY Returns\")\nplt.ylabel(\"KO Returns\")\n#plt.title(\"Simple linear regression model\")\nplt.scatter(df_stock['spy'],df_stock['ko'])\nplt.plot(df_stock['spy'], param_slr.Intercept+param_slr.spy * df_stock['spy'],\n         label='Y={:.4f}+{:.4f}X'.format(param_slr.Intercept, param_slr.spy), \n         color='red')\nplt.legend()\nplt.show()\n\n```\n:::\n:::\n\n### Evaluating Linear Regression Models - Assessing the Fitness\n\n::: {style=\"text-align: justify\"}\n     When we build a linear regression model, we need to check the accuracy of the model by evaluating different parameters of the model. Some of the parameters that need to be evaluated are discussed below -\n\n#### F-statistic and Overall P-value\n\n     The F-statistic of the model tests whether the linear model provides a better fit of the data than the model that contains no independent variables. The null hypothesis is that the model as a whole does not explain a significant amount of variance of the data.\n\n#### Coefficient of Determination ($R^2$) and Adjusted $R^2$\n\n     Coefficient of determination is a statistical metric that measures the proportion of the variance of the target variable explained by the independent variables. The coefficient of determination is represented by $R^2$. The value of $R^2$ ranges between $0$ and $1$, where $0$ indicates poor fit and $1$ indicates perfect fit. There is a problem with $R^2$; it increases as the number of independent variable increases. Therefore, when we compare models with different numbers of independent variables, adjuted $R^2$ is used to evaluate the model because it penalizes models with a large number of predictors.\n\n#### P-value of Parameter Estimates\n\n     In addition to overall p-value associated with the model, linear regression models generate p-value for each predicators (parameter estimates). This p-value tests the null hypothesis that the coefficient is zero (or it does not have any effect on outcome variable). A low p-value (\\<0.05) indicates that we can reject the null hypothesis, meaning that a predictor with a low p-value should be included in the model because it has statistically significant effect on the target variable.\n\n#### Residual Standard Error (RSE)\n\n     Residual Standard Error (RSE) measures how far away an observation is from the prediction (regression line). In another words, it is mean (average) distance between the actual outcome and regression line. For example, @fig-r-relation-KO-SPY regression equation's RSE is 0.01034, which means the regression model predicts CocaCola's return with an avearagr error 0.01034.\n:::\n\n::: {style=\"text-align: justify\"}\n### Assumptions of Linear Regression\n\n     Linear regression follows some assumptions. After fitting a regression model, we should check the assumptions of the model.\n\n1.  **Linearity (Linear Relationship)**:\n\n     There must be a linear relationship between the outcome variable (y) and predictors (x). @fig-assump1 shows what we want to see and @fig-r-assump1 and @fig-py-assump1 show what we see from one of our models.\n\n![Assumption 1- What We Want to See](images/Linear_Reg_Assump1.webp){#fig-assump1 fig-align=\"center\"}\n\n::: panel-tabset\n## R\n\n```{r}\n#| label: fig-r-assump1\n#| fig-cap: Testing Assumption 1\nplot(lm(KO ~ SPY, data = df_stockR), 1)\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| label: fig-py-assump1\n#| fig-cap: Testing Assumption 1\nsns.residplot(x = 'fitted', y = 'ko', data=df_stock, \nlowess=True,\nline_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\n\n# Diagnostic Plots in Python similar to R - \n# https://towardsdatascience.com/going-from-r-to-python-linear-regression-diagnostic-plots-144d1c4aa5a\n# https://robert-alvarez.github.io/2018-06-04-diagnostic_plots/\n# https://www.kirenz.com/blog/posts/2021-11-14-linear-regression-diagnostics-in-python/\n```\n:::\n\n2.  **Independence**:\n\n     Indpendence means that each observation (data point) is independent of the others, meaning that the error terms in the model are not correlated with each other and the occurrence of one observation does not influence the probability of another observation occurring; essentially, each data point should be considered a separate, unrelated event from the others. We can check this assumption by Durbin-Watson test. The null hypothesis (H~0~) is there is no correlation among the residuals. Basically, the test detects autocorrelation in the residuals of a linear regression model. The test statistic is a value between 0 and 4, with the following interpretations: 2: No autocorrelation; \\< 2: Positive autocorrelation; \\> 2: Negative autocorrelation. To solve the independence issue, it is suggested that for positive serial correlation, consider adding lags of the dependent and/or independent variable to the model. For negative serial correlation, check to make sure that none of your variables are overdifferenced, and For seasonal correlation, consider adding seasonal dummy variables to the model. However, as a rule of thumb, test statistic values between the range of 1.5 and 2.5 are considered normal. One limitation of the DW test is that it can only test for first-order serial correlation.\n\n     Alternatively, we can can visually examine a scatterplot of the residuals (errors) against the fitted values (@fig-r-assump1 and @fig-py-assump1); if there is no clear pattern, it suggests that the independence assumption is likely satisfied. Independence assmption is important because violating it can lead to unreliable hypothesis tests and confidence intervals.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false \nlibrary(car)\ndurbinWatsonTest(lm(KO ~ SPY, data = df_stockR))\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\nfrom statsmodels.stats.stattools import durbin_watson\ndurbin_watson(slr_sm_model_ko.resid)\n```\n:::\n\n3.  **Residual Error**:\n\n     The errors have a mean (expected) value zero ($0$), constant varianace (Homoscedasticity), and are independent from each other (otherwise auto-correlation problem) and predictor variables. We can check the assumption - expected value of errors are equal to 0 - by looking at @fig-r-assump1 and @fig-py-assump1. If there is a pattern in the plot, the assumption is violated.\n\n     The assumption homoscedasticity is tested using the Scale-Location plot, in which fitted values are compared with square root of standardized residuals. Theoretically, we want to see @fig-assump3. @fig-r-assump3 and @fig-py-assump3 show Scale-Location plot (also called spread-location plot) of our model. From @fig-r-assump3 and @fig-py-assump3, it is clear that residual plots have a trend (are not all equally spread out). Thus, the assumption is violated. One solution to the problem is that using log or square root transformation of outcome variable. Moreover, we can use Non-Constant Error Variance (NVC) test to test the assumption. If p-value of the test is less than 0.05, then null hypothesis is rejected, meaning that homoscedasticity is violated. Moreover, Breusch-Pagan test or White test can also be used to test homoscedasticity.\n\n     Finally, testing the assumption - errors are independent from each other and predictors - requires the knowledge of study design and data collection to establish the validity of this assumption. Violation of this assumption is also called endogeneity problem of the model, which is the same as assumption 6 - No Endogeneity below.\n\n![Assumption 3- What We Want to See](images/Linear_Reg_Assump3.webp){#fig-assump3 fig-align=\"center\"}\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false \n#| label: fig-r-assump3\n#| fig-cap: Testing Assumption 3 \n# Scale-Location Plot \nplot(lm(KO ~ SPY, data = df_stockR),3)\n# Non-Constant Error Variance Test \nncvTest (lm(KO ~ SPY, data = df_stockR))\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| warning: false \n#| label: fig-py-assump3\n#| fig-cap: Testing Assumption 3 \n\n# Scale-Location Plot \nsns.regplot(\n    x = slr_sm_model_ko.fittedvalues, \n    y = np.sqrt(np.abs(slr_sm_model_ko.get_influence().resid_studentized_internal)),\n    scatter=True, \n    ci= False, \n    lowess=True,\n    line_kws={'color': 'blue', 'lw': 1, 'alpha': 0.8} \n)\nplt.title('Scale-Location', fontsize=10)\nplt.xlabel('Fitted Values', fontsize=15)\nplt.ylabel('$\\sqrt{|Standardized Residuals|}$', fontsize=15)\n\n# Non-Constant Error Variance Test \nimport statsmodels.stats.api as sms\n## Breusch-Pagan test \nbp_test = sms.het_white(slr_sm_model_ko.resid, slr_sm_model_ko.model.exog)\nlabels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']\nfor stat, value in zip(labels,bp_test):\n    print('{}: {:.3f}'.format(stat, value))\n\n## White Test \nwhite_test = sms.het_white(slr_sm_model_ko.resid, slr_sm_model_ko.model.exog)\nfor stat, value in zip(labels,white_test):\n    print('{}: {:.3f}'.format(stat, value))\n```\n:::\n\n4.  **Normality**:\n\n     The residuals are normally distributed. We can test this assumption by Q-Q plots (also called Quantile-Quantile plot) of the residuals. @fig-r-qqplot and @fig-py-qqplot show a QQ plot. The dots do not lie perfectly along the straight line. Moreover, Kolmogorov-Smirnov (KS) test can be used to check the normality assumption. KS tests whether a sample comes from a certain distribution. From the KS result below, we can see test statistic is 0.484 and p-value is 2.2e-16, thus rejecting null hypothesis and indicating that sample data does not come from a normal distribution.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false \n#| label: fig-r-qqplot\n#| fig-cap: Testing Assumption 4\n\n#######################################\n# Drawing QQ Plot\n########################################\n## qqnorm () function \n#qqnorm((df_stockR$residual))\n## qqline () function \n#qqline(df_stockR$residual)\n# qqPlot () function from car package \nqqPlot(df_stockR$residual) \n\n#########################################\n# Kolmogorov-Smirnov (KS) test\n##########################################\nlibrary(stats)\nks.test(df_stockR$residual, \"pnorm\")\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| warning: false\n#| label: fig-py-qqplot\n#| fig-cap: Testing Assumption 4\n#######################################\n# Drawing QQ Plot\n########################################\nsm.qqplot(slr_sm_model_ko.resid, fit=True, line=\"45\")\n\n#########################################\n# Kolmogorov-Smirnov (KS) test\n##########################################\nfrom scipy.stats import kstest\nks_test = kstest(slr_sm_model_ko.resid, \"norm\")\nlabel_ks = [\"KS Statistic\", \"P-value\"]\n\nfor stat, value in zip(label_ks, ks_test[:2]):\n    print('{}: {:.5f}'.format(stat, value))\n```\n:::\n\n5.  **Multicollinearity**:\n\n     There is little or no correlation between the predictor variables. It makes difficult to interpret the coefficients of the model. Variance Inflation Factor (VIF) can be used to test the multicollinearity. A value equal to or greater than 10 of VIF indicates multicollinearity in the data. We see the VIF of SPY and PEP is 1.84 and 1.84 respectively. Moreover, correlation matrix can also be used to identify the predictors that are highly correlated. @fig-r-corplot and @fig-py-corplot show a correlation plot of our stock returns data.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false\n#| label: fig-r-corplot\n#| fig-cap: Correlation Plot \n\n## Variance Inflation Factor (VIF)\nlibrary(car)\nvif(lm(KO ~ SPY+PEP, data = df_stockR))\n\n## Correlation Matrix and Correlation Plot \n# to get the function \nsource(\"http://www.sthda.com/upload/rquery_cormat.r\")\nrquery.cormat(df_stockR |> select(KO:DX.Y.NYB), type = c(\"lower\"), graph = TRUE)$r\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n```\n\n```{python}\n#| warning: false\n#| label: fig-py-corplot\n#| fig-cap: Correlation Plot \n\n## Correlation Matrix and Correlation Plot \n### Correlation Matrix\npd.DataFrame(np.tril(df_stock[['usdx', 'spy', 'ko','pep']].corr()),\ncolumns = ['usdx', 'spy', 'ko','pep'], index = ['usdx', 'spy', 'ko','pep']).replace(0,'')\n### Correlation Plot\nmask = np.triu(np.ones_like(df_stock[['usdx', 'spy', 'ko','pep']].corr()))\nsns.heatmap(df_stock[['usdx', 'spy', 'ko','pep']].corr(), \nannot=True, cmap=\"YlGnBu\", \nmask=mask)\n\n## Variance Inflation Factor (VIF)\n#orrelaiton_coeff = np.corrcoef(df_stock[['ko','spy']], rowvar=False)\n#VIF = np.linalg.inv(correlaiton_coeff)\n#VIF.diagonal()\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(df_stock[['ko','spy']])\n\npd.Series([variance_inflation_factor(X.values, i) \n               for i in range(1,X.shape[1])], \n              index=X.columns[1:])\n\n```\n:::\n\n6.  **No Endogeneity**:\n\n     There is no relation between the errors and the independent variables.\n:::\n\n::: {style=\"text-align: justify\"}\n### Multiple Linear Regression\n\n      Multiple regression analysis estimates the relationship between an outcome variable and two or more independent variables. More specifically, multiple regression analysis helps to understand how the value of the dependent variable changes when one of the independent variables varies, while the other independent variables remain constant.\n\n     We use Grunfeld dataset for multiple regressions. The dataset contains investment data for 11 US firms. The variables include - `invest`, which is Gross investment in 1947 dollars; `value`, which is market value as of Dec 31 in 1947 dollars; `capital`, which is stock of plant and equipment in 1947 dollars; `firm`, which include 11 US firms (General Motors, US Steel, General Electric, Chrysler, Atlantic Refining, IBM, Union Oil, Westinghouse, Goodyear, Diamond Match, American Steel); and `year`, which is 1935-1954. Our multiple regeression model is @eq-multiple -\n\n$$\ninvest_{it} = \\beta_{0} + \\beta_{1}value_{i} + \\beta_{3}capital_{it} + \\alpha_{it} + \\delta_{t} + \\epsilon_{it}\n$$ {#eq-multiple}\n\nWhere\n\n$invest_{it}$ is the gross investment of firm $i$ in year $t$\n\n$value_{it}$ is the market value of assets of firm $i$ in year $t$\n\n$capital_{it}$ is the stock value of plant and equipment of firm $i$ in year $t$\n\n$alpha_{i}$ is the fixed effect for firm $i$ (capturing unobserved firm-specific factors that don’t vary over time)\n\n$delta_{t}$ is the fixed effect for year $t$ (capturing unobserved year-specific factors that are common to all firms in that year)\n\n$epsilon_{it}$ is the error term, which includes all other unobserved factors that affect investment but are not accounted for by the independent variables or the fixed effects.\n\n     In @tbl-r-multiple and @tbl-py-multiple, we generate some multiple regression models.\n\n::: panel-tabset\n## R\n\n```{r}\n#| warning: false\n#| label: tbl-r-multiple\n#| tbl-cap: Regression of Investment on Market Value and PPE\n\n#############################################################\n## Grunfeld Data from AER Package\n#############################################################\n# Grunfeld data contains investment data for 11 US firms. The variables \n# include - invest = \nlibrary(AER)\nlibrary(modelsummary)\nlibrary(kableExtra)\nlibrary(fixest)\ndata(\"Grunfeld\", package = \"AER\")\n\nmodels_r = list()\n\n# Model (1)\nmodels_r [['OLS_FYfe']] = feols(invest ~ value + capital | firm + year, data = Grunfeld)\n\n# Model (2)\nmodels_r [['OLS_FYfeC']] = feols(invest ~ value + capital | firm + year,\ncluster = ~firm,  \ndata = Grunfeld)\n\n# Model (3)\nmodels_r [['OLS_FYfeCtwo']] = feols(invest ~ value + capital | firm + year,\ncluster = ~firm+year,  \ndata = Grunfeld)\n\nrows = tribble(\n    ~term, ~'OLS_FYfe',~'OLS_FYfeC', ~'OLS_FYfeCtwo',\n    'Firm Fixed Effects', \"YES\", \"YES\", \"YES\",\n    \"Year Fixed Effects\", \"YES\", \"YES\", \"YES\"\n)\nattr(rows, 'position') = c (5,6)\n\n\n\nmodelsummary(models_r, fmt = 2,\nestimate = \"{estimate}{stars}\",\nstatistic = 'statistic'\n#,vcov = \"robust\" # robust clustering \n#,vcov = ~firm # clusting by firm\n#,vcov = vcovHAC\n,stars = c (\"*\" = 0.10, \"**\" = 0.05, \"***\" =  0.01)\n,coef_rename = c(\n    \"value\" = \"VALUE\",\n    \"capital\" = \"CAPITAL\"\n)\n#,gof_omit = 'DF|Deviance|AIC|BIC'\n,gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\")\n,add_rows = rows\n#,notes = list(\n#'Note: In all models both firm and year fixed effects are controlled and in models 2 and 3 starndard errors', \n#'are clustered by firm (one-way cluster) and firm and year (two-way cluster) respectively. The numbers in parentheses indicate t values.')\n#,title = \"Title of the Table\"\n,output = \"kableExtra\"\n) |>\n# The line below is for styling the table, not necessary for regression table\n#kableExtra::kbl() |>\nkable_styling(full_width = TRUE) |>\nrow_spec(c(6,9), extra_css = \"border-bottom: 1.25px solid\") |>\nfootnote('In all models both firm and year fixed effects are controlled and in models 2 and 3 starndard errors are clustered by firm (one-way cluster) and firm and year (two-way cluster) respectively. The numbers in parentheses indicate t values.')\n\n\n\n```\n\n## Python\n\n```{r}\n#| include: false\nlibrary(reticulate)\n#Sys.setenv('RETICULATE_PYTHON' = 'C:\\\\Users\\\\mshar\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe')\n\n\n```\n\n```{python}\n#| warning: false\n#| label: tbl-py-multiple\n#| tbl-cap: Regression of Investment on Market Value and PPE\n\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.datasets import grunfeld\ndata = grunfeld.load_pandas().data\ndata.year = data.year.astype(np.int64)\n\n# Link for pyfixest - \n# https://py-econometrics.github.io/pyfixest/quickstart.html\nimport pyfixest as pf\nfrom pyfixest.estimation import feols\n\n# Model (1)\nfe_model1 = feols(\n  \"invest ~ value + capital | firm + year\", \n  data = data)\n#fe_model1.summary()\n#pf.etable(fe_model1)\n#fe_model1.tidy()\n#fe_model1.coefplot()\n\n\n# Model (2)\nfe_model2 = feols(\n  \"invest ~ value + capital | firm + year\",\n  vcov= {\"CRV1\": \"firm\"}, \n  data = data)\n\n# Model (3)\nfe_model3 = feols(\n  \"invest ~ value + capital | firm + year\",\n  vcov= {\"CRV1\": \"firm+year\"}, \n  data = data)\n\npf.etable([fe_model1, fe_model2, fe_model3]\n#, type = \"gt\"\n,coef_fmt= 'b \\n (t)'\n,signif_code= [0.01,0.05,0.10]\n)\n```\n:::\n:::\n\n## Time Series Analysis\n\n## Exercises\n\n1.  Calculate a linear model between PepsiCo stock returns and S&P 500 Market index (the Ticker of S&P 500 Index is `^GSPC`)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"predictive.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","jupyter":"python3","number-depth":5,"site-url":"https://github.com/msharifbd","repo-url":"https://github.com/msharifbd","repo-branch":"main","sharing":["twitter","facebook","linkedin"],"downloads":["pdf","epub"],"bibliography":["references.bib"],"editor":"visual","theme":"cosmo","mainfont":"Georgia, serif","callout-appearance":"simple","title":"Predictive Modeling - Linear Regression"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"pdf-book"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"jupyter":"python3","number-depth":5,"site-url":"https://github.com/msharifbd","repo-url":"https://github.com/msharifbd","repo-branch":"main","sharing":["twitter","facebook","linkedin"],"downloads":["pdf","epub"],"bibliography":["references.bib"],"editor":"visual","documentclass":"scrreprt","mainfont":"Times New Roman","sansfont":"Arial","monofont":"Courier New","colorlinks":true,"title":"Predictive Modeling - Linear Regression"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}